<!DOCTYPE html><html lang="en" class="no-js"><head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://ironbar.github.io/arc25/03_State_of_the_art/">
      
      
        <link rel="prev" href="../02_Data_Understanding/">
      
      
        <link rel="next" href="../04_Initial_Plan/">
      
      
      <link rel="icon" href="../res/arc_icon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.40">
    
    
      
        <title>State of the art - arc25</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8c3ca2c6.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"><script src="../assets/javascripts/glightbox.min.js"></script><style id="glightbox-style">
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#state-of-the-art" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="arc25" class="md-header__button md-logo" aria-label="arc25" data-md-component="logo">
      
  <img src="../res/arc_icon.jpg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            arc25
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              State of the art
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ironbar/arc25" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../01_Business_Understanding/" class="md-tabs__link">
        
  
    
  
  Business Understanding

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../02_Data_Understanding/" class="md-tabs__link">
        
  
    
  
  Data Understanding

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  State of the art

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../04_Initial_Plan/" class="md-tabs__link">
        
  
    
  
  Initial Plan

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../modeling/Iteration_01_architects_baseline/" class="md-tabs__link">
          
  
    
  
  Modeling

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../utils/00_Challenge_Workflow/" class="md-tabs__link">
          
  
    
  
  Utils

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../05_Solution_Summary/" class="md-tabs__link">
        
  
    
  
  Solution summary

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="arc25" class="md-nav__button md-logo" aria-label="arc25" data-md-component="logo">
      
  <img src="../res/arc_icon.jpg" alt="logo">

    </a>
    arc25
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ironbar/arc25" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"></path></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../01_Business_Understanding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Business Understanding
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../02_Data_Understanding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Understanding
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    State of the art
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    State of the art
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learnings-from-arc24" class="md-nav__link">
    <span class="md-ellipsis">
      Learnings from ARC24
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learnings from ARC24">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-solved-the-arc-challenge-with-a-tuned-version-of-o3" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI solved the ARC challenge with a tuned version of o3
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI solved the ARC challenge with a tuned version of o3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#other-reasoning-models" class="md-nav__link">
    <span class="md-ellipsis">
      Other reasoning models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-time-training-ttt" class="md-nav__link">
    <span class="md-ellipsis">
      Test-time training (TTT)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transduction-and-induction" class="md-nav__link">
    <span class="md-ellipsis">
      Transduction and induction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#code-generation-search" class="md-nav__link">
    <span class="md-ellipsis">
      Code generation (Search)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Code generation (Search)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inference-time-scaling-and-collective-intelligence-for-frontier-ai" class="md-nav__link">
    <span class="md-ellipsis">
      Inference-Time Scaling and Collective Intelligence for Frontier AI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#funsearch-and-alphaevolve" class="md-nav__link">
    <span class="md-ellipsis">
      FunSearch and AlphaEvolve
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jeremy-berman-high-score-on-arc-agi-2" class="md-nav__link">
    <span class="md-ellipsis">
      Jeremy Berman high score on ARC-AGI-2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reasoning-code-and-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Reasoning, code and RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reasoning, code and RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-improving-language-models-for-evolutionary-program-synthesis-a-case-study-on-arc-agi" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#codeit-self-improving-language-models-with-prioritized-hindsight-replay" class="md-nav__link">
    <span class="md-ellipsis">
      CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rlef-grounding-code-llms-in-execution-feedback-with-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improving-multi-turn-tool-use-with-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Improving Multi-Turn Tool Use with Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#retool-reinforcement-learning-for-strategic-tool-use-in-llms" class="md-nav__link">
    <span class="md-ellipsis">
      ReTool: Reinforcement Learning for Strategic Tool Use in LLMs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model" class="md-nav__link">
    <span class="md-ellipsis">
      Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#absolute-zero-reinforced-self-play-reasoning-with-zero-data" class="md-nav__link">
    <span class="md-ellipsis">
      Absolute Zero Reinforced Self-play Reasoning with Zero Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llms-for-engineering-teaching-models-to-design-high-powered-rockets" class="md-nav__link">
    <span class="md-ellipsis">
      LLMs for Engineering: Teaching Models to Design High Powered Rockets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-from-sparse-and-binary-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Learning from sparse and binary rewards
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other" class="md-nav__link">
    <span class="md-ellipsis">
      Other
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Other">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arc-agi-without-pretraining" class="md-nav__link">
    <span class="md-ellipsis">
      ARC-AGI without pretraining
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#searching-latent-program-spaces" class="md-nav__link">
    <span class="md-ellipsis">
      Searching Latent Program Spaces
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-2d-ngpt-model-for-arc-prize" class="md-nav__link">
    <span class="md-ellipsis">
      A 2D nGPT Model for Arc Prize
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../04_Initial_Plan/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Initial Plan
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5">
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Modeling
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Modeling
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_01_architects_baseline/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 1. Architects baseline
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_02_8_fold/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 2. Architects solution with 8 data splits
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_03_ideal_test_time_training/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 3. Ideal test-time training setup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_04_first_steps_with_code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 4. First steps with code
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_05_test_time_training_with_code_HER/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 5. Test-time training with code. Hindsight Experience Replay (HER)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_06_reinforcement_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 6. Reinforcement learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_07_optimize_ttt_on_evaluation_set/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 7. Optimize TTT on the evaluation set
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_08_improve_HER/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 8. Improve HER
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_09_improve_training_script/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 9. Improve training script
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_10_solve_arc_tasks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 10. Try to solve real ARC tasks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_11_pretrain_lora_on_new_tasks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 11. Pretrain LoRA on new tasks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_12_solve_a_few_arc_tasks/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 12. Solve a few ARC tasks
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_13_reflections/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 13. Reflections
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_14_optimize_inference/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 14. Optimize inference
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_15_the_path_forward/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 15. The path forward: Search &amp; Learn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_16_search_with_base_models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 16. Search with base models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_17_increase_search_diversity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 17. Increase search diversity
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_19_search_with_BARC/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 19. Search with BARC
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_20_data_augmentation_with_BARC/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 20. Data augmentation with BARC
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_21_fix_bug_with_data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 21. Fix bug with data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_22_ttt_BARC/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 22. Test-time Training with BARC induction model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_23_ttt_BARC_v2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 23. All in with test-time training with BARC induction model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_24_RL_BARC/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 24. Using RL to improve BARC induction model
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_25_debug_parallel_code_execution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 25. Debug parallel code execution
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_26_more_compute/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 26. Acquire more compute
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_27_improve_search_and_learn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 27. Improve search and learn
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_28_refine_predictions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 28. Refine predictions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_29_multi-gpu-rl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 29. Multi-gpu RL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_30_solve_RL_collapse/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 30. Solve RL Collapse
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_31_how_to_improve/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 31. How to improve from 20% to 100%?
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_32_analyze_model_predictions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 32. Analyze model predictions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_33_rl_barc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 33. RL with BARC data
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_34_multi-turn_rl/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 34. Multi-turn RL
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_35_fp16_vs_bf16/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration 35. FP16 vs BF16
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../modeling/Iteration_n/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Iteration n. Iteration_title
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6">
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Utils
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Utils
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utils/00_Challenge_Workflow/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Challenge workflow
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utils/markdown_cheatsheet/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Markdown cheatsheet
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utils/methodology/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Methodology
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../05_Solution_Summary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Solution summary
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#learnings-from-arc24" class="md-nav__link">
    <span class="md-ellipsis">
      Learnings from ARC24
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learnings from ARC24">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-solved-the-arc-challenge-with-a-tuned-version-of-o3" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI solved the ARC challenge with a tuned version of o3
    </span>
  </a>
  
    <nav class="md-nav" aria-label="OpenAI solved the ARC challenge with a tuned version of o3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#other-reasoning-models" class="md-nav__link">
    <span class="md-ellipsis">
      Other reasoning models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-time-training-ttt" class="md-nav__link">
    <span class="md-ellipsis">
      Test-time training (TTT)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transduction-and-induction" class="md-nav__link">
    <span class="md-ellipsis">
      Transduction and induction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#code-generation-search" class="md-nav__link">
    <span class="md-ellipsis">
      Code generation (Search)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Code generation (Search)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inference-time-scaling-and-collective-intelligence-for-frontier-ai" class="md-nav__link">
    <span class="md-ellipsis">
      Inference-Time Scaling and Collective Intelligence for Frontier AI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#funsearch-and-alphaevolve" class="md-nav__link">
    <span class="md-ellipsis">
      FunSearch and AlphaEvolve
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#jeremy-berman-high-score-on-arc-agi-2" class="md-nav__link">
    <span class="md-ellipsis">
      Jeremy Berman high score on ARC-AGI-2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reasoning-code-and-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Reasoning, code and RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reasoning, code and RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#self-improving-language-models-for-evolutionary-program-synthesis-a-case-study-on-arc-agi" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#codeit-self-improving-language-models-with-prioritized-hindsight-replay" class="md-nav__link">
    <span class="md-ellipsis">
      CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rlef-grounding-code-llms-in-execution-feedback-with-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improving-multi-turn-tool-use-with-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Improving Multi-Turn Tool Use with Reinforcement Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#retool-reinforcement-learning-for-strategic-tool-use-in-llms" class="md-nav__link">
    <span class="md-ellipsis">
      ReTool: Reinforcement Learning for Strategic Tool Use in LLMs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model" class="md-nav__link">
    <span class="md-ellipsis">
      Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#absolute-zero-reinforced-self-play-reasoning-with-zero-data" class="md-nav__link">
    <span class="md-ellipsis">
      Absolute Zero Reinforced Self-play Reasoning with Zero Data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llms-for-engineering-teaching-models-to-design-high-powered-rockets" class="md-nav__link">
    <span class="md-ellipsis">
      LLMs for Engineering: Teaching Models to Design High Powered Rockets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-from-sparse-and-binary-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      Learning from sparse and binary rewards
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other" class="md-nav__link">
    <span class="md-ellipsis">
      Other
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Other">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arc-agi-without-pretraining" class="md-nav__link">
    <span class="md-ellipsis">
      ARC-AGI without pretraining
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#searching-latent-program-spaces" class="md-nav__link">
    <span class="md-ellipsis">
      Searching Latent Program Spaces
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-2d-ngpt-model-for-arc-prize" class="md-nav__link">
    <span class="md-ellipsis">
      A 2D nGPT Model for Arc Prize
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="state-of-the-art">State of the art</h1>
<!--- --->

<p>I'm going to recap all the learnings from the previous <a href="https://www.kaggle.com/competitions/arc-prize-2024">ARC24 challenge</a>. I would also add new and relevant papers during the ARC25 competition.</p>
<h2 id="learnings-from-arc24">Learnings from ARC24</h2>
<ul>
<li>Reasoning models trained with RL like <code>o3</code> can solve ARC-AGI-1, but they need a lot of compute. That is around <a href="https://x.com/guille_bar/status/1870479630383329472">x40000 times</a> the compute allowed in the competition ($8 vs $340k). However on ARC-AGI-2 they seem to be scoring below 5%.</li>
<li>Test-time training is crucial to improve the accuracy of transduction models. In my case the score improves from 11 to 33.</li>
<li>Frontier LLMs can generate code that solves more than half of the semi-private ARC set.</li>
<li>Induction and transduction are complementary approaches. It would have sense to first try with induction (which has higher guarantees) and use transduction only if induction fails.</li>
<li>LLMs struggle with tasks that have big grids, however the fact that <code>o3</code> can solve ARC might hint that a 2d representation for the grid is not needed.</li>
</ul>
<h3 id="openai-solved-the-arc-challenge-with-a-tuned-version-of-o3"><a href="https://arcprize.org/blog/oai-o3-pub-breakthrough">OpenAI solved the ARC challenge with a tuned version of <code>o3</code></a></h3>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../res/2025-03-18-13-59-03.png" data-desc-position="bottom"><img alt="o3 performance" src="../res/2025-03-18-13-59-03.png"></a></p>
<p>Details are not public, but it is very likely that <code>o3</code> is trained just with reinforcement learning like DeepSeek's <code>r1</code>. When <a href="https://openai.com/index/learning-to-reason-with-llms/">o1 was announced</a> they said:</p>
<blockquote>
<p>Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process.</p>
</blockquote>
<p>This sounds very similar to the training process described in the <a href="https://arxiv.org/abs/2501.12948"><code>r1</code> paper</a>.
The only different thing is that in the table with the results there is a <code>samples</code> field that is 6 for low compute and 1024 for high compute. If we compare the numbers it seems that it is the number of times each task was tried to be solved. So there must be an aggregation mechanism to combine the responses from all the runs. It could be as simple as a voting mechanism and as complex as the model receiving as input the responses and choosing.</p>
<p>On average it uses 55k tokens per run. For 100 tasks that would be 5.5M output tokens if each task is run only once. If we are allowed 12 hours for the submission that would require an output speed of 127 token/second. That might be possible for a model like <a href="https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html">Qwen2.5 1.5B</a>, probably not for a model like Llama3 8B.</p>
<p>So in theory we could take a reasoning model such as <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B">deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</a> and fine-tune it to do ARC tasks using RL. We could reward the model for creating shorter and correct answers. OpenAI also offers a <a href="https://openai.com/form/rft-research-program/">service</a> to do "Reinforcement Fine-tuning". </p>
<p>Notice that OpenAI has decided <a href="https://techcrunch.com/2025/02/12/openai-cancels-its-o3-ai-model-in-favor-of-a-unified-next-gen-release/">not to release <code>o3</code></a>, so we don't know how necessary the tuning for ARC was needed.</p>
<p>Mikel Bobel-Irizar did an <a href="https://anokas.substack.com/p/llms-struggle-with-perception-not-reasoning-arcagi">awesome analysis</a> of the effect of task length on the accuracy of <code>o3</code>. We could
use upscaling as data augmentation so the model learns to work with bigger images. There is also another <a href="https://anokas.substack.com/p/o3-and-arc-agi-the-unsolved-tasks">blogpost</a> with the unsolved evaluation tasks.</p>
<h4 id="other-reasoning-models">Other reasoning models</h4>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../res/2025-03-18-14-48-47.png" data-desc-position="bottom"><img alt="other reasoning model results" src="../res/2025-03-18-14-48-47.png"></a></p>
<p>There are other reasoning models such as <code>r1</code> and Sonnet 3.7 but none of them achieve as high results as OpenAI's model. That does not happen in other fields such as mathematics, so probably OpenAI is using some 2d data for its RL training.</p>
<p>Interestingly in the <a href="https://arcprize.org/blog/r1-zero-r1-results-analysis">results</a> we can see that <code>r1</code> uses 6-11k tokens to solve each task. That is between 5 and 10 times less than <code>o3</code>.</p>
<h3 id="test-time-training-ttt">Test-time training (TTT)</h3>
<p>Test-time training was arguably the biggest discovery of ARC24 challenge. In retrospective it is clear that if intelligence is all about adaptation to novelty, then we should not keep the models frozen but let them adapt to do new tasks. The MindsAI team found this approach but they decided not to make their solution public.</p>
<p>Probably the best implementation and description was done by <a href="https://arxiv.org/abs/2411.07279">the Architects</a>. There is also a paper named <a href="https://arxiv.org/abs/2411.07279">The Surprising Effectiveness of Test-Time Training for Abstract Reasoning</a> and my own <a href="https://ironbar.github.io/arc24/05_Solution_Summary/">solution</a> also used TTT.</p>
<p>Update: The MindsAI team has published <a href="https://github.com/MohamedOsman1998/deep-learning-for-arc/blob/main/deep_learning_for_arc.pdf">a paper</a> describing their approach.</p>
<h3 id="transduction-and-induction"><a href="https://arxiv.org/abs/2411.02272">Transduction and induction</a></h3>
<p>This paper defined the terms transduction (generating the output grid directly) and induction (writing code to solve the tasks) and showed they were complimentary. Additionally they generated 400k new tasks using LLMs, showing that is possible to augment the data.</p>
<p>The <a href="https://github.com/xu3kev/BARC">code</a> is open-source and I should take a look at it, it could serve as inspiration for creating the DSL.</p>
<p>Notice that they are able to generate new tasks using LLMs because they work directly with code, not with the grid images. So they switch the modality from image to text
and that way are able to harness the power of LLMs. They used gpt-4o-mini for generation, so today we could use more powerful models for more diverse and complex tasks.</p>
<h2 id="code-generation-search">Code generation (Search)</h2>
<p>Different attempts have tried using LLMs to generate python code to solve the ARC tasks. This induction approach has the advantage that the functions can be verified, whereas output grids from the transduction approach cannot be verified.
This allows to generate thousands of candidate solutions and filter all those that do not generate correct outputs
for the training samples. The main differences between this methods is how the model is prompted to generate the responses.</p>
<ul>
<li><a href="https://arcprize.org/blog/2024-progress-arc-agi-pub">Summary of the progress in the public leaderboard in 2024</a></li>
<li><a href="https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi">Jeremy Berman</a> uses an approach similar to <a href="https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/">FunSearch</a></li>
<li><a href="https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt">Ryan Greenblatt</a> was the first to show that this approach could work and how it scaled with the number of predictions.</li>
<li><a href="https://ctpang.substack.com/p/arc-agi-2-sota-efficient-evolutionary">Eric Pang</a> scored 77.1% on ARC-AGI-1 and 26% on ARC-AGI-2 using a code generation approach similar to FunSearch.</li>
</ul>
<h3 id="inference-time-scaling-and-collective-intelligence-for-frontier-ai"><a href="https://sakana.ai/ab-mcts/">Inference-Time Scaling and Collective Intelligence for Frontier AI</a></h3>
<blockquote>
<p>Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to "go wider" by expanding new candidate responses or "go deeper" by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS consistently outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling.</p>
</blockquote>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../res/1752561777382_image.png" data-desc-position="bottom"><img alt="Overview of the search algorithm in Multi-LLM AB-MCTS" src="../res/1752561777382_image.png"></a></p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../res/1752562034376_image.png" data-desc-position="bottom"><img alt="Results of AB-MCTS and Multi-LLM AB-MCTS on ARC-AGI-2, showing Pass@k as a function of the number of LLM calls." src="../res/1752562034376_image.png"></a></p>
<p>This paper is interesting because clearly explains the challenges of search: go wider or deeper?
Their search strategy learns to decide which LLM to use and wether to go wider or deeper for each ARC problem.</p>
<p>One worrying thing is that even when generating code, they pass@250 of 30% goes down to 19% when doing pass@2. I thought that selecting the correct code was more or less trivial, but does not seem to be the case.</p>
<h3 id="funsearch-and-alphaevolve"><a href="https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/">FunSearch</a> and <a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">AlphaEvolve</a></h3>
<p>These two papers describe an evolutionary method where LLMs write code to optimize some function. Funsearch
used small LLMs that were sampled millions of times, whereas AlphaEvolve uses frontier models that
only need in the order of thousand calls to optimize the task.</p>
<p>On each new generation the LLM receives the context of the problem, and previous code generation
with their scores.</p>
<p>This method shows how we can use search with LLMs to discover new solutions to problems. The main difference
with ARC is that AlphaEvolve requires a continuous metric (f.e. execution time, memory usage...) and
on ARC we have sparse rewards. AlphaEvolve requires the continuous metric to be able to evolve the population (they used an island-based method).</p>
<p>Finally they use multiple prompts to increase diversity in the responses.</p>
<h3 id="jeremy-berman-high-score-on-arc-agi-2"><a href="https://jeremyberman.substack.com/p/how-i-got-the-highest-score-on-arc-agi-again">Jeremy Berman high score on ARC-AGI-2</a></h3>
<p>The <a href="https://github.com/jerber/arc-lang-public">code</a> is public.</p>
<blockquote>
<p>score_instructions_on_challenge records per-example results, calculates a simple cell-wise similarity score</p>
</blockquote>
<p>I'm not sure if cell-wise similarity is a good guiding metrics. I believe that checking if a grid
is correct is a better option, however if none of the grids are correct it might be useful.</p>
<blockquote>
<p>I used the same Evolutionary Test-Time Compute architecture as my v1 solution but replaced Python functions with plain English instructions.
My original solution used language models to generate Python functions to solve tasks. This approach had a key advantage: functions are deterministic and testable. I could generate hundreds of candidate functions, rank them by their performance on training examples, and evolve better solutions from the highest-scoring ones.
This strategy hits a wall with ARC v2. The transformations are often too complex to express elegantly in Pythonthey require nuanced pattern recognition and contextual understanding that would result in unwieldy, brittle code. So I turned to a language much older than Python: English.</p>
</blockquote>
<p>When we use natural language the model becomes the "python interpreter". If we have access to a model
like Grok-4 that might be an option, but I don't believe we have a similar small open-source model.
I still believe that python code with the right set of primitive functions is the way to go.</p>
<h2 id="reasoning-code-and-rl">Reasoning, code and RL</h2>
<h3 id="self-improving-language-models-for-evolutionary-program-synthesis-a-case-study-on-arc-agi"><a href="https://icml.cc/virtual/2025/poster/43499">Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI</a></h3>
<blockquote>
<p>Many program synthesis tasks prove too challenging for even state-of-the-art language models to solve in single attempts. Search-based evolutionary methods offer a promising alternative by exploring solution spaces iteratively, but their effectiveness remain limited by the fixed capabilities of the underlying generative model. We propose SOAR, a method that learns program synthesis by integrating language models into a self-improving evolutionary loop. SOAR alternates between an evolutionary search that uses an LLM to sample and refine candidate solutions, and a hindsight learning phase that converts search attempts into valid problem-solution pairs used to fine-tune the LLM's sampling and refinement capabilitiesenabling increasingly effective search in subsequent iterations. On the challenging ARC-AGI benchmark, SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52\% of the public test set.</p>
</blockquote>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../res/1752485393103_image.png" data-desc-position="bottom"><img alt="SOAR results" src="../res/1752485393103_image.png"></a></p>
<p><a class="glightbox" data-type="image" data-width="auto" data-height="auto" href="../res/1752492484931_image.png" data-desc-position="bottom"><img alt="alt text" src="../res/1752492484931_image.png"></a></p>
<ul>
<li>It does not use a DSL, writes all the python code directly. I believe that the DSL allows for shorter programs, enabling faster and easier exploration. Maybe we can give the interface of the DSL functions as input to the model.</li>
<li>Search is mostly independent, they do 3k samples in the first place and then try to refine those. I believe it is more efficient to have a more global view when searching.</li>
<li>One thing that worries me is that the improvement of test-time training is small, just 3-5%. There is no table that makes this explicit</li>
<li>It seems that no data augmentation was used, this could improve the results slightly but won't be game-changer</li>
<li>It is beneficial to use data from all the models, diversity is crucial</li>
<li>Bigger models get better scores</li>
<li>Remember that closed frontier models could score higher than 50% just with search</li>
<li>They use <a href="https://github.com/sgl-project/sglang">sglang</a> for inference</li>
<li>By looking at the code it seems that they use <code>temperature=1.0</code> for inference</li>
</ul>
<p>I believe this paper is in the right direction. It uses code, hindsight relabelling and combines search and learning. Why it does not solve ARC?</p>
<ul>
<li>Maybe a tighter integration between search and learning is needed, a more integral approach.</li>
<li>Using a DSL could also have a big effect, but probably not enough to solve ARC</li>
</ul>
<h3 id="codeit-self-improving-language-models-with-prioritized-hindsight-replay"><a href="https://arxiv.org/abs/2402.04858">CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay</a></h3>
<p>This is a very interesting paper that uses code and hindsight experience replay. They use <a href="https://github.com/michaelhodel/re-arc">Hodel's DSL</a> as a start point but they apply mutation to augment the tasks.</p>
<p>This paper shows that it's possible to learn from the test set using hindsight replay. How can we improve it?</p>
<ul>
<li>Using a bigger and better model, they use a small 220M LLM, we could be using a 7B parameter model</li>
<li>Fine-tune the model individually for each task</li>
<li>Do the search first on the training tasks to generate more training data</li>
<li>More data augmentation</li>
<li>Use a more simple and minimal DSL</li>
</ul>
<h3 id="rlef-grounding-code-llms-in-execution-feedback-with-reinforcement-learning"><a href="https://arxiv.org/abs/2410.02089">RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning</a></h3>
<p>In this paper they train the models to use effectively the feedback from code execution by using reinforcement learning. </p>
<ul>
<li>They train on 13k problems, an order of magnitude higher than ARC. The model is updated 12k times, each update is done with a batch size of 256. </li>
<li>The model is given 3 attempts to solve the tasks. </li>
<li>Only the final response is considered to compute the reward</li>
<li>Trained took 5700 A100 GPU hours (20*288), that is around $10k. If I can work an order of magnitude below I would be fine. </li>
<li>The 70B model roughly doubles the performance of the 8B model. </li>
<li>Their implementation of SFT does not match the results from RL (This contradicts the R1 paper)</li>
</ul>
<h3 id="deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning"><a href="https://arxiv.org/abs/2501.12948">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></h3>
<p>They show that the model can develop capabilities such as self-verification, reflection  just with RL, without the need of SFT.</p>
<p>One interesting finding is that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.</p>
<h3 id="improving-multi-turn-tool-use-with-reinforcement-learning"><a href="https://www.bespokelabs.ai/blog/improving-multi-turn-tool-use-with-reinforcement-learning">Improving Multi-Turn Tool Use with Reinforcement Learning</a></h3>
<p>Bespoke Labs employed Group Relative Policy Optimization (GRPO) to train Qwen2.5-7B-Instruct on just 100 examples from the BFCL benchmark, enhancing multi-turn tool use performance by 23% without relying on human or teacher demonstrations. This approach enabled the agent to learn complex tool orchestrationsuch as sequencing API calls for tasks like flight bookingthrough reinforcement learning guided solely by outcome-based rewards.</p>
<p>They trained for around 15 hours on 4xH200s. <a href="https://x.com/RichardZ412/status/1917621469871251943">source</a></p>
<h3 id="retool-reinforcement-learning-for-strategic-tool-use-in-llms"><a href="https://arxiv.org/abs/2504.11536">ReTool: Reinforcement Learning for Strategic Tool Use in LLMs</a></h3>
<p><strong>ReTool</strong> is a reinforcement learning framework that equips LLMs with the ability to strategically invoke tools through multi-turn code execution during reasoning. By integrating real-time code execution into the learning loop, ReTool not only improves performance on complex tasks like the AIME math benchmark but also achieves significantly more efficient training compared to text-only RL methods. The model autonomously learns when and how to use code, resulting in concise, accurate reasoning and emergent behaviors such as code self-correction.</p>
<h3 id="does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model"><a href="https://natolambert.substack.com/p/does-reinforcement-learning-really">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a></h3>
<p>This paper shows that if we look at <code>pass@k</code> metric the base model outperform RL fine-tuned models when we use big <code>k</code> values. This shows that the base model has higher variance than the fine-tuned one, so if we sample long enough we will eventually get some correct answer.</p>
<p>The advantage of RL is that it increases the <code>pass@1</code> metric, which is arguably much more important than the <code>pass@256</code> metric. If we don't have a reliable method to select the correct answer there is no value in generating a correct answer among a lot of incorrect ones.</p>
<p>It is likely that in the future better exploration methods during RL might change these results. Otherwise the models will be constrained by the goodness of the base model.</p>
<h3 id="absolute-zero-reinforced-self-play-reasoning-with-zero-data"><a href="https://arxiv.org/abs/2505.03335">Absolute Zero Reinforced Self-play Reasoning with Zero Data</a></h3>
<p>This paper shows that it is possible to train with reinforcement learning a model to both propose and solve tasks if we have access to an external verifier such a python interpreter.
The goal of the proposer is to generate tasks that are neither too easy or too hard, maximizing learnability.
One interesting thing is that they have triplets of inputs, outputs and program, and they teach the model to predict any of the elements of the triplet, in a very similar way to my omni-arc approach. So they do induction (program), deduction (output) and abduction (inputs).</p>
<p>Although they train for 500 steps, it seems that the biggest improvement is due to the first 100 steps.</p>
<p>Bigger models benefit more from this technique. Probably they can generate more diverse training samples. This agrees with the findings in the DeepSeek R1 paper: use RL on a big model and distill to smaller models.</p>
<p>They check for program safety and determinism, I should also do that.</p>
<h3 id="llms-for-engineering-teaching-models-to-design-high-powered-rockets"><a href="https://arxiv.org/abs/2504.19394">LLMs for Engineering: Teaching Models to Design High Powered Rockets</a></h3>
<p>Very cool paper that shows how test-time RL can help an LLM to optimize rocket designs given access to a simulation environment and a continuous reward.</p>
<p>Similar to AlphaEvolve in spirit, but using a much smaller LLM that evolves over time.</p>
<p>Is there room for a platform that helps users do this kind of things?</p>
<h3 id="learning-from-sparse-and-binary-rewards">Learning from sparse and binary rewards</h3>
<ol>
<li>Reward shaping</li>
<li>Curiosity: Intrinsic motivation</li>
<li>Hindsight Experience Replay</li>
<li>Curriculum learning</li>
<li>Imitation learning</li>
</ol>
<p>This search for methods that allow learning from sparse and binary rewards hasn't thrown
anything new. I was already aware of HER. Curriculum learning might naturally arise in ARC because the model will likely solve easier tasks first, train on them and then solve more complex tasks. Imitation learning is the basis of the training of an LLM. </p>
<p><strong>Reward shaping</strong> might be achieved by defining a metric function that isn't binary. However I'm afraid that simple pixel accuracy is not enough: there might be some program that is on the correct direction but has lower accuracy than a program that is in the wrong direction.</p>
<p>Curiosity might be used during training, or during search for node exploration. However again it's difficult to write a metric for curiosity.</p>
<ul>
<li><a href="https://chatgpt.com/share/68765887-ca40-8012-b400-0a46ffd0be8b">ChatGPT Deep Research</a></li>
<li><a href="https://g.co/gemini/share/5d2672835546">Gemini Deep Research</a></li>
</ul>
<h2 id="other">Other</h2>
<h3 id="arc-agi-without-pretraining"><a href="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_without_pretraining.html">ARC-AGI without pretraining</a></h3>
<p>This novel approach does not use any training data! Scores 4.17 on ARC-AGI-2.</p>
<blockquote>
<p>We propose that lossless information compression can serve as an effective framework for solving ARC-AGI puzzles. A more efficient (i.e., lower-bit) compression of a puzzle correlates with a more accurate solution.</p>
</blockquote>
<p>I don't understand the method well but it seems to be trying to create a compressed representation of the task, that is used to generate the output for the test sample.</p>
<h3 id="searching-latent-program-spaces"><a href="https://arxiv.org/abs/2411.08706">Searching Latent Program Spaces</a></h3>
<p>They use an autoencoder to learn the space of programs. At inference the encoder gives a good starting point, but the gradient is used to find a better task representation. The idea is interesting but the performance is very weak, will have to wait if they are able to make it work at <a href="https://ndea.com/">Ndea</a>.</p>
<h3 id="a-2d-ngpt-model-for-arc-prize"><a href="https://github.com/jfpuget/ARC-AGI-Challenge-2024/blob/main/arc.pdf">A 2D nGPT Model for Arc Prize</a></h3>
<p>Interesting because it uses a 2d transformer, not 1d as most of the other solutions.</p>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg>
    </span>
    2025-10-12
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer">
        
          
          <a href="../02_Data_Understanding/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Data Understanding">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Data Understanding
              </div>
            </div>
          </a>
        
        
          
          <a href="../04_Initial_Plan/" class="md-footer__link md-footer__link--next" aria-label="Next: Initial Plan">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Initial Plan
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      2025. Guillermo Barbadillo
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/guillermobarbadillo/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/guille_bar" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253"></path></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.youtube.com/channel/UCOHmUwHnd2hmUpiDzaQ1Isg" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"></path></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.kaggle.com/ironbar" target="_blank" rel="noopener" title="www.kaggle.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M304.2 501.5 158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.25 0 6-3z"></path></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.indexes", "navigation.top", "navigation.footer"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.525ec568.min.js"></script>
      
        <script src="../javascript/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  
<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>