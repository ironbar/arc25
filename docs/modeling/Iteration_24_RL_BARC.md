# Iteration 2x. Using RL to improve BARC induction model

_11-09-2025_

<!---
The work is done using short iterations. Each iteration needs to have a very
clear goal. This allows to gain greater knowledge of the problem on each iteration.

<details>
  <summary>Click to expand/collapse this section</summary>
</details>
--->

## Goal

Can I improve the BARC induction model using reinforcement learning?

## Motivation

I have read the [RL guide](https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide) from unsloth
and they say that 300 samples are enough to see an improvement in the model. Probably I will need much
more compute for ARC but I would like to try.

The BARC induction model seems to have non-zero probability of solving the ARC-AGI-1 tasks, RL is
the way to increase that probability.

![alt text](res/1756274756683_image.png)

Ideas for the reward function:

- +1 if the model generates code
- +1 if the model generates running code
- Finally sum the ratio of correct grids. I believe that pixel accuracy is not a good metric, but I could try it also. I have the feeling that ARC is an all or nothing dataset, and pixel accuracy might lead to local optimums instead of leading to the global maximum.

On a first step I could try with a single training task. Then I could move to use all the training tasks.
I would measure the improvement on the training and the evaluation dataset. Finally if the technique
is helpful, I would move to using the synthetic dataset in a following iteration.

An additional motivation is that I have found that I would be able to make 512 predictions at maximum
for task on the Kaggle submission. That would solve just 22% of the ARC-AGI-1 evaluation
tasks. I need a model with a higher pass rate. RL is the way to get that.

## Development

### Thoughts

- As far as I know, the best setup is to use some GPUs for inference and others for training. First
  experiments on my PC will use one 3090 for inference and another for training.
- The trickier part of the configuration is that we need reward signal to be able to learn. I will
  start by using the training tasks from ARC-AGI-1 that have a mean pass rate of 12%. [Source](Iteration_21_fix_bug_with_data.md#accuracy-on-the-different-datasets)
  That is around 1 out of 8 runs. Thus I could be doing 8 or 16 predictions per task and that should
  work. On the evaluation set the pass rate falls to ~2%, requiring to do 64 or 128 predictions per
  task to have some signal. So let's start with the train set and measure if that translates to
  improvements on the model.
- I need to implement a reward function that executes the code generated by the model

### Tutorials

- [Implementing GRPO in TRL](https://huggingface.co/learn/llm-course/en/chapter12/4)

> The group size should be chosen based on your computational resources and the complexity of your task. For simple tasks, smaller groups (4-8) may be sufficient, while more complex reasoning tasks might benefit from larger groups (8-16).

This is very likely related to the probability of solving the task correctly, or at least to have
differences in the reward between the prompts.

- [Official documentation](https://huggingface.co/docs/trl/main/en/grpo_trainer)
- [TRL VLLM server](https://huggingface.co/docs/trl/en/vllm_integration)

Examples of reward functions:

- https://huggingface.co/learn/llm-course/en/chapter12/6?fw=pt#defining-reward-functions

#### Launching the server

```bash
# one gpu
export CUDA_VISIBLE_DEVICES=0; trl vllm-serve --max_model_len 12000 --model /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B
# multiple gpus
export CUDA_VISIBLE_DEVICES=0,1; trl vllm-serve --max_model_len 12000 --model /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B --data-parallel-size 2
```

### Modify pad token in the tokenizer configuration

Simply replace the following line on `Llama-3.1-ARC-Potpourri-Induction-8B/tokenizer_config.json`:

```bash
- "pad_token": "<|eot_id|>",
+ "pad_token": "<|finetune_right_pad_id|>",
```

### Max sequence length

I have studied all the datasets and the longest prompt is always 8635 tokens, corresponding to
tasks with 4 train samples with inputs and outputs of 30x30, and a test sample with the same input
shape.

Thus if I don't want to generate more than 2000 tokens, the max sequence length would be 10635 tokens.

## Results

### Reward is not improving

<https://wandb.ai/guillermobarbadillo/2025-09-14-debug-grpo>

On a first step I'm training on a single task to see if the reward improves over training.

![alt text](res/1757909812849_image.png)

Sadly reward doesn't seem to change. Maybe I'm using a too small learning rate. Let's use a
constant with warmup schedule and try different learning rates.

What could be explaining that the reward is not improving (model not learning):

- Learning rate might be too small.
- Maybe unsloth does not work well when using server model. However I have tried without a server and got OOM error.
- Data augmentation could be making the problem harder
- Maybe the reward is not good enough
- I might have to wait for longer
- Maybe the model needs more capacity, I could increase the LoRA size.
- Some parameter might be hurting, such as `completion_only_loss=True,`

TODO: I'm going to use a very high learning rate to try to destroy the policy.
However it did not have any effect.

Then I have tried using trl without unsloth. Then I have noticed problems in the server, it does not
seem to support LoRA. My hypothesis is that unsloth was not trying to modify the weights.

TODO: play with the smallest possible task so I can check if it works:
1. Unsloth without server
2. TRL with server

Solution: It seems that unsloth does not support the trl server, and wasn't update the server. I have
been able to train on one of the smallest task with unsloth and without server and it is clearly learning
because if I use a small learning rate the average reward stays flat, but if I use a proper learning
rate raises. I can see that with just 20 training steps, because I'm using the same task and no
data augmentation for this experiment.

![alt text](res/1757951689198_image.png)

## Conclusion

## Next steps

## TODO

- [x] How to pass the responses to the reward function? It seems that the data should be provided as a field in the dictionary
- [x] If I understand correctly each step a single problem is seen
- [ ] Should I modify the tokenizer directly in the model to avoid problems?
- [ ] 24GB of VRAM is not enough to do RL training with the sequence lengths of ARC -> Need to go to H100
  - [ ] Update environment
  - [ ] Update tokenizer conf
  - [ ] Create RL training script
    - [ ] Generator for the prompts
    - [x] Add verbose option to code evaluation
    - [ ] More smooth reward, combine test and train
- [ ] Training experiments
  - [ ] What is the configuration that better uses the hardware
  - [ ] Best learning rate
- [ ] What is the max prompt length for all the datasets available? -> Use that to better set the max_model_len of VLLM
- [ ] GPU usage is not efficient with server mode: https://huggingface.co/blog/vllm-colocate
- [ ] Kaggle scoring error
  - [ ] Add tests for create submission
  - [ ] Check problems on existing submission
  - [ ] Change priority to dataset (there might be missing tasks)
  - [ ] Maybe I'm using numpy instead of float?
