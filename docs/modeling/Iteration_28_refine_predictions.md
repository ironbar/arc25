# Iteration 28. Refine predictions

_01-09-2025_

<!---
The work is done using short iterations. Each iteration needs to have a very
clear goal. This allows to gain greater knowledge of the problem on each iteration.

<details>
  <summary>Click to expand/collapse this section</summary>
</details>
--->

## Goal

Study if asking the model to refine its prediction is helpful

## Motivation

All the evolutionary search approaches use the model to refine its most promising solutions.

I want to explore:

1. Does the GPU 25GB VRAM allow to do prediction refine with BARC induction model?
2. How much improvement do we get compared to doing independent predictions?

## Development

### Estimate the number of tokens

Without refining the longest tasks are those which have 4 training tasks of input and outputs with shape 30x30 and the test task
is also 30x30. If we consider the newline token that accounts for `8370=30*31*(4*2+1)` just for the tokens.
In my case adding the prompt increases the token count to 8650.

When we refine the token we have to add:

- Code generated by the model: 1000 tokens max
- Outputs of the training samples: 3720 tokens max

Thus without considering any message in the prompt it would be 13090 tokens. Being conservative we
could request for 13500 tokens in the refining prompt, and a total sequence length of 14500 tokens
considering that we allow to predict 1000 tokens.

### How much VRAM is needed for 14500 sequence length?

When using unsloth I need 0.75 of the 3090 VRAM to be able to make those predictions, with VLLM is enough with 0.5.

If I don't quantize the model to 4-bit then I need at least 0.8 memory with VLLM.

### Experiment design

The easiest experiment is to create a notebook where I just do solution refinement. This implies I already need
to have the solutions generated and saved to disk. Probably the easiest way is to reuse predictions
from search and learn experiments.

I could select n random unsolved predictions for each task, and compare the accuracy against the baseline
that does not use prediction refinement.

## Results

## Conclusion

## Next steps

## TODO

- [x] How much memory is needed to do refinement? Estimate the number of necessary tokens and try with VLLM
- [ ] Collect predictions from previous experiments
- [ ] Create a notebook to see experiment with solution refinement
