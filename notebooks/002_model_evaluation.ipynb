{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has the model learned to draw?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task\n",
    "from arc25.code_execution import safe_code_execution\n",
    "from arc25.utils import set_random_seed\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "base_model_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/model-6400'\n",
    "# lora_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/checkpoint-6400'\n",
    "# lora_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_5_no_dora/checkpoint-200'\n",
    "lora_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_4_no_dora_rank16/checkpoint-50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=base_model_path,\n",
    "    enable_lora=True,\n",
    "    trust_remote_code=True,\n",
    "    dtype='auto',\n",
    "    tensor_parallel_size=1, # to use 2 gpus\n",
    "    max_model_len=10240,\n",
    "    disable_log_stats=True,\n",
    "    max_num_seqs=255, # default is supposed to be 256 I have used it to solve some weird illegal memory error\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_request = LoRARequest(lora_name='lora', lora_int_id=1, lora_path=lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_generator = RandomDrawingTaskOnEmptyImg()\n",
    "prompt_version = 'code-from-examples-v3'\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(n_tasks, task_generator, sampling_params, random_seed=42):\n",
    "    set_random_seed(random_seed)\n",
    "    tasks = [task_generator.sample() for _ in range(n_tasks)]\n",
    "    prompts = [\n",
    "        create_prompt_from_task(\n",
    "            task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "        for task in tasks\n",
    "    ]\n",
    "    request_output = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "    predicted_codes = [[output.text.replace('\\n```', '') for output in task_output.outputs] for task_output in request_output]\n",
    "\n",
    "    pass_n, accuracy, mean_correct_pixels, max_correct_pixels = [], [], [], []\n",
    "    for task, task_predicted_codes in tqdm(zip(tasks, predicted_codes), total=len(tasks), desc='evaluating'):\n",
    "        predicted_outputs = [safe_code_execution(predicted_code, task.inputs) for predicted_code in task_predicted_codes]\n",
    "        pass_n.append(np.mean([any(np.all(output == predicted_output[idx]) for predicted_output in predicted_outputs)for idx, output in enumerate(task.outputs)]))\n",
    "        accuracy.append(np.mean([np.mean([np.all(output == predicted_output[idx]) for predicted_output in predicted_outputs]) for idx, output in enumerate(task.outputs)]))\n",
    "        mean_correct_pixels.append(np.mean([np.mean([np.mean(output == predicted_output[idx]) for predicted_output in predicted_outputs]) for idx, output in enumerate(task.outputs)]))\n",
    "        max_correct_pixels.append(np.mean([np.max([np.mean(output == predicted_output[idx]) for predicted_output in predicted_outputs]) for idx, output in enumerate(task.outputs)]))\n",
    "    metrics = {\n",
    "        f'acc@{sampling_params.n}': np.mean(accuracy),\n",
    "        f'pass@{sampling_params.n}': np.mean(pass_n),\n",
    "        f'mean_correct_pixels': np.mean(mean_correct_pixels),\n",
    "        f'max_correct_pixels': np.mean(max_correct_pixels),\n",
    "    }\n",
    "    metrics = {key: float(value) for key, value in metrics.items()}\n",
    "    for key, value in metrics.items():\n",
    "        print(f'{key}: {value:.2%}')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = 256\n",
    "metrics = []\n",
    "temperatures = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "for temperature in temperatures:\n",
    "    sampling_params = SamplingParams(n=8, temperature=temperature, max_tokens=1024, logprobs=0, skip_special_tokens=False)\n",
    "    print(f\"Evaluating with temperature {temperature}\")\n",
    "    metrics.append(evaluate_model(n_tasks=n_tasks, task_generator=task_generator, sampling_params=sampling_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_idx, key in enumerate(metrics[0], 1):\n",
    "    plt.subplot(1, len(metrics[0]), plot_idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(temperatures, [metric[key] for metric in metrics], marker='o')\n",
    "    plt.xlabel('temperature')\n",
    "    plt.ylabel(key)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the temperature on the metrics')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual created tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = llm.generate([prompt], sampling_params, use_tqdm=True, lora_request=lora_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 10), color=0)\n",
    "output_img = draw_rectangle(input_img.copy(), (0, 0), (4, 4), color=1)\n",
    "output_img = draw_rectangle(output_img, (5, 5), (9, 9), color=1)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "outputs = llm.generate([prompt], sampling_params, use_tqdm=True)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 10), color=0)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 2):\n",
    "    draw_vertical_line(output_img, x, color=x)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "outputs = llm.generate([prompt], sampling_params, use_tqdm=True)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 10), color=0)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 1):\n",
    "    draw_vertical_line(output_img, x, color=x)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "outputs = llm.generate([prompt], sampling_params, use_tqdm=True)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 10), color=0)\n",
    "draw_horizontal_line(input_img, 4, color=1)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 2):\n",
    "    draw_vertical_line(output_img, x, color=x)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "outputs = llm.generate([prompt], sampling_params, use_tqdm=True)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is limited by the number of drawing functions in the train set\n",
    "- It has only been trained with blank images, that does not require a good comparison between the images.\n",
    "\n",
    "TODO:\n",
    "\n",
    "- I want to visualize the transformation of the code.\n",
    "- Also compute some metrics of accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
