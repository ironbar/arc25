{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has the model learned to draw?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task\n",
    "from arc25.code_execution import safe_code_execution\n",
    "from arc25.utils import set_random_seed\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "base_model_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/model-6400'\n",
    "# base_model_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_3200/model-3200'\n",
    "# base_model_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_1600/model-1600'\n",
    "# base_model_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_800/model-800'\n",
    "# base_model_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_400/model-400'\n",
    "# base_model_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_9/model-200'\n",
    "# lora_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/checkpoint-6400'\n",
    "# lora_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_5_no_dora/checkpoint-200'\n",
    "lora_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_4_no_dora_rank16/checkpoint-50'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=base_model_path,\n",
    "    enable_lora=True,\n",
    "    trust_remote_code=True,\n",
    "    dtype='auto',\n",
    "    tensor_parallel_size=1, # to use 2 gpus\n",
    "    max_model_len=10240,\n",
    "    disable_log_stats=True,\n",
    "    max_num_seqs=255, # default is supposed to be 256 I have used it to solve some weird illegal memory error\n",
    "    enforce_eager=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_request = LoRARequest(lora_name='lora', lora_int_id=1, lora_path=lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_version = 'code-from-examples-v3'\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(n_tasks, task_generator, sampling_params, random_seed=42, verbose=False):\n",
    "    set_random_seed(random_seed)\n",
    "    tasks = [task_generator.sample() for _ in range(n_tasks)]\n",
    "    prompts = [\n",
    "        create_prompt_from_task(\n",
    "            task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "        for task in tasks\n",
    "    ]\n",
    "    request_output = llm.generate(prompts, sampling_params, use_tqdm=True)\n",
    "    predicted_codes = [[output.text.replace('\\n```', '') for output in task_output.outputs] for task_output in request_output]\n",
    "\n",
    "    pass_n, accuracy, mean_correct_pixels, max_correct_pixels, valid_predictions = [], [], [], [], []\n",
    "    for task, task_predicted_codes in tqdm(zip(tasks, predicted_codes), total=len(tasks), desc='evaluating'):\n",
    "        predicted_outputs = []\n",
    "        for predicted_code in task_predicted_codes:\n",
    "            try:\n",
    "                predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "                predicted_outputs.append(predicted_output)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f'Error executing code: {predicted_code}')\n",
    "                    print(e)\n",
    "        if not predicted_outputs:\n",
    "            print(f'No valid outputs for task {task}')\n",
    "            pass_n.append(0)\n",
    "            accuracy.append(0)\n",
    "            mean_correct_pixels.append(0)\n",
    "            max_correct_pixels.append(0)\n",
    "            valid_predictions.append(0)\n",
    "            continue\n",
    "        valid_predictions.append(len(predicted_outputs)/len(task_predicted_codes))\n",
    "        pass_n.append(np.mean([any(np.all(output == predicted_output[idx]) for predicted_output in predicted_outputs)for idx, output in enumerate(task.outputs)]))\n",
    "        accuracy.append(np.mean([np.mean([np.all(output == predicted_output[idx]) for predicted_output in predicted_outputs]) for idx, output in enumerate(task.outputs)]))\n",
    "        mean_correct_pixels.append(np.mean([np.mean([np.mean(output == predicted_output[idx]) for predicted_output in predicted_outputs]) for idx, output in enumerate(task.outputs)]))\n",
    "        max_correct_pixels.append(np.mean([np.max([np.mean(output == predicted_output[idx]) for predicted_output in predicted_outputs]) for idx, output in enumerate(task.outputs)]))\n",
    "    metrics = {\n",
    "        f'acc@{sampling_params.n}': np.mean(accuracy),\n",
    "        f'pass@{sampling_params.n}': np.mean(pass_n),\n",
    "        f'mean_correct_pixels': np.mean(mean_correct_pixels),\n",
    "        f'max_correct_pixels': np.mean(max_correct_pixels),\n",
    "        'valid_predictions': np.mean(valid_predictions),\n",
    "    }\n",
    "    metrics = {key: float(value) for key, value in metrics.items()}\n",
    "    for key, value in metrics.items():\n",
    "        print(f'{key}: {value:.2%}')\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws = range(1, 10)\n",
    "metrics = []\n",
    "for n in n_draws:\n",
    "    task_generator = RandomDrawingTaskOnEmptyImg(min_draws=n, max_draws=n)\n",
    "    sampling_params = SamplingParams(n=8, temperature=0.5, top_p=0.95, max_tokens=1024, logprobs=0, skip_special_tokens=False)\n",
    "    print(f'Running {task_generator.__class__.__name__} with {n} draws')\n",
    "    metrics.append(evaluate_model(n_tasks=128, task_generator=task_generator, sampling_params=sampling_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_idx, key in enumerate(metrics[0], 1):\n",
    "    plt.subplot(1, len(metrics[0]), plot_idx)\n",
    "    plt.title(key)\n",
    "    values = [metric[key] for metric in metrics]\n",
    "    plt.plot(n_draws, values, marker='o')\n",
    "    plt.fill_between([1, 5], np.min(values), np.max(values), color='green', alpha=0.2)\n",
    "    plt.fill_between([5, 9], np.min(values), np.max(values), color='orange', alpha=0.2)\n",
    "    plt.xlabel('temperature')\n",
    "    plt.ylabel(key)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the number of drawings on the metrics')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=8, temperature=0.5, top_p=0.95, max_tokens=1024, logprobs=0, skip_special_tokens=False)\n",
    "metrics = evaluate_model(n_tasks=512, task_generator=task_generator, sampling_params=sampling_params)\n",
    "print(base_model_path)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = 512\n",
    "metrics = []\n",
    "temperatures = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.4]\n",
    "for temperature in temperatures:\n",
    "    sampling_params = SamplingParams(n=8, temperature=temperature, top_p=0.95, max_tokens=1024, logprobs=0, skip_special_tokens=False)\n",
    "    print(f\"Evaluating with temperature {temperature}\")\n",
    "    metrics.append(evaluate_model(n_tasks=n_tasks, task_generator=task_generator, sampling_params=sampling_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_idx, key in enumerate(metrics[0], 1):\n",
    "    plt.subplot(1, len(metrics[0]), plot_idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(temperatures, [metric[key] for metric in metrics], marker='o')\n",
    "    plt.xlabel('temperature')\n",
    "    plt.ylabel(key)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the temperature on the metrics')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_range = [1, 2, 4, 8, 16, 32, 64]\n",
    "metrics = []\n",
    "for n in n_range:\n",
    "    sampling_params = SamplingParams(n=n, temperature=0.5, top_p=0.95, max_tokens=1024, logprobs=0, skip_special_tokens=False)\n",
    "    print(f\"Evaluating with n={n}\")\n",
    "    metrics.append(evaluate_model(n_tasks=n_tasks, task_generator=task_generator, sampling_params=sampling_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(n_range, [metrics[idx][f'pass@{n}'] for idx, n in enumerate(n_range)], marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xticks(n_range, n_range);\n",
    "plt.grid()\n",
    "plt.title('Effect of the n on the pass rate')\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('pass rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plot_idx, key in enumerate(metrics[0], 1):\n",
    "    plt.subplot(1, len(metrics[0]), plot_idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(n_range, [metric[key] for metric in metrics], marker='o')\n",
    "    plt.xlabel('n')\n",
    "    plt.ylabel(key)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the number of predictions on the metrics')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_9/model-200\n",
    "{'acc@8': 0.278076171875, 'pass@8': 0.45703125, 'mean_correct_pixels': 0.8456284646562617, 'max_correct_pixels': 0.9308726824786628, 'valid_predictions': 0.999755859375}\n",
    "/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_400/model-400\n",
    "{'acc@8': 0.45166015625, 'pass@8': 0.658203125, 'mean_correct_pixels': 0.9149275320163149, 'max_correct_pixels': 0.9706963654602466, 'valid_predictions': 1.0}\n",
    "/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_800/model-800\n",
    "{'acc@8': 0.601318359375, 'pass@8': 0.7890625, 'mean_correct_pixels': 0.951842735600009, 'max_correct_pixels': 0.9852759201192052, 'valid_predictions': 1.0}\n",
    "/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_1600/model-1600\n",
    "{'acc@8': 0.7294921875, 'pass@8': 0.89453125, 'mean_correct_pixels': 0.9746637164726459, 'max_correct_pixels': 0.9936216837718884, 'valid_predictions': 1.0}\n",
    "/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_3200/model-3200\n",
    "{'acc@8': 0.826416015625, 'pass@8': 0.9453125, 'mean_correct_pixels': 0.9869367756823143, 'max_correct_pixels': 0.997600836413848, 'valid_predictions': 1.0}\n",
    "/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/model-6400\n",
    "{'acc@8': 0.875732421875, 'pass@8': 0.966796875, 'mean_correct_pixels': 0.9911085811662086, 'max_correct_pixels': 0.9987733240721057, 'valid_predictions': 1.0}\n",
    "\"\"\"\n",
    "metrics = {\n",
    "    200: {'acc@8': 0.278076171875, 'pass@8': 0.45703125, 'mean_correct_pixels': 0.8456284646562617, 'max_correct_pixels': 0.9308726824786628, 'valid_predictions': 0.999755859375},\n",
    "    400: {'acc@8': 0.45166015625, 'pass@8': 0.658203125, 'mean_correct_pixels': 0.9149275320163149, 'max_correct_pixels': 0.9706963654602466, 'valid_predictions': 1.0},\n",
    "    800: {'acc@8': 0.601318359375, 'pass@8': 0.7890625, 'mean_correct_pixels': 0.951842735600009, 'max_correct_pixels': 0.9852759201192052, 'valid_predictions': 1.0},\n",
    "    1600: {'acc@8': 0.7294921875, 'pass@8': 0.89453125, 'mean_correct_pixels': 0.9746637164726459, 'max_correct_pixels': 0.9936216837718884, 'valid_predictions': 1.0},\n",
    "    3200: {'acc@8': 0.826416015625, 'pass@8': 0.9453125, 'mean_correct_pixels': 0.9869367756823143, 'max_correct_pixels': 0.997600836413848, 'valid_predictions': 1.0},\n",
    "    6400: {'acc@8': 0.875732421875, 'pass@8': 0.966796875, 'mean_correct_pixels': 0.9911085811662086, 'max_correct_pixels': 0.9987733240721057, 'valid_predictions': 1.0},\n",
    "}\n",
    "training_steps = sorted(metrics.keys())\n",
    "for plot_idx, key in enumerate(metrics[training_steps[0]], 1):\n",
    "    plt.subplot(1, len(metrics[training_steps[0]]), plot_idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(training_steps, [metrics[steps][key] for steps in training_steps], marker='o')\n",
    "    plt.xlabel('training_steps')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel(key)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the training_steps on the metrics')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    200: {'acc@8': 0.278076171875, 'pass@8': 0.45703125, 'mean_correct_pixels': 0.8456284646562617, 'max_correct_pixels': 0.9308726824786628, 'valid_predictions': 0.999755859375},\n",
    "    400: {'acc@8': 0.45166015625, 'pass@8': 0.658203125, 'mean_correct_pixels': 0.9149275320163149, 'max_correct_pixels': 0.9706963654602466, 'valid_predictions': 1.0},\n",
    "    800: {'acc@8': 0.601318359375, 'pass@8': 0.7890625, 'mean_correct_pixels': 0.951842735600009, 'max_correct_pixels': 0.9852759201192052, 'valid_predictions': 1.0},\n",
    "    1600: {'acc@8': 0.7294921875, 'pass@8': 0.89453125, 'mean_correct_pixels': 0.9746637164726459, 'max_correct_pixels': 0.9936216837718884, 'valid_predictions': 1.0},\n",
    "    3200: {'acc@8': 0.826416015625, 'pass@8': 0.9453125, 'mean_correct_pixels': 0.9869367756823143, 'max_correct_pixels': 0.997600836413848, 'valid_predictions': 1.0},\n",
    "    6400: {'acc@8': 0.875732421875, 'pass@8': 0.966796875, 'mean_correct_pixels': 0.9911085811662086, 'max_correct_pixels': 0.9987733240721057, 'valid_predictions': 1.0},\n",
    "}\n",
    "training_steps = sorted(metrics.keys())\n",
    "training_samples = [steps*16 for steps in training_steps] # batch size is 16\n",
    "for plot_idx, key in enumerate(metrics[training_steps[0]], 1):\n",
    "    plt.subplot(1, len(metrics[training_steps[0]]), plot_idx)\n",
    "    plt.title(key)\n",
    "    plt.plot(training_samples, [metrics[steps][key] for steps in training_steps], marker='o')\n",
    "    plt.xlabel('training samples')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel(key)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the training samples on the metrics')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual created tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = llm.generate([prompt], sampling_params, use_tqdm=True, lora_request=lora_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 10), color=0)\n",
    "output_img = draw_rectangle(input_img.copy(), (0, 0), (4, 4), color=1)\n",
    "output_img = draw_rectangle(output_img, (5, 5), (9, 9), color=1)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "outputs = llm.generate([prompt], sampling_params, use_tqdm=True)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 10), color=0)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 2):\n",
    "    draw_vertical_line(output_img, x, color=x)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "outputs = llm.generate([prompt], sampling_params, use_tqdm=True)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 10), color=0)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 1):\n",
    "    draw_vertical_line(output_img, x, color=x)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "outputs = llm.generate([prompt], sampling_params, use_tqdm=True)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 10), color=0)\n",
    "draw_horizontal_line(input_img, 4, color=1)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 2):\n",
    "    draw_vertical_line(output_img, x, color=x)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "outputs = llm.generate([prompt], sampling_params, use_tqdm=True)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is limited by the number of drawing functions in the train set\n",
    "- It has only been trained with blank images, that does not require a good comparison between the images.\n",
    "\n",
    "TODO:\n",
    "\n",
    "- I want to visualize the transformation of the code.\n",
    "- Also compute some metrics of accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
