{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d90467",
   "metadata": {},
   "source": [
    "# Test-time Training Exploration: Hindsight Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944074db",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f49b5",
   "metadata": {},
   "source": [
    "Can I solve tasks using test-time training?\n",
    "\n",
    "I want to explore different TTT techniques such as hindsight experience replay and RL to see if a model can solve novel tasks that cannot be solve with the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580a327",
   "metadata": {},
   "source": [
    "I have to focus on the techniques, not on efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d84226",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b9cf8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task, plot_grids_with_shape\n",
    "from arc25.code_execution import safe_code_execution\n",
    "from arc25.utils import set_random_seed\n",
    "from arc25.logging import configure_logging\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"../scripts\"))\n",
    "from finetuning import get_data_collator\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f6f77",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindsight_experience_replay(task, cfg):\n",
    "    \"\"\"\n",
    "    Use hindsight experience replay to try to solve new tasks\n",
    "    \"\"\"\n",
    "    plot_task(task); plt.suptitle('Task to solve'); plt.show()\n",
    "    model, tokenizer = load_model(cfg.base_model_path, cfg.lora_path)\n",
    "    epoch_accuracies = []\n",
    "    for epoch in range(cfg.max_epochs):\n",
    "        logging.info(f'Starting epoch {epoch}...')\n",
    "        new_tasks, pixel_accuracies = inference(task, model, tokenizer, cfg.grid_encoder, cfg.prompt_version)\n",
    "        epoch_accuracies.append(pixel_accuracies)\n",
    "        plot_best_prediction(task, new_tasks[-1])\n",
    "        if np.max(pixel_accuracies) == 1:\n",
    "            logger.info(f'Found a perfect prediction at epoch {epoch}!')\n",
    "            break\n",
    "        finetuning(new_tasks, model, tokenizer, cfg.grid_encoder, cfg.prompt_version)\n",
    "\n",
    "    for epoch, pixel_accuracies in enumerate(epoch_accuracies):\n",
    "        plt.hist(pixel_accuracies, bins=np.linspace(0, 1, 20), label=f'Epoch {epoch}')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Pixel accuracy')\n",
    "    plt.ylabel('Number of predictions')\n",
    "    plt.title('Evolution of pixel accuracy')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_model(base_model_path, lora_path):\n",
    "    logging.info(f\"Loading model from {base_model_path} and LoRA from {lora_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "    model = PeftModel.from_pretrained(model, lora_path, is_trainable=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def inference(task, model, tokenizer, grid_encoder, prompt_version):\n",
    "    prompt = create_prompt_from_task(\n",
    "        task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=256\n",
    "    )\n",
    "    generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "    print(f'Generated ids shape: {generated_ids.shape}')\n",
    "    predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    predicted_codes = [prediction.replace('\\n```', '') for prediction in predictions]\n",
    "    new_tasks = []\n",
    "    pixel_accuracies = []\n",
    "    for predicted_code in tqdm(predicted_codes):\n",
    "        try:\n",
    "            predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "            new_tasks.append(Task(inputs=task.inputs, outputs=predicted_output, code=predicted_code, name=task.name))\n",
    "            pixel_accuracies.append(float(np.mean(new_tasks[-1].outputs[0] == task.outputs[0])))\n",
    "        except Exception as e:\n",
    "                print(f'Error executing code: {predicted_code}')\n",
    "                print(e)\n",
    "    plt.hist(pixel_accuracies, bins=np.linspace(0, 1, 20), log=True, label='all predictions');\n",
    "    new_tasks_with_unique_outputs = [new_tasks[0]]\n",
    "    for new_task in new_tasks[1:]:\n",
    "        if not any([np.all(new_task.outputs[0] == t.outputs[0]) for t in new_tasks_with_unique_outputs]):\n",
    "            new_tasks_with_unique_outputs.append(new_task)\n",
    "    print(f'Number of unique outputs: {len(new_tasks_with_unique_outputs)}')\n",
    "\n",
    "    pixel_accuracies = []\n",
    "    for new_task in tqdm(new_tasks_with_unique_outputs):\n",
    "        pixel_accuracies.append(float(np.mean(new_task.outputs[0] == task.outputs[0])))\n",
    "    print(f'Max pixel accuracy: {max(pixel_accuracies)}')\n",
    "\n",
    "    plt.hist(pixel_accuracies, bins=np.linspace(0, 1, 20), log=True, label='unique predictions', alpha=0.5);\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    new_tasks_with_unique_outputs = sorted(new_tasks_with_unique_outputs, key=lambda x: float(np.mean(x.outputs[0] == task.outputs[0])), reverse=False)\n",
    "    return new_tasks_with_unique_outputs, pixel_accuracies\n",
    "\n",
    "\n",
    "def finetuning(new_tasks, model, tokenizer, grid_encoder, prompt_version):\n",
    "    prompts = []\n",
    "    for task in new_tasks:\n",
    "        prompts.append(create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=True))\n",
    "    train_dataset = Dataset.from_dict({'text': prompts})\n",
    "\n",
    "    training_arguments = SFTConfig(\n",
    "        output_dir='/mnt/hdd0/Kaggle/arc25/trainings/20250505_TTT/debug',\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='constant_with_warmup', #constant_with_warmup, cosine, cosine_with_restarts\n",
    "        # lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "        gradient_checkpointing=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        max_grad_norm=1.0,\n",
    "\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=4096,\n",
    "\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"no\", #TODO: previously it was steps\n",
    "        # save_steps=cfg.save_steps or cfg.eval_steps,\n",
    "        logging_steps=10, #50,\n",
    "        log_level=\"info\",\n",
    "        report_to='none',\n",
    "\n",
    "        # parameters added to make the code work with accelerate\n",
    "        # dispatch_batches=False,\n",
    "        # https://huggingface.co/transformers/v4.9.1/main_classes/trainer.html#trainingarguments\n",
    "        ddp_find_unused_parameters=False, # only used with accelerate, got a warning saying that it slows down if True\n",
    "\n",
    "        ignore_data_skip=True, # otherwise it takes too long to start training when resuming from checkpoint\n",
    "\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=get_data_collator(tokenizer),\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "def plot_best_prediction(task, best_prediction):\n",
    "    plot_grids_with_shape(task.outputs + best_prediction.outputs, suptitle='Best prediction')\n",
    "    display(Markdown(f'```python\\n{best_prediction.code}\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c4370",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    base_model_path = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "    lora_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/checkpoint-6400'\n",
    "    prompt_version = 'code-from-examples-v3'\n",
    "    grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "    max_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28754bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((9, 9), color=0)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 1):\n",
    "    draw_vertical_line(output_img, x, color=x+1)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "hindsight_experience_replay(task, Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((9, 9), color=0)\n",
    "output_img = input_img.copy()\n",
    "color = 0\n",
    "for x in range(0, input_img.shape[1], 3):\n",
    "    for y in range(0, input_img.shape[0], 3):\n",
    "        color += 1\n",
    "        draw_rectangle(output_img, (x, y), (x+2, y+2), color=color)\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "hindsight_experience_replay(task, max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((3, 4), color=0)\n",
    "output_img = input_img.copy()\n",
    "color = 0\n",
    "for y in range(0, input_img.shape[0], 1):\n",
    "    for x in range(0, input_img.shape[1], 1):\n",
    "        color = (color + 1) % 10\n",
    "        if color == 0: color = 1\n",
    "        draw_pixel(output_img, (y, x), color=color)\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "hindsight_experience_replay(task, Config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987cfe25",
   "metadata": {},
   "source": [
    "| batch size | inference time(s) | throughput (preds/s) |\n",
    "|------------|-------------------|----------------------|\n",
    "| 1          | 6.4               | 0.2                  |\n",
    "| 4          | 7.4               | 0.5                  |\n",
    "| 16         | 8.5               | 1.9                  |\n",
    "| 64         | 9                 | 7.1                  |\n",
    "| 128        | 10.9              | 11.7                 |\n",
    "| 256        | 15.3              | 16.7                 |\n",
    "| 512        | 30.1              | 17.0                 |\n",
    "\n",
    "A batch size of 256 might be the sweet spot. It takes just twice as making two predictions with batch size 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055461c5",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab80f11",
   "metadata": {},
   "source": [
    "- Encapsulate steps into function\n",
    "- Try on other tasks (I might think of a more complex tasks with pixels)\n",
    "- Parametrize batch size\n",
    "- print successfull code\n",
    "- Better progress visualization\n",
    "- Stop criteria"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
