{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d90467",
   "metadata": {},
   "source": [
    "# Test-time Training Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944074db",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f49b5",
   "metadata": {},
   "source": [
    "Can I solve tasks using test-time training?\n",
    "\n",
    "I want to explore different TTT techniques such as hindsight experience replay and RL to see if a model can solve novel tasks that cannot be solve with the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580a327",
   "metadata": {},
   "source": [
    "I have to focus on the techniques, not on efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d84226",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b9cf8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task\n",
    "from arc25.code_execution import safe_code_execution\n",
    "from arc25.utils import set_random_seed\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"../scripts\"))\n",
    "from finetuning import get_data_collator\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a06f3",
   "metadata": {},
   "source": [
    "### Task definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1548746",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((9, 9), color=0)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 1):\n",
    "    draw_vertical_line(output_img, x, color=x+1)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fc121",
   "metadata": {},
   "source": [
    "## Hindsight Experience Replay (HER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11dec42",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a020b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "lora_path = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/checkpoint-6400'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "model = PeftModel.from_pretrained(model, lora_path, is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_version = 'code-from-examples-v3'\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25222da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3222755",
   "metadata": {},
   "source": [
    "### Verify that task is not solvable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff535ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=256\n",
    ")\n",
    "generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "print(f'Generated ids shape: {generated_ids.shape}')\n",
    "predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa670c9a",
   "metadata": {},
   "source": [
    "| batch size | inference time(s) | throughput (preds/s) |\n",
    "|------------|-------------------|----------------------|\n",
    "| 1          | 6.4               | 0.2                  |\n",
    "| 4          | 7.4               | 0.5                  |\n",
    "| 16         | 8.5               | 1.9                  |\n",
    "| 64         | 9                 | 7.1                  |\n",
    "| 128        | 10.9              | 11.7                 |\n",
    "| 256        | 15.3              | 16.7                 |\n",
    "| 512        | 30.1              | 17.0                 |\n",
    "\n",
    "A batch size of 256 might be the sweet spot. It takes just twice as making two predictions with batch size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8964ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_codes = [prediction.replace('\\n```', '') for prediction in predictions]\n",
    "new_tasks = []\n",
    "for predicted_code in tqdm(predicted_codes):\n",
    "    try:\n",
    "        predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "        new_tasks.append(Task(inputs=task.inputs, outputs=predicted_output, code=predicted_code, name=task.name))\n",
    "    except Exception as e:\n",
    "            print(f'Error executing code: {predicted_code}')\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tasks_with_unique_outputs = [new_tasks[0]]\n",
    "for new_task in new_tasks[1:]:\n",
    "    if not any([np.all(new_task.outputs[0] == t.outputs[0]) for t in new_tasks_with_unique_outputs]):\n",
    "        new_tasks_with_unique_outputs.append(new_task)\n",
    "print(f'Number of unique outputs: {len(new_tasks_with_unique_outputs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e770ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_accuracies = []\n",
    "for new_task in tqdm(new_tasks_with_unique_outputs):\n",
    "    pixel_accuracies.append(float(np.mean(new_task.outputs[0] == task.outputs[0])))\n",
    "print(f'Max pixel accuracy: {max(pixel_accuracies)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b62157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pixel_accuracies, bins=20, log=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tasks_with_unique_outputs = sorted(new_tasks_with_unique_outputs, key=lambda x: float(np.mean(x.outputs[0] == task.outputs[0])), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cbbafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task(new_tasks_with_unique_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9491e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task(new_tasks_with_unique_outputs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a25a036",
   "metadata": {},
   "source": [
    "This probes that if we use the fine-tuned model as it is it is not capable of solving the task. What if we further fine-tune it on the new tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb40db4",
   "metadata": {},
   "source": [
    "### HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95951779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hindsight_experience_replay(task, max_epochs=10):\n",
    "    \"\"\"\n",
    "    Use hindsight experience replay to try to solve new tasks\n",
    "    \"\"\"\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f'Epoch {epoch}')\n",
    "        new_tasks = inference(task)\n",
    "        finetuning(new_tasks)\n",
    "\n",
    "\n",
    "def inference(task):\n",
    "    prompt = create_prompt_from_task(\n",
    "        task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=256\n",
    "    )\n",
    "    generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "    print(f'Generated ids shape: {generated_ids.shape}')\n",
    "    predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    predicted_codes = [prediction.replace('\\n```', '') for prediction in predictions]\n",
    "    new_tasks = []\n",
    "    pixel_accuracies = []\n",
    "    for predicted_code in tqdm(predicted_codes):\n",
    "        try:\n",
    "            predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "            new_tasks.append(Task(inputs=task.inputs, outputs=predicted_output, code=predicted_code, name=task.name))\n",
    "            pixel_accuracies.append(float(np.mean(new_tasks[-1].outputs[0] == task.outputs[0])))\n",
    "        except Exception as e:\n",
    "                print(f'Error executing code: {predicted_code}')\n",
    "                print(e)\n",
    "    plt.hist(pixel_accuracies, bins=np.linspace(0, 1, 20), log=True, label='all predictions');\n",
    "    new_tasks_with_unique_outputs = [new_tasks[0]]\n",
    "    for new_task in new_tasks[1:]:\n",
    "        if not any([np.all(new_task.outputs[0] == t.outputs[0]) for t in new_tasks_with_unique_outputs]):\n",
    "            new_tasks_with_unique_outputs.append(new_task)\n",
    "    print(f'Number of unique outputs: {len(new_tasks_with_unique_outputs)}')\n",
    "\n",
    "    pixel_accuracies = []\n",
    "    for new_task in tqdm(new_tasks_with_unique_outputs):\n",
    "        pixel_accuracies.append(float(np.mean(new_task.outputs[0] == task.outputs[0])))\n",
    "    print(f'Max pixel accuracy: {max(pixel_accuracies)}')\n",
    "\n",
    "    plt.hist(pixel_accuracies, bins=np.linspace(0, 1, 20), log=True, label='unique predictions', alpha=0.5);\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    new_tasks_with_unique_outputs = sorted(new_tasks_with_unique_outputs, key=lambda x: float(np.mean(x.outputs[0] == task.outputs[0])), reverse=False)\n",
    "    return new_tasks_with_unique_outputs\n",
    "\n",
    "\n",
    "def finetuning(new_tasks):\n",
    "    prompts = []\n",
    "    for task in new_tasks:\n",
    "        prompts.append(create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=True))\n",
    "    train_dataset = Dataset.from_dict({'text': prompts})\n",
    "\n",
    "    training_arguments = SFTConfig(\n",
    "        output_dir='/mnt/hdd0/Kaggle/arc25/trainings/20250505_TTT/debug',\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='constant_with_warmup', #constant_with_warmup, cosine, cosine_with_restarts\n",
    "        # lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "        gradient_checkpointing=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        max_grad_norm=1.0,\n",
    "\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=4096,\n",
    "\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"no\", #TODO: previously it was steps\n",
    "        # save_steps=cfg.save_steps or cfg.eval_steps,\n",
    "        logging_steps=10, #50,\n",
    "        log_level=\"info\",\n",
    "        report_to='none',\n",
    "\n",
    "        # parameters added to make the code work with accelerate\n",
    "        # dispatch_batches=False,\n",
    "        # https://huggingface.co/transformers/v4.9.1/main_classes/trainer.html#trainingarguments\n",
    "        ddp_find_unused_parameters=False, # only used with accelerate, got a warning saying that it slows down if True\n",
    "\n",
    "        ignore_data_skip=True, # otherwise it takes too long to start training when resuming from checkpoint\n",
    "\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=get_data_collator(tokenizer),\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28754bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindsight_experience_replay(task, max_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((9, 9), color=0)\n",
    "output_img = input_img.copy()\n",
    "color = 0\n",
    "for x in range(0, input_img.shape[1], 3):\n",
    "    for y in range(0, input_img.shape[0], 3):\n",
    "        color += 1\n",
    "        draw_rectangle(output_img, (x, y), (x+2, y+2), color=color)\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task)\n",
    "hindsight_experience_replay(task, max_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055461c5",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab80f11",
   "metadata": {},
   "source": [
    "- Encapsulate steps into function\n",
    "- Try on other tasks (I might think of a more complex tasks with pixels)\n",
    "- Parametrize batch size\n",
    "- print successfull code\n",
    "- Better progress visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
