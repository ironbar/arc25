{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d90467",
   "metadata": {},
   "source": [
    "# GRPO exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944074db",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f49b5",
   "metadata": {},
   "source": [
    "Can we solve novel tasks using GRPO?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b9cf8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "from IPython.display import Markdown, display\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task, plot_grids_with_shape, plot_grid\n",
    "from arc25.code_execution import safe_code_execution, validate_code\n",
    "from arc25.utils import set_random_seed, get_timestamp\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"../scripts\"))\n",
    "from finetuning import get_data_collator\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"def task(img):\n",
    "    import time\n",
    "    time.sleep(2)\n",
    "\"\"\"\n",
    "try:\n",
    "    safe_code_execution(code, [create_img((3, 3))])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Code execution failed.\")\n",
    "# this works, I don't understand why it does not in the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f6f77",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(base_model_path, lora_path):\n",
    "    logging.info(f\"Loading model from {base_model_path} and LoRA from {lora_path}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "    model = PeftModel.from_pretrained(model, lora_path, is_trainable=True)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c4370",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    base_model_path: str = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "    lora_path: str = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/checkpoint-6400'\n",
    "    prompt_version: str = 'code-from-examples-v3'\n",
    "    grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "    max_epochs: int = 10\n",
    "    n_predictions: int = 256 # 256 seems to be the best for my hardware\n",
    "    use_accuracy_for_sorting: bool = True\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((9, 9), color=0)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 1):\n",
    "    draw_vertical_line(output_img, x, color=x+1)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='9-vertical-lines')\n",
    "plot_task(task); plt.suptitle('Task to solve'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((10, 8), color=0)\n",
    "output_img = input_img.copy()\n",
    "color = 0\n",
    "for x in range(0, input_img.shape[1], 2):\n",
    "    for y in range(0, input_img.shape[0], 2):\n",
    "        color = (color + 1) % 10\n",
    "        if color == 0: color = 1\n",
    "        draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='20-squares')\n",
    "plot_task(task); plt.suptitle('Task to solve'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(cfg.base_model_path, cfg.lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb08437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a little bit tricky but since we are not using SFT I should set is_train_prompt=False\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=cfg.prompt_version, grid_encoder=cfg.grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "train_dataset = Dataset.from_dict({'prompt': [prompt]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd34ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_completions, all_rewards = [], []\n",
    "\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    all_completions.append(completions)\n",
    "    return [len(completion) for completion in completions]\n",
    "\n",
    "\n",
    "def reward_accuracy(completions, **kwargs):\n",
    "    all_completions.append(completions)\n",
    "    accuracy = []\n",
    "    for completion in completions:\n",
    "        predicted_code = completion.replace('\\n```', '')\n",
    "        try:\n",
    "            predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "            accuracy.append(float(np.mean(predicted_output[0] == task.outputs[0])))\n",
    "        except Exception as e:\n",
    "            print(f'Error executing code: {predicted_code}')\n",
    "            print(e)\n",
    "            accuracy.append(0)\n",
    "    all_rewards.append(accuracy)\n",
    "    return accuracy\n",
    "\n",
    "learning_rate = 1e-6\n",
    "num_generations = 8\n",
    "num_train_epochs = 400\n",
    "per_device_train_batch_size = 4\n",
    "output_dir = f\"/mnt/hdd0/Kaggle/arc25/trainings/20250508_GRPO/{get_timestamp()}_{task.name}_lr{learning_rate:.0e}_n{num_generations}\"\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=1,\n",
    "    num_generations=num_generations, # default is 8\n",
    "    gradient_accumulation_steps=num_generations//per_device_train_batch_size,\n",
    "    per_device_train_batch_size=per_device_train_batch_size, # default is 8\n",
    "    generation_batch_size=num_generations, # needs the latest version of trl\n",
    "    temperature=0.5, # default is 0.9\n",
    "    top_p=0.95, # default i 1.0\n",
    "    max_completion_length=2048, # default is 256\n",
    "    max_prompt_length=8192, # default is 512\n",
    "    learning_rate=learning_rate, # default is 1e-6\n",
    "    lr_scheduler_type='constant' # default is 'linear'\n",
    "    log_completions=True,\n",
    ")\n",
    "\n",
    "os.environ['WANDB_PROJECT'] = \"20250508_GRPO\"\n",
    "os.environ['WANDB_NAME'] = os.path.basename(output_dir)\n",
    "os.environ['WANDB_JOB_NAME'] = os.path.basename(output_dir)\n",
    "os.environ['WANDB_DIR'] = output_dir\n",
    "\n",
    "import wandb\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class RewardExtremaCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is None or not all_rewards:\n",
    "            return control\n",
    "\n",
    "        # Grab the last raw batch of rewards\n",
    "        last_batch = all_rewards[-1]\n",
    "        rmax, rmin = float(np.max(last_batch)), float(np.min(last_batch))\n",
    "\n",
    "        # 1) Inject into the HF logs dict so HF will record them\n",
    "        logs[\"train/reward_accuracy/max\"] = rmax\n",
    "        logs[\"train/reward_accuracy/min\"] = rmin\n",
    "        logging.info(f\"Step {state.global_step}: Max reward: {rmax:.2f}, Min reward: {rmin:.2f}\")\n",
    "\n",
    "        # 2) Immediately push to W&B for real-time curves\n",
    "        wandb.log({\n",
    "            \"train/reward_accuracy/max\": rmax,\n",
    "            \"train/reward_accuracy/min\": rmin,\n",
    "        })\n",
    "\n",
    "        return control\n",
    "\n",
    "class StopOnRewardCallback(TrainerCallback):\n",
    "    def __init__(self, threshold=1.0):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_step_end(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        last_batch = all_rewards[-1]\n",
    "        if np.max(last_batch) >= self.threshold:\n",
    "            logger.info(f\"Stopping training at step {state.global_step} as max reward {np.max(last_batch):.2f} is above threshold {self.threshold:.2f}\")\n",
    "            control.should_training_stop = True\n",
    "            control.should_save = True\n",
    "        return control\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=reward_accuracy,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[StopOnRewardCallback(threshold=1.0), RewardExtremaCallback()],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffac05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GRPOConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of completions: {len(all_completions)}')\n",
    "np.unique([len(completion) for completion in all_completions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.mean([len(completion) for completion in completions]) for completions in all_completions])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean completion length')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_completions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751958f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(all_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.max(accuracy) for accuracy in all_rewards])\n",
    "plt.plot([np.mean(accuracy) for accuracy in all_rewards])\n",
    "plt.plot([np.min(accuracy) for accuracy in all_rewards])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([np.max(accuracy) == 1 for accuracy in all_rewards])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac767d10",
   "metadata": {},
   "source": [
    "## Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd44ff1",
   "metadata": {},
   "source": [
    "Dummy reward experiments:\n",
    "\n",
    "- Running a first dummy train with a reward based on output length for 100 epochs took around 21 minutes\n",
    "- A second run after fixing the initial prompt took 17 minutes\n",
    "- The effective batch size has to be greater or equal to the number of generations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e515e4",
   "metadata": {},
   "source": [
    "True reward experiments:\n",
    "\n",
    "- 100 epochs takes 13 min\n",
    "- 200 epochs takes around 30 min, and after epoch 100 almost always generates the correct solution\n",
    "- Increasing the learning rate to 2e-6 results on solving the task on epoch 20\n",
    "- Increasing the learning rate to 4e-6 results on solving the task on epoch 11\n",
    "- With a learning rate of 1e-5 it is solved at epoch 3, but collapses at epoch 20\n",
    "- when trying the 16 squares task I get OOM\n",
    "- I have increased the number of generations to 128, but it diverges on the 16 squares task. It has run\n",
    "  for 46 epochs, for 48 minutes.\n",
    "- With lr=1e-6, 128 generations, it takes 1h35 to do 100 epochs and diverges. Let's go back to 8 generations and use more epochs.\n",
    "- I'm not sure why I see timeout errors, but I increased the maximum number of generated tokens from 1024 to 2048 because it might be the root of the problem.\n",
    "\n",
    "cst with warmup learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17956bb1",
   "metadata": {},
   "source": [
    "So far it seems that GRPO is much less efficient than HER. Or maybe I haven't found the right configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45f53d",
   "metadata": {},
   "source": [
    "Sometimes I get TimeoutException\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "TimeoutException                          Traceback (most recent call last)\n",
    "Cell In[8], line 96\n",
    "     86         return control\n",
    "     88 trainer = GRPOTrainer(\n",
    "     89     model=model,\n",
    "     90     reward_funcs=reward_accuracy,\n",
    "   (...)\n",
    "     94     callbacks=[StopOnRewardCallback(threshold=1.0), RewardExtremaCallback()],\n",
    "     95 )\n",
    "---> 96 trainer.train()\n",
    "\n",
    "File ~/miniconda3/envs/arc25/lib/python3.10/site-packages/transformers/trainer.py:2245, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
    "   2243         hf_hub_utils.enable_progress_bars()\n",
    "   2244 else:\n",
    "-> 2245     return inner_training_loop(\n",
    "   2246         args=args,\n",
    "   2247         resume_from_checkpoint=resume_from_checkpoint,\n",
    "   2248         trial=trial,\n",
    "   2249         ignore_keys_for_eval=ignore_keys_for_eval,\n",
    "   2250     )\n",
    "\n",
    "File ~/miniconda3/envs/arc25/lib/python3.10/site-packages/transformers/trainer.py:2560, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n",
    "   2553 context = (\n",
    "   2554     functools.partial(self.accelerator.no_sync, model=model)\n",
    "   2555     if i != len(batch_samples) - 1\n",
    "   2556     and self.accelerator.distributed_type != DistributedType.DEEPSPEED\n",
    "   2557     else contextlib.nullcontext\n",
    "   2558 )\n",
    "   2559 with context():\n",
    "-> 2560     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
    "   2562 if (\n",
    "   2563     args.logging_nan_inf_filter\n",
    "   2564     and not is_torch_xla_available()\n",
    "   2565     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
    "   2566 ):\n",
    "   2567     # if loss is nan or inf simply add the average of previous logged losses\n",
    "   2568     tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n",
    "\n",
    "File ~/miniconda3/envs/arc25/lib/python3.10/site-packages/transformers/trainer.py:3736, in Trainer.training_step(self, model, inputs, num_items_in_batch)\n",
    "   3733     return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "   3735 with self.compute_loss_context_manager():\n",
    "-> 3736     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
    "   3738 del inputs\n",
    "   3739 if (\n",
    "   3740     self.args.torch_empty_cache_steps is not None\n",
    "   3741     and self.state.global_step % self.args.torch_empty_cache_steps == 0\n",
    "   3742 ):\n",
    "\n",
    "File ~/miniconda3/envs/arc25/lib/python3.10/site-packages/trl/extras/profiling.py:96, in profiling_decorator.<locals>.wrapper(self, *args, **kwargs)\n",
    "     93 @functools.wraps(func)\n",
    "     94 def wrapper(self, *args, **kwargs):\n",
    "     95     with profiling_context(self, func.__name__):\n",
    "---> 96         return func(self, *args, **kwargs)\n",
    "\n",
    "File ~/miniconda3/envs/arc25/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py:1312, in GRPOTrainer.compute_loss(self, model, inputs, return_outputs, num_items_in_batch)\n",
    "   1310     return self._forward_redirection(model, unwrapped_model, self.compute_liger_loss, unwrapped_model, inputs)\n",
    "   1311 else:\n",
    "-> 1312     return self._compute_loss(model, inputs)\n",
    "\n",
    "File ~/miniconda3/envs/arc25/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py:1370, in GRPOTrainer._compute_loss(self, model, inputs)\n",
    "   1368 if self.beta != 0.0:\n",
    "   1369     mean_kl = (per_token_kl * completion_mask).sum() / completion_mask.sum()\n",
    "-> 1370     self._metrics[mode][\"kl\"].append(self.accelerator.gather_for_metrics(mean_kl).nanmean().item())\n",
    "   1372 # Compute the clipped probability ratios\n",
    "   1373 is_low_clipped = (coef_1 < 1 - self.epsilon_low) & (advantages.unsqueeze(1) < 0)\n",
    "\n",
    "File /mnt/hdd0/MEGA/AI/22_Kaggle/arc25/arc25/code_execution.py:85, in timeout_handler(signum, frame)\n",
    "     84 def timeout_handler(signum, frame):\n",
    "---> 85     raise TimeoutException(\"Code execution exceeded time limit!\")\n",
    "\n",
    "TimeoutException: Code execution exceeded time limit!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f955ba",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d907a80",
   "metadata": {},
   "source": [
    "- [x] Implement a reward function\n",
    "- [x] Set wandb project\n",
    "- [x] wandb save dir\n",
    "- [x] Check if I can see the completions done during training\n",
    "- [x] Stop criteria\n",
    "- [x] Log more metrics such as max and min reward\n",
    "- [ ] Try with more complex tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
