{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d90467",
   "metadata": {},
   "source": [
    "# GRPO exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944074db",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f49b5",
   "metadata": {},
   "source": [
    "Can we solve novel tasks using GRPO?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b9cf8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "from IPython.display import Markdown, display\n",
    "import torch\n",
    "import random\n",
    "import wandb\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task, plot_grids_with_shape, plot_grid\n",
    "from arc25.code_execution import safe_code_execution, validate_code\n",
    "from arc25.utils import set_random_seed, get_timestamp\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"../scripts\"))\n",
    "from finetuning import get_data_collator\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f6f77",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6cac3",
   "metadata": {},
   "source": [
    "### Tasks definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(task_name):\n",
    "    tasks = []\n",
    "\n",
    "    input_img = create_img((9, 9), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    for x in range(0, input_img.shape[1], 1):\n",
    "        draw_vertical_line(output_img, x, color=x+1)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='9-vertical-lines'))\n",
    "\n",
    "    input_img = create_img((10, 8), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    color = 0\n",
    "    for x in range(0, input_img.shape[1], 2):\n",
    "        for y in range(0, input_img.shape[0], 2):\n",
    "            color = (color + 1) % 10\n",
    "            if color == 0: color = 1\n",
    "            draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='20-squares'))\n",
    "\n",
    "    input_img = create_img((6, 8), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    color = 0\n",
    "    for x in range(0, input_img.shape[1], 2):\n",
    "        for y in range(0, input_img.shape[0], 2):\n",
    "            color = (color + 1) % 10\n",
    "            if color == 0: color = 1\n",
    "            draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='12-squares'))\n",
    "\n",
    "    input_img = create_img((8, 8), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    color = 0\n",
    "    for x in range(0, input_img.shape[1], 2):\n",
    "        for y in range(0, input_img.shape[0], 2):\n",
    "            color = (color + 1) % 10\n",
    "            if color == 0: color = 1\n",
    "            draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='16-squares'))\n",
    "\n",
    "    for task in tasks:\n",
    "        if task.name == task_name:\n",
    "            return task\n",
    "    raise ValueError(f\"Task {task_name} not found. Available tasks: {[task.name for task in tasks]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ab00c",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(base_model_path, lora_path):\n",
    "    logging.info(f\"Loading model from {base_model_path} and LoRA from {lora_path}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "    model = PeftModel.from_pretrained(model, lora_path, is_trainable=True)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19459c60",
   "metadata": {},
   "source": [
    "### Reward definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778918fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_completions, all_rewards = [], []\n",
    "\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    all_completions.append(completions)\n",
    "    return [len(completion) for completion in completions]\n",
    "\n",
    "\n",
    "def reward_accuracy(completions, **kwargs):\n",
    "    all_completions.append(completions)\n",
    "    accuracy = []\n",
    "    for completion in completions:\n",
    "        predicted_code = completion.replace('\\n```', '')\n",
    "        try:\n",
    "            predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "            accuracy.append(float(np.mean(predicted_output[0] == task.outputs[0])))\n",
    "        except Exception as e:\n",
    "            print(f'Error executing code: {predicted_code}')\n",
    "            print(e)\n",
    "            accuracy.append(0)\n",
    "    all_rewards.append(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91f894",
   "metadata": {},
   "source": [
    "### Calbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardExtremaCallback(TrainerCallback):\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is None or not all_rewards:\n",
    "            return control\n",
    "\n",
    "        # Grab the last raw batch of rewards\n",
    "        last_batch = all_rewards[-1]\n",
    "        rmax, rmin = float(np.max(last_batch)), float(np.min(last_batch))\n",
    "\n",
    "        # 1) Inject into the HF logs dict so HF will record them\n",
    "        logs[\"train/reward_accuracy/max\"] = rmax\n",
    "        logs[\"train/reward_accuracy/min\"] = rmin\n",
    "        logging.debug(f\"Step {state.global_step}: Max reward: {rmax:.2f}, Min reward: {rmin:.2f}\")\n",
    "\n",
    "        # 2) Immediately push to W&B for real-time curves\n",
    "        wandb.log({\n",
    "            \"train/reward_accuracy/max\": rmax,\n",
    "            \"train/reward_accuracy/min\": rmin,\n",
    "        })\n",
    "\n",
    "        return control\n",
    "\n",
    "class StopOnRewardCallback(TrainerCallback):\n",
    "    def __init__(self, threshold=1.0):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_step_end(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        last_batch = all_rewards[-1]\n",
    "        if np.max(last_batch) >= self.threshold:\n",
    "            logger.info(f\"Stopping training at step {state.global_step} as max reward {np.max(last_batch):.2f} is above threshold {self.threshold:.2f}\")\n",
    "            control.should_training_stop = True\n",
    "            control.should_save = True\n",
    "        return control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8acd9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def grpo(cfg):\n",
    "    global task\n",
    "    task = get_task(cfg.task_name)\n",
    "    plot_task(task); plt.suptitle('Task to solve'); plt.tight_layout(); plt.show()\n",
    "    model, tokenizer = load_model(cfg.base_model_path, cfg.lora_path)\n",
    "\n",
    "    # it's a little bit tricky but since we are not using SFT I should set is_train_prompt=False\n",
    "    prompt = create_prompt_from_task(\n",
    "        task, prompt_version=cfg.prompt_version, grid_encoder=cfg.grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "    train_dataset = Dataset.from_dict({'prompt': [prompt]})\n",
    "\n",
    "\n",
    "    per_device_train_batch_size = 4 # initially 8, but had to lower it do to OOM\n",
    "    run_name = f\"{task.name}_lr{cfg.learning_rate:.0e}_n{cfg.num_generations}\"\n",
    "    output_dir = f\"/mnt/hdd0/Kaggle/arc25/trainings/20250508_GRPO/{get_timestamp()}_{run_name}\"\n",
    "    training_args = GRPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        run_name=run_name,\n",
    "        num_train_epochs=cfg.num_train_epochs,\n",
    "        logging_steps=1,\n",
    "        num_generations=cfg.num_generations, # default is 8\n",
    "        gradient_accumulation_steps=cfg.num_generations//per_device_train_batch_size,\n",
    "        per_device_train_batch_size=per_device_train_batch_size, # default is 8\n",
    "        generation_batch_size=cfg.num_generations, # needs the latest version of trl\n",
    "        temperature=0.5, # default is 0.9\n",
    "        top_p=0.95, # default i 1.0\n",
    "        max_completion_length=768, # default is 256\n",
    "        max_prompt_length=8192, # default is 512\n",
    "        learning_rate=cfg.learning_rate, # default is 1e-6\n",
    "        lr_scheduler_type='constant', # default is 'linear'\n",
    "        log_completions=False,\n",
    "    )\n",
    "\n",
    "    os.environ['WANDB_PROJECT'] = \"20250508_GRPO_v2\"\n",
    "    # os.environ['WANDB_NAME'] = os.path.basename(output_dir)\n",
    "    # os.environ['WANDB_JOB_NAME'] = os.path.basename(output_dir)\n",
    "    os.environ['WANDB_DIR'] = output_dir\n",
    "\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        reward_funcs=reward_accuracy,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[StopOnRewardCallback(threshold=1.0), RewardExtremaCallback()],\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c4370",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    task_name: str = '12-squares' # '9-vertical-lines', '20-squares' or '12-squares'\n",
    "    learning_rate: float = 4e-6\n",
    "    num_generations: int = 16\n",
    "    num_train_epochs: int = 400\n",
    "\n",
    "    base_model_path: str = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "    lora_path: str = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/checkpoint-6400'\n",
    "    prompt_version: str = 'code-from-examples-v3'\n",
    "    grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "\n",
    "cfg = Config()\n",
    "grpo(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of completions: {len(all_completions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.mean([len(completion) for completion in completions]) for completions in all_completions])\n",
    "plt.plot([np.max([len(completion) for completion in completions]) for completions in all_completions])\n",
    "plt.plot([np.min([len(completion) for completion in completions]) for completions in all_completions])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean completion length')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_completions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751958f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Max reward obtained: {np.max(all_rewards)}')\n",
    "if np.max(all_rewards) >= 1.0:\n",
    "    print(f'Task was solved at epoch {np.argmax([np.max(accuracy) == 1 for accuracy in all_rewards])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.max(accuracy) for accuracy in all_rewards])\n",
    "plt.plot([np.mean(accuracy) for accuracy in all_rewards])\n",
    "plt.plot([np.min(accuracy) for accuracy in all_rewards])\n",
    "plt.title('Reward accuracy')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac767d10",
   "metadata": {},
   "source": [
    "## Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd44ff1",
   "metadata": {},
   "source": [
    "Dummy reward experiments:\n",
    "\n",
    "- Running a first dummy train with a reward based on output length for 100 epochs took around 21 minutes\n",
    "- A second run after fixing the initial prompt took 17 minutes\n",
    "- The effective batch size has to be greater or equal to the number of generations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e515e4",
   "metadata": {},
   "source": [
    "True reward experiments:\n",
    "\n",
    "- 100 epochs takes 13 min\n",
    "- 200 epochs takes around 30 min, and after epoch 100 almost always generates the correct solution\n",
    "- Increasing the learning rate to 2e-6 results on solving the task on epoch 20\n",
    "- Increasing the learning rate to 4e-6 results on solving the task on epoch 11\n",
    "- With a learning rate of 1e-5 it is solved at epoch 3, but collapses at epoch 20\n",
    "- when trying the 20 squares task I get OOM\n",
    "- I have increased the number of generations to 128, but it diverges on the 16 squares task. It has run\n",
    "  for 46 epochs, for 48 minutes.\n",
    "- With lr=1e-6, 128 generations, it takes 1h35 to do 100 epochs and diverges. Let's go back to 8 generations and use more epochs.\n",
    "- I'm not sure why I see timeout errors, but I increased the maximum number of generated tokens from 1024 to 2048 because it might be the root of the problem. However this causes OOM errors because it predicts very long functions.\n",
    "- If I try with 12 squares it is able to solve it at step 121 with lr=1e-6\n",
    "- When trying with lr=2e-6 I get another OOM error, sometimes it makes a very long prediction. Same with lr=4e-6\n",
    "- I decrease the batch size per device and then I get timeout error because it is generating a 2k tokens function WTF. I have finally solved the problem with the timeouts. Now I could set a maximum number of tokens, and give bad reward when it is exceeded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef11a3",
   "metadata": {},
   "source": [
    "After solving the problem with timeouts I'm going to set the maximum number of tokens to 768, solving the task with 25 squares needed less than 600 tokens. That will enforce the model to keep the functions short.\n",
    "\n",
    "[wandb](https://wandb.ai/guillermobarbadillo/20250508_GRPO_v2?nw=nwuserguillermobarbadillo)\n",
    "\n",
    "9-vertical-lines:\n",
    "\n",
    "- Solved on 61 steps with lr=1e-6 and num_generations=8\n",
    "- With lr=2e-6 it is solved at step 21, 3m39s\n",
    "- With lr=4e-6 it is solved in 48s, 5 steps\n",
    "\n",
    "12-squares:\n",
    "\n",
    "- with lr=4e-6 solves the task in 1194s, 73 steps.\n",
    "- If I increase the number of generations to 16, it takes 150 steps and almost 100 minutes\n",
    "- with lr=2e-5 and 16 generations it does not converge despite spending 140 minutes\n",
    "\n",
    "16-squares:\n",
    "\n",
    "- with lr=4e-6 the training diverges\n",
    "- with lr=2e-6 I have stopped the training at epoch 123 and 70 minutes. It might seem that the training was going to diverge. At least I need something much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17956bb1",
   "metadata": {},
   "source": [
    "So far it seems that GRPO is much less efficient than HER. Or maybe I haven't found the right configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f955ba",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d907a80",
   "metadata": {},
   "source": [
    "- [x] Implement a reward function\n",
    "- [x] Set wandb project\n",
    "- [x] wandb save dir\n",
    "- [x] Check if I can see the completions done during training\n",
    "- [x] Stop criteria\n",
    "- [x] Log more metrics such as max and min reward\n",
    "- [x] Try with more complex tasks\n",
    "- [ ] Save the outputs to view the evolution\n",
    "- [ ] Better reward and output tokens distribution evolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
