{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d90467",
   "metadata": {},
   "source": [
    "# GRPO exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944074db",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f49b5",
   "metadata": {},
   "source": [
    "Can we solve novel tasks using GRPO?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b9cf8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "from IPython.display import Markdown, display\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task, plot_grids_with_shape, plot_grid\n",
    "from arc25.code_execution import safe_code_execution, validate_code\n",
    "from arc25.utils import set_random_seed, get_timestamp\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"../scripts\"))\n",
    "from finetuning import get_data_collator\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f6f77",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EpochResults = namedtuple(\"EpochResults\", [\"best_prediction\", 'pixel_accuracies'])\n",
    "\n",
    "@log_execution_time\n",
    "def hindsight_experience_replay(task, cfg):\n",
    "    \"\"\"\n",
    "    Use hindsight experience replay to try to solve new tasks\n",
    "    \"\"\"\n",
    "    plot_task(task); plt.suptitle('Task to solve'); plt.tight_layout(); plt.show()\n",
    "    model, tokenizer = load_model(cfg.base_model_path, cfg.lora_path)\n",
    "    metrics = []\n",
    "    for epoch in range(cfg.max_epochs):\n",
    "        logging.info(f'Starting epoch {epoch}...')\n",
    "        new_tasks, pixel_accuracies = inference(\n",
    "            task, model, tokenizer, cfg.grid_encoder, cfg.prompt_version,\n",
    "            n_predictions=cfg.n_predictions)\n",
    "        metrics.append(EpochResults(best_prediction=new_tasks[-1], pixel_accuracies=pixel_accuracies))\n",
    "        plot_metrics_evolution(metrics)\n",
    "        if np.max(pixel_accuracies) == 1:\n",
    "            logger.info(f'Found a perfect prediction at epoch {epoch}!')\n",
    "            break\n",
    "        if not cfg.use_accuracy_for_sorting:\n",
    "            logging.info('Shuffling the tasks, no information about the accuracy is used')\n",
    "            random.shuffle(new_tasks)\n",
    "        finetuning(new_tasks, model, tokenizer, cfg.grid_encoder, cfg.prompt_version)\n",
    "    display(Markdown(f'# Best prediction code\\n\\n```python\\n{metrics[-1].best_prediction.code}\\n```'))\n",
    "    return metrics\n",
    "\n",
    "@log_execution_time\n",
    "def load_model(base_model_path, lora_path):\n",
    "    logging.info(f\"Loading model from {base_model_path} and LoRA from {lora_path}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "    model = PeftModel.from_pretrained(model, lora_path, is_trainable=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "@log_execution_time\n",
    "def inference(task, model, tokenizer, grid_encoder, prompt_version, n_predictions=256):\n",
    "    prompt = create_prompt_from_task(\n",
    "        task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=n_predictions\n",
    "    )\n",
    "    generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "    predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    predicted_codes = [prediction.replace('\\n```', '') for prediction in predictions]\n",
    "    new_tasks = []\n",
    "    pixel_accuracies = []\n",
    "    for predicted_code in tqdm(predicted_codes):\n",
    "        try:\n",
    "            predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "            validated_code = validate_code(predicted_code, task.inputs)\n",
    "            new_tasks.append(Task(inputs=task.inputs, outputs=predicted_output, code=validated_code, name=task.name))\n",
    "            pixel_accuracies.append(float(np.mean(new_tasks[-1].outputs[0] == task.outputs[0])))\n",
    "        except Exception as e:\n",
    "                print(f'Error executing code: {predicted_code}')\n",
    "                print(e)\n",
    "\n",
    "    new_tasks_with_unique_outputs = [new_tasks[0]]\n",
    "    filtered_pixel_accuracies = []\n",
    "    for new_task in new_tasks[1:]:\n",
    "        if not any([np.all(new_task.outputs[0] == t.outputs[0]) for t in new_tasks_with_unique_outputs]):\n",
    "            new_tasks_with_unique_outputs.append(new_task)\n",
    "            filtered_pixel_accuracies.append(float(np.mean(new_task.outputs[0] == task.outputs[0])))\n",
    "    logging.info(f'Number of unique outputs: {len(new_tasks_with_unique_outputs)}/{len(new_tasks)}')\n",
    "    logging.info(f'Max pixel accuracy: {max(pixel_accuracies)}')\n",
    "    new_tasks_with_unique_outputs = sorted(new_tasks_with_unique_outputs, key=lambda x: float(np.mean(x.outputs[0] == task.outputs[0])), reverse=False)\n",
    "    return new_tasks_with_unique_outputs, pixel_accuracies\n",
    "\n",
    "@log_execution_time\n",
    "def finetuning(new_tasks, model, tokenizer, grid_encoder, prompt_version):\n",
    "    prompts = []\n",
    "    for task in new_tasks:\n",
    "        prompts.append(create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=True))\n",
    "    train_dataset = Dataset.from_dict({'text': prompts})\n",
    "\n",
    "    training_arguments = SFTConfig(\n",
    "        output_dir=None, #'/mnt/hdd0/Kaggle/arc25/trainings/20250505_TTT/debug',\n",
    "        save_strategy='no',\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='constant_with_warmup', #constant_with_warmup, cosine, cosine_with_restarts\n",
    "        # lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "        gradient_checkpointing=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        max_grad_norm=1.0,\n",
    "\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=4096,\n",
    "\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"no\", #TODO: previously it was steps\n",
    "        # save_steps=cfg.save_steps or cfg.eval_steps,\n",
    "        logging_steps=10, #50,\n",
    "        log_level=\"info\",\n",
    "        report_to='none',\n",
    "\n",
    "        # parameters added to make the code work with accelerate\n",
    "        # dispatch_batches=False,\n",
    "        # https://huggingface.co/transformers/v4.9.1/main_classes/trainer.html#trainingarguments\n",
    "        ddp_find_unused_parameters=False, # only used with accelerate, got a warning saying that it slows down if True\n",
    "\n",
    "        ignore_data_skip=True, # otherwise it takes too long to start training when resuming from checkpoint\n",
    "\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=get_data_collator(tokenizer),\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "def plot_best_prediction(task, best_prediction, accuracy):\n",
    "    plot_grids_with_shape(task.outputs + best_prediction.outputs, suptitle=f'Best prediction accuracy: {accuracy:.1%}')\n",
    "    display(Markdown(f'```python\\n{best_prediction.code}\\n```'))\n",
    "\n",
    "\n",
    "def plot_metrics_evolution(metrics):\n",
    "    plot_score_histograms(metrics)\n",
    "\n",
    "    for epoch, epoch_results in enumerate(metrics):\n",
    "        plt.subplot(1, len(metrics), epoch + 1)\n",
    "        plot_grid(epoch_results.best_prediction.outputs[0])\n",
    "        plt.title(f'Epoch {epoch} acc: {max(epoch_results.pixel_accuracies):.1%}')\n",
    "    plt.suptitle('Evolution of best predictions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_score_histograms(metrics, offset_scale=1):\n",
    "    \"\"\"\n",
    "    Plots stacked (y-offset) histograms\n",
    "    \"\"\"\n",
    "    cmap = mpl.colormaps['viridis']#get_cmap(\"viridis\")\n",
    "    norm = plt.Normalize(0, len(metrics) - 1)\n",
    "    bins = np.linspace(0, 1, 100)\n",
    "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, epoch_results in enumerate(metrics):\n",
    "        color = cmap(norm(i))\n",
    "        counts, _ = np.histogram(epoch_results.pixel_accuracies, bins=bins)\n",
    "        counts = np.log1p(counts)\n",
    "        offset = i * np.max(counts) * offset_scale  # Add spacing between histograms\n",
    "        plt.fill_between(bin_centers, offset, counts + offset, color=color, label=f'Epoch {i}', alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"Pixel accuracy\")\n",
    "    plt.ylabel(\"Epoch ->\")\n",
    "    plt.title(\"Evolution of pixel accuracy\")\n",
    "    plt.yticks([])  # Hide y-ticks since they don't represent absolute values\n",
    "    plt.grid(axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c4370",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    base_model_path: str = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "    lora_path: str = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/checkpoint-6400'\n",
    "    prompt_version: str = 'code-from-examples-v3'\n",
    "    grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "    max_epochs: int = 10\n",
    "    n_predictions: int = 256 # 256 seems to be the best for my hardware\n",
    "    use_accuracy_for_sorting: bool = True\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = create_img((9, 9), color=0)\n",
    "output_img = input_img.copy()\n",
    "for x in range(0, input_img.shape[1], 1):\n",
    "    draw_vertical_line(output_img, x, color=x+1)\n",
    "\n",
    "task = Task(inputs=[input_img], outputs=[output_img], code='', name='manual')\n",
    "plot_task(task); plt.suptitle('Task to solve'); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(cfg.base_model_path, cfg.lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb08437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a little bit tricky but since we are not using SFT I should set is_train_prompt=False\n",
    "prompt = create_prompt_from_task(\n",
    "    task, prompt_version=cfg.prompt_version, grid_encoder=cfg.grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "train_dataset = Dataset.from_dict({'prompt': [prompt]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd34ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_completions, accuracy_evolution = [], []\n",
    "\n",
    "\n",
    "def reward_len(completions, **kwargs):\n",
    "    all_completions.append(completions)\n",
    "    return [len(completion) for completion in completions]\n",
    "\n",
    "\n",
    "def reward_accuracy(completions, **kwargs):\n",
    "    all_completions.append(completions)\n",
    "    accuracy = []\n",
    "    for completion in completions:\n",
    "        predicted_code = completion.replace('\\n```', '')\n",
    "        try:\n",
    "            predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "            accuracy.append(float(np.mean(predicted_output[0] == task.outputs[0])))\n",
    "        except Exception as e:\n",
    "            print(f'Error executing code: {predicted_code}')\n",
    "            print(e)\n",
    "            accuracy.append(0)\n",
    "    accuracy_evolution.append(accuracy)\n",
    "    return accuracy\n",
    "\n",
    "learning_rate = 1e-5\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=f\"/mnt/hdd0/Kaggle/arc25/trainings/20250508_GRPO/{get_timestamp()}_lr{learning_rate:.0e}\",\n",
    "    num_train_epochs=100,\n",
    "    logging_steps=1,\n",
    "    num_generations=8, # default is 8\n",
    "    per_device_train_batch_size=8, # default is 8\n",
    "    temperature=0.5, # default is 0.9\n",
    "    learning_rate=learning_rate, # default is 1e-6\n",
    ")\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=reward_accuracy,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of completions: {len(all_completions)}')\n",
    "np.unique([len(completion) for completion in all_completions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.mean([len(completion) for completion in completions]) for completions in all_completions])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean completion length')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_completions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751958f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(accuracy_evolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([np.max(accuracy) for accuracy in accuracy_evolution])\n",
    "plt.plot([np.mean(accuracy) for accuracy in accuracy_evolution])\n",
    "plt.plot([np.min(accuracy) for accuracy in accuracy_evolution])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([np.max(accuracy) == 1 for accuracy in accuracy_evolution])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac767d10",
   "metadata": {},
   "source": [
    "## Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd44ff1",
   "metadata": {},
   "source": [
    "Dummy reward experiments:\n",
    "\n",
    "- Running a first dummy train with a reward based on output length for 100 epochs took around 21 minutes\n",
    "- A second run after fixing the initial prompt took 17 minutes\n",
    "- The effective batch size has to be greater or equal to the number of generations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e515e4",
   "metadata": {},
   "source": [
    "True reward experiments:\n",
    "\n",
    "- 100 epochs takes 13 min\n",
    "- 200 epochs takes around 30 min, and after epoch 100 almost always generates the correct solution\n",
    "- Increasing the learning rate to 2e-6 results on solving the task on epoch 20\n",
    "- Increasing the learning rate to 4e-6 results on solving the task on epoch 11\n",
    "- With a learning rate of 1e-5 it is solved at epoch 3, but collapses at epoch 20\n",
    "\n",
    "cst with warmup learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17956bb1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80f955ba",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d907a80",
   "metadata": {},
   "source": [
    "- [x] Implement a reward function\n",
    "- [ ] Set wandb project\n",
    "- [x] Check if I can see the completions done during training\n",
    "- [ ] Stop criteria\n",
    "- [ ] Log more metrics such as max and min reward\n",
    "- [ ] Try with more complex tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
