{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d90467",
   "metadata": {},
   "source": [
    "# Hindsight Experience Replay v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944074db",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86768f78",
   "metadata": {},
   "source": [
    "Improve the HER algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b9cf8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "from IPython.display import Markdown, display\n",
    "import torch\n",
    "import random\n",
    "from typing import List\n",
    "from dataclasses import field\n",
    "import wandb\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task, plot_grids_with_shape, plot_grid\n",
    "from arc25.code_execution import safe_code_execution, validate_code\n",
    "from arc25.utils import set_random_seed, get_timestamp\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"../scripts\"))\n",
    "from finetuning import get_data_collator\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f6f77",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(task_name):\n",
    "    tasks = []\n",
    "\n",
    "    input_img = create_img((9, 9), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    for x in range(0, input_img.shape[1], 1):\n",
    "        draw_vertical_line(output_img, x, color=x+1)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='9-vertical-lines'))\n",
    "\n",
    "    input_img = create_img((10, 8), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    color = 0\n",
    "    for x in range(0, input_img.shape[1], 2):\n",
    "        for y in range(0, input_img.shape[0], 2):\n",
    "            color = (color + 1) % 10\n",
    "            if color == 0: color = 1\n",
    "            draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='20-squares'))\n",
    "\n",
    "    input_img = create_img((6, 8), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    color = 0\n",
    "    for x in range(0, input_img.shape[1], 2):\n",
    "        for y in range(0, input_img.shape[0], 2):\n",
    "            color = (color + 1) % 10\n",
    "            if color == 0: color = 1\n",
    "            draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='12-squares'))\n",
    "\n",
    "    input_img = create_img((8, 8), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    color = 0\n",
    "    for x in range(0, input_img.shape[1], 2):\n",
    "        for y in range(0, input_img.shape[0], 2):\n",
    "            color = (color + 1) % 10\n",
    "            if color == 0: color = 1\n",
    "            draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='16-squares'))\n",
    "\n",
    "    input_img = create_img((10, 10), color=0)\n",
    "    output_img = Img([\n",
    "        [8, 8, 8, 8, 4, 4, 8, 8, 8, 8],\n",
    "        [8, 8, 4, 4, 4, 4, 4, 4, 8, 8],\n",
    "        [8, 4, 4, 0, 4, 4, 0, 4, 4, 8],\n",
    "        [8, 4, 2, 4, 4, 7, 4, 2, 4, 8],\n",
    "        [8, 4, 4, 4, 7, 7, 4, 4, 4, 8],\n",
    "        [8, 8, 4, 4, 4, 4, 4, 4, 8, 8],\n",
    "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "        [8, 4, 4, 4, 4, 4, 4, 4, 4, 8],\n",
    "        [8, 8, 4, 7, 4, 4, 7, 4, 8, 8],\n",
    "    ])\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='chick'))\n",
    "\n",
    "    for task in tasks:\n",
    "        if task.name == task_name:\n",
    "            return task\n",
    "    raise ValueError(f\"Task {task_name} not found. Available tasks: {[task.name for task in tasks]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EpochResults = namedtuple(\"EpochResults\", [\"best_prediction\", 'pixel_accuracies'])\n",
    "InferenceParams = namedtuple(\"InferenceParams\", [\"num_return_sequences\", \"temperature\"])\n",
    "\n",
    "@log_execution_time\n",
    "def hindsight_experience_replay(task, cfg):\n",
    "    \"\"\"\n",
    "    Use hindsight experience replay to try to solve new tasks\n",
    "    \"\"\"\n",
    "    wandb.init(project='HER_v2', name=f'{task.name}_{get_timestamp()}', config=cfg, reinit=True)\n",
    "    #wandb.run.log_code(os.path.dirname(__file__))\n",
    "    fig = plt.figure()\n",
    "    plot_task(task); plt.suptitle('Task to solve'); plt.tight_layout()\n",
    "    wandb.log({\"task\": wandb.Image(fig)}); plt.show()\n",
    "    model, tokenizer = load_model(cfg.base_model_path, cfg.lora_path)\n",
    "    metrics = []\n",
    "    for epoch in range(1, cfg.max_epochs + 1):\n",
    "        logging.info(f'Starting epoch {epoch}...')\n",
    "        new_tasks, pixel_accuracies = inference(\n",
    "            task, model, tokenizer, cfg.grid_encoder, cfg.prompt_version,\n",
    "            inference_params=cfg.inference_params)\n",
    "        metrics.append(EpochResults(best_prediction=new_tasks[-1], pixel_accuracies=pixel_accuracies))\n",
    "        fig = plt.figure()\n",
    "        plot_grid(new_tasks[-1].outputs[0])\n",
    "        wandb.log({\"epoch\": epoch, \"max_pixel_accuracy\": max(pixel_accuracies),\n",
    "                   \"mean_pixel_accuracy\": np.mean(pixel_accuracies),\n",
    "                   \"min_pixel_accuracy\": min(pixel_accuracies),\n",
    "                   \"best_prediction\": wandb.Image(fig),\n",
    "                   'pixel_accuracy': wandb.Histogram(pixel_accuracies),\n",
    "                   'best_code': wandb.Html(f'<pre>{new_tasks[-1].code}</pre>'),\n",
    "                   },\n",
    "                   step=epoch, commit=True)\n",
    "        plt.close(fig)\n",
    "        plot_metrics_evolution(metrics)\n",
    "        if np.max(pixel_accuracies) == 1:\n",
    "            logger.info(f'Found a perfect prediction at epoch {epoch}!')\n",
    "            break\n",
    "        if not cfg.use_accuracy_for_sorting:\n",
    "            logging.info('Shuffling the tasks, no information about the accuracy is used')\n",
    "            random.shuffle(new_tasks)\n",
    "        finetuning(new_tasks, model, tokenizer, cfg.grid_encoder, cfg.prompt_version)\n",
    "    display(Markdown(f'# Best prediction code\\n\\n```python\\n{metrics[-1].best_prediction.code}\\n```'))\n",
    "    wandb.finish()\n",
    "    return metrics\n",
    "\n",
    "@log_execution_time\n",
    "def load_model(base_model_path, lora_path):\n",
    "    logging.info(f\"Loading model from {base_model_path} and LoRA from {lora_path}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "    model = PeftModel.from_pretrained(model, lora_path, is_trainable=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "@log_execution_time\n",
    "def inference(task, model, tokenizer, grid_encoder, prompt_version, inference_params):\n",
    "    prompt = create_prompt_from_task(\n",
    "        task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    predicted_codes = []\n",
    "    for params in inference_params:\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=params.temperature,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=params.num_return_sequences,\n",
    "        )\n",
    "        generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "        predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        predicted_codes.extend([prediction.replace('\\n```', '') for prediction in predictions])\n",
    "\n",
    "    new_tasks = []\n",
    "    pixel_accuracies = []\n",
    "    for predicted_code in tqdm(predicted_codes):\n",
    "        try:\n",
    "            predicted_output = safe_code_execution(predicted_code, task.inputs)\n",
    "            validated_code = validate_code(predicted_code, task.inputs)\n",
    "            new_tasks.append(Task(inputs=task.inputs, outputs=predicted_output, code=validated_code, name=task.name))\n",
    "            pixel_accuracies.append(float(np.mean(new_tasks[-1].outputs[0] == task.outputs[0])))\n",
    "        except Exception as e:\n",
    "                print(f'Error executing code: {predicted_code}')\n",
    "                print(e)\n",
    "\n",
    "    new_tasks_with_unique_outputs = [new_tasks[0]]\n",
    "    filtered_pixel_accuracies = []\n",
    "    for new_task in new_tasks[1:]:\n",
    "        if not any([np.all(new_task.outputs[0] == t.outputs[0]) for t in new_tasks_with_unique_outputs]):\n",
    "            new_tasks_with_unique_outputs.append(new_task)\n",
    "            filtered_pixel_accuracies.append(float(np.mean(new_task.outputs[0] == task.outputs[0])))\n",
    "    logging.info(f'Number of unique outputs: {len(new_tasks_with_unique_outputs)}/{len(new_tasks)}')\n",
    "    logging.info(f'Max pixel accuracy: {max(pixel_accuracies)}')\n",
    "    new_tasks_with_unique_outputs = sorted(new_tasks_with_unique_outputs, key=lambda x: float(np.mean(x.outputs[0] == task.outputs[0])), reverse=False)\n",
    "    return new_tasks_with_unique_outputs, pixel_accuracies\n",
    "\n",
    "@log_execution_time\n",
    "def finetuning(new_tasks, model, tokenizer, grid_encoder, prompt_version):\n",
    "    prompts = []\n",
    "    for task in new_tasks:\n",
    "        prompts.append(create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=True))\n",
    "    train_dataset = Dataset.from_dict({'text': prompts})\n",
    "\n",
    "    training_arguments = SFTConfig(\n",
    "        output_dir=None, #'/mnt/hdd0/Kaggle/arc25/trainings/20250505_TTT/debug',\n",
    "        save_strategy='no',\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='constant_with_warmup', #constant_with_warmup, cosine, cosine_with_restarts\n",
    "        # lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "        gradient_checkpointing=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        max_grad_norm=1.0,\n",
    "\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=4096,\n",
    "\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"no\", #TODO: previously it was steps\n",
    "        # save_steps=cfg.save_steps or cfg.eval_steps,\n",
    "        logging_steps=10, #50,\n",
    "        log_level=\"info\",\n",
    "        report_to='none',\n",
    "\n",
    "        # parameters added to make the code work with accelerate\n",
    "        # dispatch_batches=False,\n",
    "        # https://huggingface.co/transformers/v4.9.1/main_classes/trainer.html#trainingarguments\n",
    "        ddp_find_unused_parameters=False, # only used with accelerate, got a warning saying that it slows down if True\n",
    "\n",
    "        ignore_data_skip=True, # otherwise it takes too long to start training when resuming from checkpoint\n",
    "\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=get_data_collator(tokenizer),\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "def plot_best_prediction(task, best_prediction, accuracy):\n",
    "    plot_grids_with_shape(task.outputs + best_prediction.outputs, suptitle=f'Best prediction accuracy: {accuracy:.1%}')\n",
    "    display(Markdown(f'```python\\n{best_prediction.code}\\n```'))\n",
    "\n",
    "\n",
    "def plot_metrics_evolution(metrics):\n",
    "    plot_score_histograms(metrics)\n",
    "\n",
    "    for epoch, epoch_results in enumerate(metrics):\n",
    "        plt.subplot(1, len(metrics), epoch + 1)\n",
    "        plot_grid(epoch_results.best_prediction.outputs[0])\n",
    "        plt.title(f'Epoch {epoch} acc: {max(epoch_results.pixel_accuracies):.1%}')\n",
    "    plt.suptitle('Evolution of best predictions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_score_histograms(metrics, offset_scale=1):\n",
    "    \"\"\"\n",
    "    Plots stacked (y-offset) histograms\n",
    "    \"\"\"\n",
    "    cmap = mpl.colormaps['viridis']#get_cmap(\"viridis\")\n",
    "    norm = plt.Normalize(0, len(metrics) - 1)\n",
    "    bins = np.linspace(0, 1, 100)\n",
    "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, epoch_results in enumerate(metrics):\n",
    "        color = cmap(norm(i))\n",
    "        counts, _ = np.histogram(epoch_results.pixel_accuracies, bins=bins)\n",
    "        counts = np.log1p(counts)\n",
    "        offset = i * np.max(counts) * offset_scale  # Add spacing between histograms\n",
    "        plt.fill_between(bin_centers, offset, counts + offset, color=color, label=f'Epoch {i}', alpha=0.5)\n",
    "\n",
    "    plt.xlabel(\"Pixel accuracy\")\n",
    "    plt.ylabel(\"Epoch ->\")\n",
    "    plt.title(\"Evolution of pixel accuracy\")\n",
    "    plt.yticks([])  # Hide y-ticks since they don't represent absolute values\n",
    "    plt.grid(axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c4370",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    base_model_path: str = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "    lora_path: str = '/mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/steps_6400/checkpoint-6400'\n",
    "    prompt_version: str = 'code-from-examples-v3'\n",
    "    grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "    max_epochs: int = 10\n",
    "    temperature: float = 0.5\n",
    "    n_predictions: int = 256 # 256 seems to be the best for my hardware\n",
    "    use_accuracy_for_sorting: bool = True\n",
    "    inference_params: List[InferenceParams] = field(default_factory=lambda: [\n",
    "        InferenceParams(num_return_sequences=16, temperature=0.25),\n",
    "        InferenceParams(num_return_sequences=128, temperature=0.5),\n",
    "        InferenceParams(num_return_sequences=128, temperature=0.75),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ec002",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_task('12-squares')\n",
    "cfg = Config(inference_params=[\n",
    "    InferenceParams(num_return_sequences=256, temperature=0.5),\n",
    "])\n",
    "hindsight_experience_replay(task, cfg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3522f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3dc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_str = \"\"\"\n",
    "def greet(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "greet(\"W&B\")\n",
    "\"\"\"\n",
    "\n",
    "# Format as Markdown with syntax highlighting\n",
    "md_code = f\"```python\\n{code_str}\\n```\"\n",
    "\n",
    "wandb.log({\"code_snippet\": wandb.Html(md_code)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76411909",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"code_snippet_text\": code_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_task('chick')\n",
    "\n",
    "fig = plt.figure()\n",
    "plot_grid(task.outputs[0])\n",
    "wandb.log({\"task\": wandb.Image(fig)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aaee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_task('chick')\n",
    "\n",
    "for step in range(1, 5):\n",
    "    fig = plt.figure()\n",
    "    plot_grid(task.outputs[0][step:-step, step:-step])\n",
    "    wandb.log({\"task\": wandb.Image(fig)}, step=step, commit=True)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a6cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de508b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c19de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_task('chick')\n",
    "cfg = Config(inference_params=[\n",
    "    InferenceParams(num_return_sequences=16, temperature=0.1),\n",
    "    InferenceParams(num_return_sequences=16, temperature=0.25),\n",
    "    InferenceParams(num_return_sequences=64, temperature=0.5),\n",
    "    InferenceParams(num_return_sequences=128, temperature=0.75),\n",
    "    InferenceParams(num_return_sequences=128, temperature=0.9),\n",
    "])\n",
    "hindsight_experience_replay(task, cfg);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90521f",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028bc7b2",
   "metadata": {},
   "source": [
    "I believe that the problem lies that in the latest epochs the number of different predictions decreases. We still need exploration to achieve the perfect solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2de8e",
   "metadata": {},
   "source": [
    "```\n",
    "cfg = Config(inference_params=[\n",
    "    InferenceParams(num_return_sequences=16, temperature=0.25),\n",
    "    InferenceParams(num_return_sequences=128, temperature=0.5),\n",
    "    InferenceParams(num_return_sequences=128, temperature=0.75),\n",
    "])\n",
    "97% 951s\n",
    "\n",
    "\n",
    "cfg = Config(inference_params=[\n",
    "    InferenceParams(num_return_sequences=16, temperature=0.1),\n",
    "    InferenceParams(num_return_sequences=16, temperature=0.25),\n",
    "    InferenceParams(num_return_sequences=64, temperature=0.5),\n",
    "    InferenceParams(num_return_sequences=128, temperature=0.75),\n",
    "    InferenceParams(num_return_sequences=128, temperature=0.9),\n",
    "])\n",
    "97% 1535s\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055461c5",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c174c5",
   "metadata": {},
   "source": [
    "- Better selection of the task for unique output (choose shortest)\n",
    "- Print the code on each epoch\n",
    "- Show task length evolution\n",
    "- Better output show when epochs > 10\n",
    "- What is the best way to distribute temperatures?\n",
    "- Improve metrics plot\n",
    "- Maybe I have to keep all the tasks to preserve diversity, and to avoid retraining all the time on the same task\n",
    "- Add wandb with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913e4a0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
