{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d90467",
   "metadata": {},
   "source": [
    "# Hindsight Experience Replay with real ARC tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944074db",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86768f78",
   "metadata": {},
   "source": [
    "Check if we can solve real ARC tasks using HER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b9cf8",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2f645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from transformers import AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import logging\n",
    "from IPython.display import Markdown, display\n",
    "import torch\n",
    "import random\n",
    "from typing import List\n",
    "from dataclasses import field\n",
    "import wandb\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt\n",
    "from arc25.plot import plot_task, plot_grids_with_shape, plot_grid\n",
    "from arc25.code_execution import safe_code_execution, validate_code\n",
    "from arc25.utils import set_random_seed, get_timestamp\n",
    "from arc25.code_analysis import analyze_dsl_usage\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"../scripts\"))\n",
    "from finetuning import get_data_collator\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f6f77",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d85e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json', 'r') as f:\n",
    "    training_challenges = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(task_name):\n",
    "    if task_name in training_challenges:\n",
    "        task_data = training_challenges[task_name]\n",
    "        inputs = [Img(sample['input']) for sample in task_data['train']]\n",
    "        outputs = [Img(sample['output']) for sample in task_data['train']]\n",
    "        return Task(inputs=inputs, outputs=outputs, code='', name=task_name)\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    input_img = create_img((9, 9), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    for x in range(0, input_img.shape[1], 1):\n",
    "        draw_vertical_line(output_img, x, color=x+1)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='9-vertical-lines'))\n",
    "\n",
    "    input_img = create_img((10, 8), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    color = 0\n",
    "    for x in range(0, input_img.shape[1], 2):\n",
    "        for y in range(0, input_img.shape[0], 2):\n",
    "            color = (color + 1) % 10\n",
    "            if color == 0: color = 1\n",
    "            draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='20-squares'))\n",
    "\n",
    "    input_img = create_img((6, 8), color=0)\n",
    "    output_img = input_img.copy()\n",
    "    color = 0\n",
    "    for x in range(0, input_img.shape[1], 2):\n",
    "        for y in range(0, input_img.shape[0], 2):\n",
    "            color = (color + 1) % 10\n",
    "            if color == 0: color = 1\n",
    "            draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='12-squares'))\n",
    "\n",
    "    for n in range(4, 11):\n",
    "        max_colors = 10 if n < 9 else 8\n",
    "        input_img = create_img((2*n, 2*n), color=0)\n",
    "        output_img = input_img.copy()\n",
    "        color = 0\n",
    "        for x in range(0, input_img.shape[1], 2):\n",
    "            for y in range(0, input_img.shape[0], 2):\n",
    "                color = (color + 1) % max_colors\n",
    "                if color == 0: color = 1\n",
    "                draw_rectangle(output_img, (y, x), (y+1, x+1), color=color)\n",
    "        tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name=f'{n*n}-squares'))\n",
    "\n",
    "    input_img = create_img((10, 10), color=0)\n",
    "    output_img = Img([\n",
    "        [8, 8, 8, 8, 4, 4, 8, 8, 8, 8],\n",
    "        [8, 8, 4, 4, 4, 4, 4, 4, 8, 8],\n",
    "        [8, 4, 4, 0, 4, 4, 0, 4, 4, 8],\n",
    "        [8, 4, 2, 4, 4, 7, 4, 2, 4, 8],\n",
    "        [8, 4, 4, 4, 7, 7, 4, 4, 4, 8],\n",
    "        [8, 8, 4, 4, 4, 4, 4, 4, 8, 8],\n",
    "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "        [8, 4, 4, 4, 4, 4, 4, 4, 4, 8],\n",
    "        [8, 8, 4, 7, 4, 4, 7, 4, 8, 8],\n",
    "    ])\n",
    "    tasks.append(Task(inputs=[input_img], outputs=[output_img], code='', name='chick'))\n",
    "\n",
    "    for task in tasks:\n",
    "        if task.name == task_name:\n",
    "            return task\n",
    "    raise ValueError(f\"Task {task_name} not found. Available tasks: {[task.name for task in tasks]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EpochResults = namedtuple(\"EpochResults\", [\"best_prediction\", 'pixel_accuracies', 'new_tasks'])\n",
    "InferenceParams = namedtuple(\"InferenceParams\", [\"num_return_sequences\", \"temperature\", 'top_p'])\n",
    "\n",
    "@log_execution_time\n",
    "def hindsight_experience_replay(task, cfg, run_name_prefix=None):\n",
    "    \"\"\"\n",
    "    Use hindsight experience replay to try to solve new tasks\n",
    "    \"\"\"\n",
    "    run_name = f'{task.name}_{cfg.learning_rate:.0e}lr_' + '_'.join([f'{params.num_return_sequences}gen_t{params.temperature}' for params in cfg.inference_params]) + '_' + get_timestamp()\n",
    "    if run_name_prefix is not None:\n",
    "        run_name = f'{run_name_prefix}_{run_name}'\n",
    "    logging.info(f'Run name: {run_name}')\n",
    "    wandb.init(project=cfg.wandb_project, name=run_name, config=cfg, reinit=True, dir=f'/mnt/hdd0/Kaggle/arc25/trainings/20250510_HER_v2/{run_name}', save_code=True)\n",
    "    #wandb.run.log_code(os.path.dirname(__file__))\n",
    "    plot_task(task); plt.suptitle('Task to solve'); plt.tight_layout()\n",
    "    wandb.log({\"task\": wandb.Image(plt.gcf())}); plt.show()\n",
    "    model, tokenizer = load_model(cfg.base_model_path, cfg.lora_path)\n",
    "    metrics, unique_generated_tasks = [], dict()\n",
    "    for epoch in range(1, cfg.max_epochs + 1):\n",
    "        logging.info(f'Starting epoch {epoch}...')\n",
    "        new_tasks, pixel_accuracies = inference(\n",
    "            task, model, tokenizer, cfg.grid_encoder, cfg.prompt_version,\n",
    "            inference_params=cfg.inference_params,\n",
    "            previously_generated_tasks=unique_generated_tasks)\n",
    "        metrics.append(EpochResults(best_prediction=new_tasks[-1], pixel_accuracies=pixel_accuracies,\n",
    "                                    new_tasks=new_tasks))\n",
    "        plot_metrics_evolution(metrics, task)\n",
    "        log_progress_to_wandb(metrics, epoch)\n",
    "        display(Markdown(f'# Best prediction code\\n\\n```python\\n{metrics[-1].best_prediction.code}\\n```'))\n",
    "        if np.max(pixel_accuracies) == 1:\n",
    "            logger.info(f'Found a perfect prediction at epoch {epoch}!')\n",
    "            break\n",
    "        if not cfg.use_accuracy_for_sorting:\n",
    "            logging.info('Shuffling the tasks, no information about the accuracy is used')\n",
    "            random.shuffle(new_tasks)\n",
    "        if cfg.only_train_on_novel_tasks:\n",
    "            new_tasks, unique_generated_tasks = filter_new_tasks(new_tasks, unique_generated_tasks)\n",
    "        if epoch == cfg.max_epochs: break # does not have sense to fine-tune if we reached the max epochs\n",
    "        finetuning(new_tasks, model, tokenizer, cfg.grid_encoder, cfg.prompt_version,\n",
    "                   learning_rate=cfg.learning_rate, lr_scheduler_type=cfg.lr_scheduler_type)\n",
    "    display(Markdown(f'# Best prediction code\\n\\n```python\\n{metrics[-1].best_prediction.code}\\n```'))\n",
    "    plot_metrics_evolution(metrics, task, log_to_wandb=True)\n",
    "    wandb.log({\n",
    "        'num_generations': epoch*sum([params.num_return_sequences for params in cfg.inference_params])})\n",
    "    wandb.finish()\n",
    "    return metrics, unique_generated_tasks\n",
    "\n",
    "\n",
    "@log_execution_time\n",
    "def load_model(base_model_path, lora_path):\n",
    "    logging.info(f\"Loading model from {base_model_path} and LoRA from {lora_path}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit= True,\n",
    "            bnb_4bit_quant_type= \"nf4\",\n",
    "            bnb_4bit_compute_dtype= torch.float16,\n",
    "            bnb_4bit_use_double_quant= True,\n",
    "            llm_int8_enable_fp32_cpu_offload= True,\n",
    "            llm_int8_skip_modules=['gate', 'lm_head'],\n",
    "    )\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=bnb_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "    model = PeftModel.from_pretrained(model, lora_path, is_trainable=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "@log_execution_time\n",
    "def inference(task, model, tokenizer, grid_encoder, prompt_version, inference_params, previously_generated_tasks):\n",
    "    prompt = create_prompt_from_task(\n",
    "        task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=False)\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    predicted_codes = []\n",
    "    for params in inference_params:\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=3072,\n",
    "            do_sample=True,\n",
    "            temperature=params.temperature,\n",
    "            top_p=params.top_p,\n",
    "            num_return_sequences=params.num_return_sequences,\n",
    "        )\n",
    "        generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "        predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        predicted_codes.extend([prediction.replace('\\n```', '') for prediction in predictions])\n",
    "\n",
    "    new_tasks = []\n",
    "    pixel_accuracies = []\n",
    "    for predicted_code in tqdm(predicted_codes):\n",
    "        try:\n",
    "            predicted_outputs = safe_code_execution(predicted_code, task.inputs)\n",
    "            validated_code = validate_code(predicted_code, task.inputs)\n",
    "            new_tasks.append(Task(inputs=task.inputs, outputs=predicted_outputs, code=validated_code, name=task.name))\n",
    "            pixel_accuracies.append(compute_pixel_accuracy(task.outputs, predicted_outputs))\n",
    "        except Exception as e:\n",
    "                print(f'Error executing code: {predicted_code}')\n",
    "                print(e)\n",
    "\n",
    "    output_to_tasks = {}\n",
    "    for new_task in new_tasks:\n",
    "        key = str(new_task.outputs)\n",
    "        if key not in output_to_tasks:\n",
    "            output_to_tasks[key] = []\n",
    "        output_to_tasks[key].append(new_task)\n",
    "\n",
    "    new_tasks_with_unique_outputs = []\n",
    "    for tasks in output_to_tasks.values():\n",
    "        # choose the task with the shortest code (measured by the number of lines)\n",
    "        best_task = min(tasks, key=lambda x: len(x.code.splitlines()))\n",
    "        new_tasks_with_unique_outputs.append(best_task)\n",
    "\n",
    "\n",
    "    logging.info(f'Max pixel accuracy: {max(pixel_accuracies)}')\n",
    "    new_tasks_with_unique_outputs = sorted(new_tasks_with_unique_outputs, key=lambda x: compute_pixel_accuracy(task.outputs, x.outputs), reverse=False)\n",
    "\n",
    "    novel_tasks, _ = filter_new_tasks(new_tasks, previously_generated_tasks.copy())\n",
    "    sys.stdout.flush()\n",
    "    logging.info(f'Number of predictions: {len(predicted_codes)}, Valid tasks: {len(new_tasks)}, Unique tasks: {len(new_tasks_with_unique_outputs)}, Novel tasks: {len(novel_tasks)}')\n",
    "\n",
    "    return new_tasks_with_unique_outputs, pixel_accuracies\n",
    "\n",
    "\n",
    "def compute_pixel_accuracy(outputs, predicted_outputs):\n",
    "    accuracy = []\n",
    "    for output, predicted_output in zip(outputs, predicted_outputs):\n",
    "        if np.any(output.shape != predicted_output.shape):\n",
    "            return -np.abs(np.mean(predicted_output.shape - output.shape))/30\n",
    "        accuracy.append(float(np.mean(output == predicted_output)))\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "\n",
    "@log_execution_time\n",
    "def finetuning(new_tasks, model, tokenizer, grid_encoder, prompt_version, learning_rate, lr_scheduler_type):\n",
    "    if not new_tasks:\n",
    "        logging.info('No new tasks to train on')\n",
    "        return\n",
    "\n",
    "    prompts = []\n",
    "    for task in new_tasks:\n",
    "        prompts.append(create_prompt_from_task(\n",
    "    task, prompt_version=prompt_version, grid_encoder=grid_encoder, tokenizer=tokenizer, is_train_prompt=True))\n",
    "    train_dataset = Dataset.from_dict({'text': prompts})\n",
    "\n",
    "    training_arguments = SFTConfig(\n",
    "        output_dir=None, #'/mnt/hdd0/Kaggle/arc25/trainings/20250505_TTT/debug',\n",
    "        save_strategy='no',\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=learning_rate,\n",
    "        lr_scheduler_type=lr_scheduler_type, #constant_with_warmup, cosine, cosine_with_restarts\n",
    "        # lr_scheduler_kwargs=lr_scheduler_kwargs,\n",
    "        gradient_checkpointing=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        max_grad_norm=1.0,\n",
    "\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=4096,\n",
    "\n",
    "        do_eval=True,\n",
    "        eval_strategy=\"no\", #TODO: previously it was steps\n",
    "        # save_steps=cfg.save_steps or cfg.eval_steps,\n",
    "        logging_steps=10, #50,\n",
    "        log_level=\"info\",\n",
    "        report_to='none',\n",
    "\n",
    "        # parameters added to make the code work with accelerate\n",
    "        # dispatch_batches=False,\n",
    "        # https://huggingface.co/transformers/v4.9.1/main_classes/trainer.html#trainingarguments\n",
    "        ddp_find_unused_parameters=False, # only used with accelerate, got a warning saying that it slows down if True\n",
    "\n",
    "        ignore_data_skip=True, # otherwise it takes too long to start training when resuming from checkpoint\n",
    "\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "\n",
    "        use_liger_kernel=True,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=get_data_collator(tokenizer),\n",
    "        args=training_arguments,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "def plot_best_prediction(task, best_prediction, accuracy):\n",
    "    plot_grids_with_shape(task.outputs + best_prediction.outputs, suptitle=f'Best prediction accuracy: {accuracy:.1%}')\n",
    "    display(Markdown(f'```python\\n{best_prediction.code}\\n```'))\n",
    "\n",
    "\n",
    "def plot_metrics_evolution(metrics, task, log_to_wandb=False):\n",
    "    plot_score_histograms(metrics, log_to_wandb=log_to_wandb)\n",
    "    n_outputs = len(metrics[0].best_prediction.outputs)\n",
    "    n_plots = len(metrics) + 1\n",
    "    for output_idx in range(n_outputs):\n",
    "        n_cols = min(9, n_plots)\n",
    "        n_rows = n_plots // n_cols\n",
    "        if n_plots % n_cols != 0:\n",
    "            n_rows += 1\n",
    "        plt.figure(figsize=(n_cols*2.25, n_rows * 2))\n",
    "        for epoch, epoch_results in enumerate(metrics):\n",
    "            plt.subplot(n_rows, n_cols, epoch + 1)\n",
    "            plot_grid(epoch_results.best_prediction.outputs[output_idx])\n",
    "            accuracy = compute_pixel_accuracy(task.outputs[output_idx: output_idx + 1], epoch_results.best_prediction.outputs[output_idx: output_idx + 1])\n",
    "            plt.title(f'Epoch {epoch} acc: {accuracy:.1%}')\n",
    "        plt.subplot(n_rows, n_cols, n_plots)\n",
    "        plot_grid(task.outputs[output_idx])\n",
    "        plt.title(f'Ground truth')\n",
    "\n",
    "        plt.suptitle(f'Evolution of best predictions for output {output_idx + 1}/{n_outputs}')\n",
    "        plt.tight_layout()\n",
    "        if log_to_wandb:\n",
    "            wandb.log({f\"best_predictions_evolution_{output_idx}\": wandb.Image(plt.gcf())})\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_score_histograms(metrics, offset_scale=0.9, log_to_wandb=False):\n",
    "    \"\"\"\n",
    "    Plots stacked (y-offset) histograms\n",
    "    \"\"\"\n",
    "    cmap = mpl.colormaps['viridis']#get_cmap(\"viridis\")\n",
    "    norm = plt.Normalize(0, len(metrics) - 1)\n",
    "    bins = np.linspace(-1, 1, 200)\n",
    "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    offset = 0\n",
    "    for i, epoch_results in enumerate(metrics):\n",
    "        color = cmap(norm(i))\n",
    "        counts, _ = np.histogram(epoch_results.pixel_accuracies, bins=bins)\n",
    "        counts = np.log1p(counts)\n",
    "        plt.fill_between(bin_centers, offset, counts + offset, color=color, label=f'Epoch {i}', alpha=0.5)\n",
    "        offset += np.max(counts) * offset_scale  # Update offset for next histogram\n",
    "\n",
    "    plt.xlabel(\"Pixel accuracy\")\n",
    "    plt.ylabel(\"Epoch ->\")\n",
    "    plt.title(\"Evolution of pixel accuracy\")\n",
    "    plt.yticks([])  # Hide y-ticks since they don't represent absolute values\n",
    "    plt.grid(axis='x')\n",
    "    plt.tight_layout()\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\"pixel_accuracy_evolution\": wandb.Image(plt.gcf())})\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def log_progress_to_wandb(metrics, epoch):\n",
    "    epoch_results = metrics[-1]\n",
    "    pixel_accuracies = epoch_results.pixel_accuracies\n",
    "    fig = plt.figure(figsize=(6*len(epoch_results.best_prediction.outputs), 6))\n",
    "    for plot_idx, img in enumerate(epoch_results.best_prediction.outputs, 1):\n",
    "        plt.subplot(1, len(epoch_results.best_prediction.outputs), plot_idx)\n",
    "        plot_grid(img)\n",
    "    plt.title(f'Epoch {epoch} max accuracy: {max(pixel_accuracies):.1%}')\n",
    "    really_new_tasks_ratio = compute_really_new_tasks_ratio(metrics)\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"max_pixel_accuracy\": max(pixel_accuracies),\n",
    "        \"mean_pixel_accuracy\": np.mean(pixel_accuracies),\n",
    "        \"min_pixel_accuracy\": min(pixel_accuracies),\n",
    "        \"best_prediction\": wandb.Image(fig),\n",
    "        'pixel_accuracy': wandb.Histogram(pixel_accuracies),\n",
    "        'best_code': wandb.Html(f'<pre>{epoch_results.best_prediction.code}</pre>'),\n",
    "        'unique_new_tasks': len(epoch_results.new_tasks),\n",
    "        'unique_new_tasks_ratio': len(epoch_results.new_tasks)/len(epoch_results.pixel_accuracies),\n",
    "        'best_code_lines': len(epoch_results.best_prediction.code.splitlines()),\n",
    "        'function_lines': wandb.Histogram([len(task.code.splitlines()) for task in epoch_results.new_tasks]),\n",
    "        'novel_new_tasks_ratio': really_new_tasks_ratio,\n",
    "        'novel_new_tasks': really_new_tasks_ratio * len(epoch_results.new_tasks),\n",
    "        },\n",
    "        step=epoch, commit=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_really_new_tasks_ratio(metrics):\n",
    "    if len(metrics) < 2:\n",
    "        return 1.0\n",
    "    new_tasks = metrics[-1].new_tasks\n",
    "    really_new_tasks = 0\n",
    "    for task in new_tasks:\n",
    "        is_new = True\n",
    "        for epoch_results in metrics[:-1]:\n",
    "            if any([str(task.outputs) == str(t.outputs) for t in epoch_results.new_tasks]):\n",
    "                is_new = False\n",
    "                break\n",
    "        if is_new:\n",
    "            really_new_tasks += 1\n",
    "    return really_new_tasks / len(new_tasks)\n",
    "\n",
    "\n",
    "def filter_new_tasks(new_tasks, unique_tasks):\n",
    "    logging.info(f'Filtering new tasks, {len(new_tasks)} tasks to filter')\n",
    "    filtered_new_tasks = []\n",
    "    for task in new_tasks:\n",
    "        key = str(task.outputs)\n",
    "        if key not in unique_tasks:\n",
    "            filtered_new_tasks.append(task)\n",
    "            unique_tasks[key] = task\n",
    "    logging.info(f'Found {len(filtered_new_tasks)} unique tasks')\n",
    "    return filtered_new_tasks, unique_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c4370",
   "metadata": {},
   "source": [
    "## First experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795d2bb",
   "metadata": {},
   "source": [
    "### Default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    wandb_project: str = 'HER_with_real_tasks'\n",
    "    base_model_path: str = '/home/gbarbadillo/models/Qwen2.5-Coder-0.5B-Instruct'\n",
    "    #lora_path: str = '/mnt/hdd0/MEGA/TEMP/2025-06-13-first-real-trainings/2xA6000-Qwen2.5-Coder-0.5B-32000steps-1e-4lr/checkpoint-32000'\n",
    "    lora_path: str = '/mnt/hdd0/MEGA/TEMP/2025-06-18-more-training-tasks/2xA6000-Qwen2.5-Coder-0.5B-4000steps-1e-4lr/checkpoint-4000'\n",
    "    prompt_version: str = 'code-from-examples-v3'\n",
    "    grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "    max_epochs: int = 2\n",
    "    use_accuracy_for_sorting: bool = True\n",
    "    only_train_on_novel_tasks: bool = True\n",
    "    inference_params: List[InferenceParams] = field(default_factory=lambda: [\n",
    "        InferenceParams(num_return_sequences=8, temperature=0.1, top_p=0.95),\n",
    "        InferenceParams(num_return_sequences=128, temperature=0.9, top_p=0.95),\n",
    "    ])\n",
    "    learning_rate: float = 1e-5\n",
    "    lr_scheduler_type: str = 'constant_with_warmup' #constant_with_warmup, cosine, cosine_with_restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003b8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = 2\n",
    "steps = 16000\n",
    "parameters = '0.5B'\n",
    "for task_name in ['1bfc4729']: #08ed6ac7, 0b148d64, 0ca9ddb6, 0d3d703e, 178fcbfb, 1bfc4729, 1c786137\n",
    "    task = get_task(task_name)\n",
    "    cfg = Config(\n",
    "        base_model_path = f'/home/gbarbadillo/models/Qwen2.5-Coder-{parameters}-Instruct',\n",
    "        lora_path = f'/mnt/hdd0/MEGA/TEMP/2025-06-18-more-training-tasks/{gpus}xA6000-Qwen2.5-Coder-{parameters}-{steps}steps-1e-4lr/checkpoint-{steps}',\n",
    "        max_epochs = 5,\n",
    "        inference_params = [\n",
    "            # InferenceParams(num_return_sequences=8, temperature=0.1),\n",
    "            InferenceParams(num_return_sequences=256, temperature=0.9, top_p=0.9),\n",
    "        ],\n",
    "    )\n",
    "    metrics, unique_generated_tasks = hindsight_experience_replay(task, cfg, run_name_prefix='16k-steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = []\n",
    "for epoch_results in metrics:\n",
    "    code.extend([task.code for task in epoch_results.new_tasks if task.code not in code])\n",
    "analyze_dsl_usage(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in unique_generated_tasks.values():\n",
    "    if 'draw_vertical_line' in task.code:\n",
    "        print(task.code)\n",
    "        plot_task(task); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56ea2bd",
   "metadata": {},
   "source": [
    "### Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aae2c7",
   "metadata": {},
   "source": [
    "2025-06-18-more-training-tasks\n",
    "\n",
    "- `0d3d703e` The 0.5B model does not understand that the task is about applying a colormap, this is worrying. However the 1.5B model understood the problem from the beginning and it just took 6 epochs to solve the task completely. This is the first ARC task solved using HER, 20-06-2025.\n",
    "- `0ca9ddb6` The 0.5B model does not understand that the color of the object matters, understands that it needs to draw something. The 1.5B model understands that the color needs to change, but tries to do it using the area. Furthermore it is only able to draw two pixels, it is unable to draw more. Another generalization worrying sign.\n",
    "- `178fcbfb`, The 1.5B model just makes 2 unique predictions, does not imagine to combine horizontal and vertical lines. Just makes vertical lines. It is wrong but it is so consistently wrong that it cannot improve. More exploration is needed. If I switch to the 8k steps model, it makes more diverse predictions and understands that it needs to do vertical and horizontal lines, but it does not understand that there is a condition.\n",
    "- The model trained for 8k steps collapsed to make a single prediction on task `1bfc4729`, increasing the temperature to 2 does not yield more diversity.\n",
    "\n",
    "| task \\ model | 0.5B@1k steps                                                                            | 1.5B@1k steps                                                                                                    | 1.5B@16k steps                                                                             | 3B@1k steps                                                                                          | 7B@1k steps                                                                             |\n",
    "|--------------|------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n",
    "| 08ed6ac7     |                                                                                          |                                                                                                                  |                                                                                            | does not understand that the task is about changing colors, sorting the objects by area              | does not understand that the task is about changing colors, sorting the objects by area |\n",
    "| 0b148d64     |                                                                                          |                                                                                                                  |                                                                                            | the most succesfull approach is downscaling instead of selecting and cropping                        | OOM                                                                                     |\n",
    "| 0ca9ddb6     | draws 3 points, tries to use area for color. Tried an attempt to use the color as input  | draws 2 points, tries to use area for color                                                                      | draws 2 points, tries to use area for color                                                | draws 4 points, but doesn't understand that color depends on the object color, tries to use the area | draws 3 points, then starts to draw lines                                               |\n",
    "| 0d3d703e     | does not understand that  is about colormaps                                             | does not understand that is about colormaps                                                                      | Solved at epoch 6                                                                          | Solved at epoch 2                                                                                    | Solved at epoch 3                                                                       |\n",
    "| 178fcbfb     | draws vertical or horizontal lines, but not both                                         | draws vertical and horizontal lines, but does not understand there is a condition                                | only vertical lines, very low diversity                                                    | draws vertical and horizontal lines, but does not understand there is a condition                    | draws vertical and horizontal lines, but does not understand there is a condition       |\n",
    "| 1bfc4729     | only horizontal lines                                                                    | only horizontal lines                                                                                            | does not understand the task, draws horizontal lines on the points and the rest is garbage | low diversity in predictions, does not improve over horizontal lines                                 | many different predictions, but not in the correct direction                            |\n",
    "| 1c786137     |                                                                                          | chooses the object using height instead of area, maybe another property is needed. Probably color should be used |                                                                                            | does not understand the task                                                                         | OOM                                                                                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba580dd4",
   "metadata": {},
   "source": [
    "**2025-06-13-first-real-trainings**\n",
    "\n",
    "- `08ed6ac`, does not understand that the task is about sorting and changing colors\n",
    "- `0b148d6`, Understands that the task is about detecting objects and cropping, but does not know to use the color\n",
    "- `0ca9ddb6`, seems to understand that the task is about drawing pixels, but it does not use the center as a reference. Neither it uses the color to select certain objects\n",
    "- `0d3d703e`, the model does not recognize that the task is about apply_colormap. Create more tasks showing how to change colors, not changing all the colors always.\n",
    "- `178fcbfb` does understand that the task is about drawing horizontal and vertical lines, but does not know to use the center as a reference\n",
    "- `1bfc4729`, understands that it needs to draw some pattern, but does not have a way to make a different drawing for each image\n",
    "- `1c786137`, does not understand that the task is about selecting the object\n",
    "\n",
    "It seems that the main problem is that the model does not have a good intuition of how to solve the tasks. I might introduce diversity in the generations but suggesting to use some DSL primitive functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0e083",
   "metadata": {},
   "source": [
    "## Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a72e1a",
   "metadata": {},
   "source": [
    "- The model can choose to solve just the first case and ignore the second one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc8f27",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df5b50",
   "metadata": {},
   "source": [
    "- [x] Allow to use real ARC tasks\n",
    "- [x] Visualize predictions on multiple samples\n",
    "- [x] Plot evolution for each output\n",
    "- [x] Allow to work with different images sizes\n",
    "- [x] Show accuracy for each sample, not global accuracy in the evolution plot\n",
    "- [ ] Need to analyze all the code, not just the better one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913e4a0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
