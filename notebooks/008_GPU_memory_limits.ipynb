{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4595b9",
   "metadata": {},
   "source": [
    "# GPU Memory limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b5aea",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3b842",
   "metadata": {},
   "source": [
    "Find the maximum sequence length that can be used for training or inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93fe67",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d7f3ef",
   "metadata": {},
   "source": [
    "- https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct\n",
    "- https://huggingface.co/docs/transformers/en/main_classes/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb347c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d966f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_desired_length(model, tokenizer, sequence_length):\n",
    "    prompt = \"Write a long essay about the impact of artificial intelligence on modern society.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    t0 = time.time()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=sequence_length,\n",
    "        min_new_tokens=sequence_length,\n",
    "    )\n",
    "    logging.info(f\"Generation speed {sequence_length/ (time.time() - t0):.2f} tokens/sec for {sequence_length} tokens\")\n",
    "    generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "    assert len(generated_ids[0]) == sequence_length, f\"Generated sequence length {len(generated_ids[0])} does not match the desired length {sequence_length}\"\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_long_prompt(model, tokenizer, input_length, sequence_length=128):\n",
    "    prompt = \"Write a long essay about the impact of artificial intelligence on modern society. \"\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    n_repeats = input_length // len(model_inputs.input_ids[0])\n",
    "    model_inputs = tokenizer([prompt*n_repeats], return_tensors=\"pt\").to(model.device)\n",
    "    real_input_length = len(model_inputs.input_ids[0])\n",
    "    t0 = time.time()\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=sequence_length,\n",
    "        min_new_tokens=sequence_length,\n",
    "    )\n",
    "    logging.info(f\"Generation speed {sequence_length/ (time.time() - t0):.2f} tokens/sec for {real_input_length} input tokens ({input_length})\")\n",
    "    generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "    assert len(generated_ids[0]) == sequence_length, f\"Generated sequence length {len(generated_ids[0])} does not match the desired length {sequence_length}\"\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888cf38",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635586ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = '3B'\n",
    "base_model_path = f'/home/gbarbadillo/models/Qwen2.5-Coder-{parameters}-Instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=None)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f21f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_with_desired_length(model, tokenizer, sequence_length=32700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 15):\n",
    "    generate_text_with_desired_length(model, tokenizer, sequence_length=2**i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3efa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10, 20):\n",
    "    generate_text_with_long_prompt(model, tokenizer, input_length=2**i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da6661",
   "metadata": {},
   "source": [
    "```\n",
    "# 0.5B\n",
    "1468MiB\n",
    "2025-06-26 17:43:40,263 - root - INFO - generate_text_with_desired_length - Generation speed 13.90 tokens/sec for 8 tokens\n",
    "2025-06-26 17:43:40,583 - root - INFO - generate_text_with_desired_length - Generation speed 50.32 tokens/sec for 16 tokens\n",
    "2025-06-26 17:43:41,201 - root - INFO - generate_text_with_desired_length - Generation speed 51.90 tokens/sec for 32 tokens\n",
    "2025-06-26 17:43:42,433 - root - INFO - generate_text_with_desired_length - Generation speed 52.05 tokens/sec for 64 tokens\n",
    "2025-06-26 17:43:44,979 - root - INFO - generate_text_with_desired_length - Generation speed 50.33 tokens/sec for 128 tokens\n",
    "2025-06-26 17:43:49,982 - root - INFO - generate_text_with_desired_length - Generation speed 51.19 tokens/sec for 256 tokens\n",
    "2025-06-26 17:44:00,041 - root - INFO - generate_text_with_desired_length - Generation speed 50.91 tokens/sec for 512 tokens\n",
    "2025-06-26 17:44:20,302 - root - INFO - generate_text_with_desired_length - Generation speed 50.55 tokens/sec for 1024 tokens\n",
    "2025-06-26 17:44:59,880 - root - INFO - generate_text_with_desired_length - Generation speed 51.75 tokens/sec for 2048 tokens\n",
    "2025-06-26 17:46:19,066 - root - INFO - generate_text_with_desired_length - Generation speed 51.73 tokens/sec for 4096 tokens\n",
    "2025-06-26 17:48:58,518 - root - INFO - generate_text_with_desired_length - Generation speed 51.38 tokens/sec for 8192 tokens\n",
    "2025-06-26 18:18:20,443 - root - INFO - generate_text_with_desired_length - Generation speed 49.80 tokens/sec for 32700 tokens\n",
    "1942MiB\n",
    "\n",
    "2025-06-26 18:02:58,654 - root - INFO - generate_text_with_long_prompt - Generation speed 43.89 tokens/sec for 953 input tokens (1024)\n",
    "2025-06-26 18:03:01,348 - root - INFO - generate_text_with_long_prompt - Generation speed 47.64 tokens/sec for 1905 input tokens (2048)\n",
    "2025-06-26 18:03:04,092 - root - INFO - generate_text_with_long_prompt - Generation speed 46.84 tokens/sec for 3823 input tokens (4096)\n",
    "2025-06-26 18:03:06,782 - root - INFO - generate_text_with_long_prompt - Generation speed 47.91 tokens/sec for 7645 input tokens (8192)\n",
    "2025-06-26 18:03:09,736 - root - INFO - generate_text_with_long_prompt - Generation speed 44.17 tokens/sec for 15289 input tokens (16384)\n",
    "2025-06-26 18:03:13,410 - root - INFO - generate_text_with_long_prompt - Generation speed 35.73 tokens/sec for 30577 input tokens (32768)\n",
    "Token indices sequence length is longer than the specified maximum sequence length for this model (61167 > 32768). Running this sequence through the model will result in indexing errors\n",
    "OOM\n",
    "\n",
    "# 1.5B\n",
    "3700MiB\n",
    "2025-06-26 18:35:18,660 - root - INFO - generate_text_with_desired_length - Generation speed 39.62 tokens/sec for 32700 tokens\n",
    "4648MiB\n",
    "\n",
    "# 3B\n",
    "6882MiB\n",
    "2025-06-26 19:30:47,143 - root - INFO - generate_text_with_desired_length - Generation speed 27.40 tokens/sec for 32700 tokens\n",
    "8002MiB\n",
    "\n",
    "# 7B\n",
    "14782MiB\n",
    "2025-06-26 17:49:53,393 - root - INFO - generate_text_with_desired_length - Generation speed 12.97 tokens/sec for 8 tokens\n",
    "2025-06-26 17:49:53,819 - root - INFO - generate_text_with_desired_length - Generation speed 37.77 tokens/sec for 16 tokens\n",
    "2025-06-26 17:49:54,590 - root - INFO - generate_text_with_desired_length - Generation speed 41.66 tokens/sec for 32 tokens\n",
    "2025-06-26 17:49:56,079 - root - INFO - generate_text_with_desired_length - Generation speed 43.03 tokens/sec for 64 tokens\n",
    "2025-06-26 17:49:59,066 - root - INFO - generate_text_with_desired_length - Generation speed 42.88 tokens/sec for 128 tokens\n",
    "2025-06-26 17:50:04,982 - root - INFO - generate_text_with_desired_length - Generation speed 43.28 tokens/sec for 256 tokens\n",
    "2025-06-26 17:50:17,005 - root - INFO - generate_text_with_desired_length - Generation speed 42.59 tokens/sec for 512 tokens\n",
    "2025-06-26 17:50:40,825 - root - INFO - generate_text_with_desired_length - Generation speed 42.99 tokens/sec for 1024 tokens\n",
    "2025-06-26 17:51:30,304 - root - INFO - generate_text_with_desired_length - Generation speed 41.39 tokens/sec for 2048 tokens\n",
    "15094MiB\n",
    "2025-06-26 17:53:14,419 - root - INFO - generate_text_with_desired_length - Generation speed 39.34 tokens/sec for 4096 tokens\n",
    "15768MiB\n",
    "2025-06-26 19:25:33,969 - root - INFO - generate_text_with_desired_length - Generation speed 32.29 tokens/sec for 8000 tokens\n",
    "16552MiB\n",
    "2025-06-26 19:20:14,560 - root - INFO - generate_text_with_desired_length - Generation speed 26.03 tokens/sec for 16000 tokens\n",
    "21088MiB\n",
    "2025-06-26 18:37:14,045 - root - INFO - generate_text_with_desired_length - Generation speed 18.63 tokens/sec for 32700 tokens\n",
    "20344MiB\n",
    "\n",
    "2025-06-26 18:00:57,276 - root - INFO - generate_text_with_long_prompt - Generation speed 35.11 tokens/sec for 953 input tokens (1024)\n",
    "2025-06-26 18:01:00,889 - root - INFO - generate_text_with_long_prompt - Generation speed 35.50 tokens/sec for 1905 input tokens (2048)\n",
    "2025-06-26 18:01:05,198 - root - INFO - generate_text_with_long_prompt - Generation speed 29.77 tokens/sec for 3823 input tokens (4096)\n",
    "2025-06-26 18:01:11,104 - root - INFO - generate_text_with_long_prompt - Generation speed 21.84 tokens/sec for 7645 input tokens (8192)\n",
    "2025-06-26 18:01:20,953 - root - INFO - generate_text_with_long_prompt - Generation speed 13.04 tokens/sec for 15289 input tokens (16384)\n",
    "2025-06-26 18:01:39,310 - root - INFO - generate_text_with_long_prompt - Generation speed 7.00 tokens/sec for 30577 input tokens (32768)\n",
    "Token indices sequence length is longer than the specified maximum sequence length for this model (61167 > 32768). Running this sequence through the model will result in indexing errors\n",
    "OOM\n",
    "```\n",
    "\n",
    "Inference speed is almost the same, but GPU utilization is much higher with the 7B model.\n",
    "Both models give OOM when trying to generate for more than 32k tokens, which is bigger than the maximum sequence lenght of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538c9a1",
   "metadata": {},
   "source": [
    "Tricks to speedup inference. https://chatgpt.com/c/685d743d-c194-8012-a34d-8cc17e18c9d0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16cd46",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
