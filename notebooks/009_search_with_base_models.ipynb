{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5ef70f",
   "metadata": {},
   "source": [
    "# Search with base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259d",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dabfd",
   "metadata": {},
   "source": [
    "Can we solve ARC tasks using base models with access to a DSL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9df7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import pretty_print_prompt, Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2076585",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ec071",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f12f27",
   "metadata": {},
   "source": [
    "https://github.com/flowersteam/SOAR/blob/main/soar/prompt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f62ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_footprint(module_name: str, show_types: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Load a module by name, then return a newline-separated list of all\n",
    "    top-level functions in it, in the form:\n",
    "\n",
    "      def func_name(arg1, arg2) -> return\n",
    "\n",
    "    If show_types=True, annotations are included; otherwise only names.\n",
    "    \"\"\"\n",
    "    mod = importlib.import_module(module_name)\n",
    "    footprints = []\n",
    "\n",
    "    for name, fn in inspect.getmembers(mod, inspect.isfunction):\n",
    "        # skip imports from elsewhere\n",
    "        if fn.__module__ != module_name or name.startswith(\"_\"):\n",
    "            continue\n",
    "\n",
    "        sig = inspect.signature(fn)\n",
    "        if not show_types:\n",
    "            # strip type info\n",
    "            params = [p.name for p in sig.parameters.values()]\n",
    "            sig_text = f\"({', '.join(params)})\"\n",
    "        else:\n",
    "            sig_text = str(sig)\n",
    "\n",
    "        footprints.append(f\"- {name}{sig_text}\")\n",
    "\n",
    "    return \"\\n\".join(footprints)\n",
    "\n",
    "print(extract_footprint('arc25.BARC_dsl', show_types=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json', 'r') as f:\n",
    "    training_challenges = json.load(f)\n",
    "\n",
    "def get_task(task_name):\n",
    "    if task_name in training_challenges:\n",
    "        task_data = training_challenges[task_name]\n",
    "        inputs = [Img(sample['input']) for sample in task_data['train']]\n",
    "        outputs = [Img(sample['output']) for sample in task_data['train']]\n",
    "        return Task(inputs=inputs, outputs=outputs, code='', name=task_name)\n",
    "    raise ValueError(f\"Task {task_name} not found in training challenges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f42cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an advanced AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks.\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = Template(\"\"\"You are tasked with solving a transformation problem from the Abstraction and Reasoning Challenge (ARC).\n",
    "Implement the transformation rules as a Python function.\n",
    "You should only write the implemented the transformation in code.\n",
    "You must write code in triple backticks (```python and then ```). You must write a function called `transform` which takes a single argument, the input grid as `list[list[int]]`, and returns the transformed grid (also as `list[list[int]]`).\n",
    "\n",
    "## Key Priors:\n",
    "\n",
    "- **Objectness**: Consider the grid as containing objects (groups of connected cells) rather than just individual pixels.\n",
    "- **Goal-Directed**: The transformation should achieve a specific goal, such as creating symmetry or changing the color of specific objects.\n",
    "- **Numbers & Counting**: Keep track of the number of objects, sizes, and their relative positions.\n",
    "- **Geometry & Topology**: Use spatial relationships such as adjacency, enclosure, or symmetry.\n",
    "\n",
    "Carefully analyze the examples and find the underlying transformation logic.\n",
    "\n",
    "## Domain Specific Primitive Functions\n",
    "\n",
    "You can use the already implemented following functions to manipulate the grid:\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are several input-output examples that illustrate the transformation.\n",
    "Your function should generalize the pattern from these examples to solve any input following the same logic.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "### Example {{ loop.index }}\n",
    "\n",
    "#### Input\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "#### Output\n",
    "\n",
    "{{ sample.output }}\n",
    "{% endfor %}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def create_prompt_from_task(task, grid_encoder, tokenizer):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae964737",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(base_model_path, lora_path=None, quantize_4bit=True):\n",
    "    logging.info(f\"Loading model from {base_model_path} and LoRA from {lora_path}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if quantize_4bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit= True,\n",
    "                bnb_4bit_quant_type= \"nf4\",\n",
    "                bnb_4bit_compute_dtype= torch.float16,\n",
    "                bnb_4bit_use_double_quant= True,\n",
    "                llm_int8_enable_fp32_cpu_offload= True,\n",
    "                llm_int8_skip_modules=['gate', 'lm_head'],\n",
    "        )\n",
    "    else:\n",
    "        bnb_config = None\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=bnb_config)\n",
    "    if lora_path is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "        return model, tokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "    model = PeftModel.from_pretrained(model, lora_path, is_trainable=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_model(model=None):\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    if model is not None:\n",
    "        del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613f311",
   "metadata": {},
   "source": [
    "## First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\"/home/gbarbadillo/models/Qwen2.5-Coder-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenizer.tokenize(extract_footprint('arc25.BARC_dsl', show_types=True))))\n",
    "prompt = create_prompt_from_task(get_task('007bbfb7'), grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))'), tokenizer=tokenizer)\n",
    "pretty_print_prompt(prompt, default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bf98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "t0 = time.time()\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_p=0.90,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "t1 = time.time()\n",
    "generated_ids = generated_ids[:, len(model_inputs.input_ids[0]):]\n",
    "output_tokens = generated_ids.shape[1]\n",
    "print(f\"Generated {output_tokens} tokens in {t1 - t0:.2f} seconds\")\n",
    "print(f\"Average tokens per second: {output_tokens / (t1 - t0):.2f} tokens/second\")\n",
    "predictions = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6171a",
   "metadata": {},
   "source": [
    "## VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aefc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Qwen2.5-Coder-3B-Instruct\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    gpu_memory_utilization=0.8,  # Use less GPU memory\n",
    "    # max_model_len=4096,  # Limit context length\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "    tensor_parallel_size=1,  # Single GPU\n",
    "    # quantization=\"bitsandbytes\",  # Use NF4 quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ebbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = list(training_challenges.keys())[:10]\n",
    "vllm_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "prompts = [create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder, tokenizer=vllm_tokenizer) for task_id in task_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=1.0, top_p=0.95, max_tokens=1024)\n",
    "t0 = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)\n",
    "t1 = time.time()\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {t1 - t0:.2f} seconds\")\n",
    "print(f\"Average time per task: {(t1 - t0) / len(outputs):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(outputs):.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / (t1 - t0):.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated output:\")\n",
    "for task_id, output in zip(task_ids, outputs):\n",
    "    print('*'*80, f\"Task ID: {task_id}\", '*'*80, '', sep='\\n')\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9ee91",
   "metadata": {},
   "source": [
    "- 3B quantized model: \n",
    "- 7B quantized model: 2.5s per task\n",
    "\n",
    "```\n",
    "0.5 unquantized\n",
    "Total tokens generated: 6672\n",
    "Time taken: 6.02 seconds\n",
    "Average time per task: 0.60 seconds\n",
    "Average tokens per task: 667.20 tokens\n",
    "Average tokens per second: 1107.48 tokens/second\n",
    "\n",
    "0.5B\n",
    "Total tokens generated: 4493\n",
    "Time taken: 7.97 seconds\n",
    "Average time per task: 0.80 seconds\n",
    "Average tokens per task: 449.30 tokens\n",
    "Average tokens per second: 563.84 tokens/second\n",
    "\n",
    "1.5B unquantized\n",
    "Total tokens generated: 1956\n",
    "Time taken: 4.59 seconds\n",
    "Average time per task: 0.46 seconds\n",
    "Average tokens per task: 195.60 tokens\n",
    "Average tokens per second: 426.54 tokens/second\n",
    "\n",
    "1.5B\n",
    "Total tokens generated: 2805\n",
    "Time taken: 9.93 seconds\n",
    "Average time per task: 0.99 seconds\n",
    "Average tokens per task: 280.50 tokens\n",
    "Average tokens per second: 282.56 tokens/second\n",
    "\n",
    "3B unquantized\n",
    "Total tokens generated: 5465\n",
    "Time taken: 12.09 seconds\n",
    "Average time per task: 1.21 seconds\n",
    "Average tokens per task: 546.50 tokens\n",
    "Average tokens per second: 452.00 tokens/second\n",
    "\n",
    "3B\n",
    "Total tokens generated: 5689\n",
    "Time taken: 31.51 seconds\n",
    "Average time per task: 3.15 seconds\n",
    "Average tokens per task: 568.90 tokens\n",
    "Average tokens per second: 180.54 tokens/second\n",
    "\n",
    "7B\n",
    "Total tokens generated: 2785\n",
    "Time taken: 22.92 seconds\n",
    "Average time per task: 2.29 seconds\n",
    "Average tokens per task: 278.50 tokens\n",
    "Average tokens per second: 121.53 tokens/second\n",
    "\n",
    "Total tokens generated: 4053\n",
    "Time taken: 56.17 seconds\n",
    "Average time per task: 5.62 seconds\n",
    "Average tokens per task: 405.30 tokens\n",
    "Average tokens per second: 72.15 tokens/second\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3166f6e",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1370bb",
   "metadata": {},
   "source": [
    "- [x] Create a prompt with the available DSL functions and the training ARC task\n",
    "- [x] Fix VLLM initialization issues with proper memory management\n",
    "- [ ] Verify the effect of caching\n",
    "- [ ] Update the library to be able to select which DSL to use when executing code\n",
    "- [ ] Try to solve some easy task\n",
    "- [ ] Create a refine prompt\n",
    "- [ ] Make a more complex tree search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
