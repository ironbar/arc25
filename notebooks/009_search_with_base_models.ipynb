{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5ef70f",
   "metadata": {},
   "source": [
    "# Search with base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259d",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dabfd",
   "metadata": {},
   "source": [
    "Can we solve ARC tasks using base models with access to a DSL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9df7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import pretty_print_prompt, Template\n",
    "from arc25.metrics import pixel_similarity_score, correct_grids_score\n",
    "import arc25.BARC_dsl as dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2076585",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ec071",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f12f27",
   "metadata": {},
   "source": [
    "https://github.com/flowersteam/SOAR/blob/main/soar/prompt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f62ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_footprint(module_name: str, show_types: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Load a module by name, then return a newline-separated list of all\n",
    "    top-level functions in it, in the form:\n",
    "\n",
    "      def func_name(arg1, arg2) -> return\n",
    "\n",
    "    If show_types=True, annotations are included; otherwise only names.\n",
    "    \"\"\"\n",
    "    mod = importlib.import_module(module_name)\n",
    "    footprints = []\n",
    "\n",
    "    for name, fn in inspect.getmembers(mod, inspect.isfunction):\n",
    "        # skip imports from elsewhere\n",
    "        if fn.__module__ != module_name or name.startswith(\"_\"):\n",
    "            continue\n",
    "\n",
    "        sig = inspect.signature(fn)\n",
    "        if not show_types:\n",
    "            # strip type info\n",
    "            params = [p.name for p in sig.parameters.values()]\n",
    "            sig_text = f\"({', '.join(params)})\"\n",
    "        else:\n",
    "            sig_text = str(sig)\n",
    "\n",
    "        footprints.append(f\"- dsl.{name}{sig_text}\")\n",
    "\n",
    "    return \"\\n\".join(footprints)\n",
    "\n",
    "print(extract_footprint('arc25.BARC_dsl', show_types=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json', 'r') as f:\n",
    "    training_challenges = json.load(f)\n",
    "\n",
    "def get_task(task_name):\n",
    "    if task_name in training_challenges:\n",
    "        task_data = training_challenges[task_name]\n",
    "        inputs = [Img(sample['input']) for sample in task_data['train']]\n",
    "        outputs = [Img(sample['output']) for sample in task_data['train']]\n",
    "        return Task(inputs=inputs, outputs=outputs, code='', name=task_name)\n",
    "    raise ValueError(f\"Task {task_name} not found in training challenges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f42cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an advanced AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks.\"\"\"\n",
    "\n",
    "prompt_template_text =\"\"\"You are tasked with solving a transformation problem from the Abstraction and Reasoning Challenge (ARC).\n",
    "Implement the transformation rules as a Python function.\n",
    "You should only write the implemented the transformation in code.\n",
    "You must write code in triple backticks (```python and then ```). You must write a function called `transform` which takes a single argument, the input grid as `list[list[int]]`, and returns the transformed grid (also as `list[list[int]]`).\n",
    "\n",
    "## Key Priors:\n",
    "\n",
    "- **Objectness**: Consider the grid as containing objects (groups of connected cells) rather than just individual pixels.\n",
    "- **Goal-Directed**: The transformation should achieve a specific goal, such as creating symmetry or changing the color of specific objects.\n",
    "- **Numbers & Counting**: Keep track of the number of objects, sizes, and their relative positions.\n",
    "- **Geometry & Topology**: Use spatial relationships such as adjacency, enclosure, or symmetry.\n",
    "\n",
    "Carefully analyze the examples and find the underlying transformation logic.\n",
    "\n",
    "## Domain Specific Primitive Functions\n",
    "\n",
    "You can use the already implemented following functions to manipulate the grid:\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "The dsl has been already imported, so just simply call the functions as needed. F.e. dsl.foo()\n",
    "Do not import the dsl again, just use it directly.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are several input-output examples that illustrate the transformation.\n",
    "Your function should generalize the pattern from these examples to solve any input following the same logic.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "### Example {{ loop.index }}\n",
    "\n",
    "#### Input\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "#### Output\n",
    "\n",
    "{{ sample.output }}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = Template(prompt_template_text)\n",
    "\n",
    "\n",
    "def create_prompt_from_task(task, grid_encoder, tokenizer, shuffle_train_samples=False):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc2a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1. One problem of v1 is that 13% of the tasks have predicted the exact same code. So let's rewrite a prompt to avoid that\n",
    "prompt_novel_tasks_text_v1 = prompt_template_text + \"\"\"\n",
    "## Already generated code\n",
    "\n",
    "Below are python functions already generated to try to solve the task. Take them into account when generating the new code.\n",
    "Your code should be different from the already generated code, it should generate a new and different solution to the task.\n",
    "The functions are given to avoid repeating the same code. Your code should be a new and different solution to the task.\n",
    "\n",
    "{% for prediction in previous_predictions %}\n",
    "### Code {{ loop.index }}\n",
    "\n",
    "```python\n",
    "{{ prediction }}\n",
    "```\n",
    "\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "# v2, 8% exact repetitions\n",
    "prompt_novel_tasks_text_v2 = prompt_template_text + \"\"\"\n",
    "## Already generated code\n",
    "\n",
    "Below are python functions already generated to try to solve the task. \n",
    "Do not repeat them, make sure to generate a new and different solution to the task.\n",
    "\n",
    "{% for prediction in previous_predictions %}\n",
    "### Code sample{{ loop.index }}\n",
    "\n",
    "```python\n",
    "{{ prediction }}\n",
    "```\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "## New solution\n",
    "\n",
    "Now implement a new and original solution to the task.\n",
    "\"\"\"\n",
    "\n",
    "# v3, 5% of repeated code\n",
    "prompt_novel_tasks_text_v3 = prompt_template_text + \"\"\"\n",
    "## Already generated code\n",
    "\n",
    "Below are python functions already generated to try to solve the task. \n",
    "Do not repeat them, make sure to generate a new and different solution to the task.\n",
    "\n",
    "{% for prediction in previous_predictions %}\n",
    "### Code sample{{ loop.index }}\n",
    "\n",
    "DO NOT REPEAT THIS CODE, generate a new and different solution to the task.\n",
    "\n",
    "```python\n",
    "{{ prediction }}\n",
    "```\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "## New solution\n",
    "\n",
    "Now implement a new and original solution to the task.\n",
    "Write some code that solves the task using a different approach than the previous ones.\n",
    "\"\"\"\n",
    "\n",
    "# system_prompt = \"\"\"You are an advanced AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks.\n",
    "# Do not repeat any of the code snippets provided in the previous predictions. Always generate a new and different solution to the task.\"\"\"\n",
    "\n",
    "prompt_novel_tasks_template = Template(prompt_novel_tasks_text_v1)\n",
    "# TODO: add scores to the prompt\n",
    "#Pixel similarity scores: {{ prediction.pixel_similarity_scores }}\n",
    "#Correct grids scores: {{ prediction.correct_grids_scores }}\n",
    "\n",
    "def create_prompt_for_novel_solutions(task, grid_encoder, tokenizer, previous_predictions):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True),\n",
    "                         previous_predictions=previous_predictions)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_novel_tasks_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb619bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_prompt_v1 = prompt_template_text + \"\"\"\n",
    "## Code to be refined\n",
    "\n",
    "Below are python functions already generated to try to solve the task.\n",
    "Those functions do not solve the task, but the direction of the solution is correct.\n",
    "Refine them into a new function that solves the task.\n",
    "\n",
    "{% for prediction in previous_predictions %}\n",
    "### Code sample {{ loop.index }}\n",
    "\n",
    "```python\n",
    "{{ prediction }}\n",
    "```\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "## Refined solution\n",
    "\n",
    "Now implement a new function that refines the previous code and solves the task.\n",
    "\"\"\"\n",
    "\n",
    "refine_prompt_template = Template(refine_prompt_v1)\n",
    "\n",
    "\n",
    "def create_refine_prompt(task, grid_encoder, tokenizer, previous_predictions):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True),\n",
    "                         previous_predictions=previous_predictions)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": refine_prompt_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae964737",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1, max_model_len=32000):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=0.9,  # Use less GPU memory\n",
    "        # max_model_len=4096,  # Limit context length\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea848918",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae210ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_python_code(text):\n",
    "    # Extract Python code from the text\n",
    "    if '```python' not in text:\n",
    "        return ''\n",
    "    code = text.split('```python')[1]\n",
    "    if not '```' in code:\n",
    "        return ''\n",
    "\n",
    "    code = code.split('```')[0].strip()\n",
    "    return code.split('```')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_python_code(code):\n",
    "    remove_line_keywords = ['import dsl', 'from dsl import ', 'print(']\n",
    "    code = '\\n'.join(line for line in code.split('\\n') if not any(keyword in line for keyword in remove_line_keywords))\n",
    "    return code.strip()\n",
    "\n",
    "def add_additional_imports(code):\n",
    "    additional_imports = [\n",
    "        'from typing import List, Tuple',\n",
    "        'import numpy as np'\n",
    "    ]\n",
    "    imports = '\\n'.join(additional_imports)\n",
    "    return imports + '\\n' + code if code else imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee204f",
   "metadata": {},
   "source": [
    "### Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_outputs(outputs):\n",
    "    return [_validate_output(output) for output in outputs]\n",
    "\n",
    "def _validate_output(output):\n",
    "    if output is None:\n",
    "        raise ValueError(\"Output is None\")\n",
    "    output = np.array(output) # otherwise I see weird outputs that mix list and numpy arrays\n",
    "    if output.ndim != 2:\n",
    "        raise ValueError(f\"Output is not a 2D array: {output.shape}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee616990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def fingerprint(prediction):\n",
    "    \"\"\"\n",
    "    Create a compact hash for a list of matrices.\n",
    "    Includes shape & dtype to distinguish e.g. (2×2) from (4×1).\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    for m in prediction:\n",
    "        # incorporate shape and dtype in a reproducible way\n",
    "        h.update(str(m.shape).encode())\n",
    "        h.update(m.dtype.str.encode())\n",
    "        # raw data bytes\n",
    "        h.update(m.tobytes())\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b4bd9",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds):\n",
    "    df = pd.DataFrame(columns=['valid code', 'valid outputs', 'unique outputs', 'dsl usage', 'pixel similarity', 'correct grids', 'solved task'])\n",
    "    for task_id in task_ids:\n",
    "        df.loc[task_id, 'valid code'] = len(predicted_code[task_id])/n_preds\n",
    "        df.loc[task_id, 'valid outputs'] = len(predicted_outputs[task_id])/n_preds\n",
    "        df.loc[task_id, 'unique outputs'] = len(set(fingerprint(output) for output in predicted_outputs[task_id]))/n_preds\n",
    "        df.loc[task_id, 'dsl usage'] = sum(1 for code in predicted_code[task_id] if 'dsl.' in code)/n_preds\n",
    "\n",
    "        task = get_task(task_id)\n",
    "        task_predicted_outputs = predicted_outputs[task_id]\n",
    "        scores = sorted([np.mean([pixel_similarity_score(output, pred) for output, pred in zip(task.outputs, predictions)]) for predictions in task_predicted_outputs])\n",
    "        df.loc[task_id, 'pixel similarity'] = np.mean(scores) if scores else 0.0\n",
    "\n",
    "        task_outputs = [np.array(output) for output in task.outputs]\n",
    "        scores = sorted([correct_grids_score(task_outputs, predictions) for predictions in task_predicted_outputs])\n",
    "        df.loc[task_id, 'correct grids'] = np.mean(scores) if scores else 0.0\n",
    "        df.loc[task_id, 'solved task'] = int(np.max(scores) == 1) if scores else 0\n",
    "\n",
    "    df.loc['MEAN'] = df.mean(axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6171a",
   "metadata": {},
   "source": [
    "## Independent search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aefc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Qwen2.5-Coder-7B-Instruct\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)\n",
    "\n",
    "# model_path = \"/home/gbarbadillo/models/Qwen3-4B\"\n",
    "# llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)\n",
    "\n",
    "# model_path = '/home/gbarbadillo/models/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q4_k_m.gguf' # Needs 2 GPUs\n",
    "# llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=2, max_model_len=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = list(training_challenges.keys())\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "prompts = [create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer) for task_id in task_ids]\n",
    "if 'Qwen3' in model_path:\n",
    "    # disable thinking without using the chat template\n",
    "    prompts = [prompt + '<think>\\n\\n</think>\\n\\n' for prompt in prompts]\n",
    "\n",
    "t0 = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in outputs)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(outputs):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(outputs) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            code = curate_python_code(code)\n",
    "            predicted_code[task_id].append(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58674fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, sampling_params.n)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2430fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'{os.path.basename(model_path)}_{len(task_ids)}tasks_{sampling_params.n}preds_{int(inference_time)}runtime.csv'\n",
    "df.to_csv(output_path, index_label='task_id')\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816116aa",
   "metadata": {},
   "source": [
    "## Sequential search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d5573",
   "metadata": {},
   "source": [
    "Let's give the model the context of all the functions generated so far to try induce more diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404b6ff",
   "metadata": {},
   "source": [
    "### Prompt tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cafa907",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Qwen2.5-Coder-7B-Instruct\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6dcd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = list(training_challenges.keys())\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258b508",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb484477",
   "metadata": {},
   "source": [
    "As a first step let's make sure that we have a single prediction that predicts a valid output for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb64856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first prediction initialization\n",
    "while True:\n",
    "    prompts, epoch_task_ids = [], []\n",
    "    for task_id in task_ids:\n",
    "        if not predicted_code[task_id]:  # only create prompt if no code has been predicted yet\n",
    "            prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer)\n",
    "            prompts.append(prompt)\n",
    "            epoch_task_ids.append(task_id)\n",
    "    if not prompts:\n",
    "        break\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    for task_id, responses in zip(epoch_task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    predicted_code[task_id].append(code)\n",
    "                    predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea5d29",
   "metadata": {},
   "source": [
    "#### Independent search Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b259554",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs = {key: values[:1] for key, values in predicted_outputs.items()}\n",
    "predicted_code = {key: values[:1] for key, values in predicted_code.items()}\n",
    "\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e334519",
   "metadata": {},
   "source": [
    "#### Compare with sequential search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ae140",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs = {key: values[:1] for key, values in predicted_outputs.items()}\n",
    "predicted_code = {key: values[:1] for key, values in predicted_code.items()}\n",
    "\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048, repetition_penalty=1.0)\n",
    "\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_for_novel_solutions(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer, previous_predictions=predicted_code[task_id])\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, equal = 0, 0\n",
    "repeated_code_task_ids = []\n",
    "for task_id in task_ids:\n",
    "    if len(predicted_code[task_id]) > 1:\n",
    "        total += 1\n",
    "        if predicted_code[task_id][0] == predicted_code[task_id][1]:\n",
    "            repeated_code_task_ids.append(task_id)\n",
    "            equal += 1\n",
    "print(f\"Total tasks with multiple predictions: {total}\")\n",
    "print(f\"Tasks with exactly equal predictions: {equal} ({equal/total:.2%})\")\n",
    "print(f\"Tasks with repeated code: {repeated_code_task_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc35777",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_prompt(prompts[task_ids.index(repeated_code_task_ids[0])], default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code[repeated_code_task_ids[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2651f",
   "metadata": {},
   "source": [
    "#### Compare with refine prompt approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs = {key: values[:1] for key, values in predicted_outputs.items()}\n",
    "predicted_code = {key: values[:1] for key, values in predicted_code.items()}\n",
    "\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048, repetition_penalty=1.0)\n",
    "\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_refine_prompt(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer, previous_predictions=predicted_code[task_id])\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, equal = 0, 0\n",
    "repeated_code_task_ids = []\n",
    "for task_id in task_ids:\n",
    "    if len(predicted_code[task_id]) > 1:\n",
    "        total += 1\n",
    "        if predicted_code[task_id][0] == predicted_code[task_id][1]:\n",
    "            repeated_code_task_ids.append(task_id)\n",
    "            equal += 1\n",
    "print(f\"Total tasks with multiple predictions: {total}\")\n",
    "print(f\"Tasks with exactly equal predictions: {equal} ({equal/total:.2%})\")\n",
    "print(f\"Tasks with repeated code: {repeated_code_task_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b03175",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_prompt(prompts[task_ids.index(repeated_code_task_ids[0])], default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code[repeated_code_task_ids[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271fac9",
   "metadata": {},
   "source": [
    "## Increase search diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15f6bb",
   "metadata": {},
   "source": [
    "I'm going to make experiments with 8 predictions per task, that should take around 30 minutes per experiment. Hopefully I will have enough resolution to measure changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab0568",
   "metadata": {},
   "source": [
    "### Default Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f640c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Qwen2.5-Coder-7B-Instruct\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = 8\n",
    "task_ids = list(training_challenges.keys())\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8b10f",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=n_preds, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                        tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7daf5",
   "metadata": {},
   "source": [
    "### Shuffle train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92cf08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "print(f\"Generating {n_preds} predictions for {len(task_ids)} tasks...\")\n",
    "for epoch in tqdm(range(n_preds), smoothing=0, desc=\"Generating predictions\"):\n",
    "    prompts = []\n",
    "    for task_id in task_ids:\n",
    "        prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                         tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for task_id, responses in zip(task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                predicted_code[task_id].append(code)\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9933b",
   "metadata": {},
   "source": [
    "### Effect of temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = 16\n",
    "sampling_params = SamplingParams(n=n_preds, temperature=1.2, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                        tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3166f6e",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1370bb",
   "metadata": {},
   "source": [
    "- [x] Create a prompt with the available DSL functions and the training ARC task\n",
    "- [x] Fix VLLM initialization issues with proper memory management\n",
    "- [x] Verify the effect of caching\n",
    "- [x] Generate some code that can be used to test the new BARC dsl\n",
    "- [x] Update the library to be able to select which DSL to use when executing code\n",
    "- [x] Verify that I can execute the code generated with the BARC dsl\n",
    "- [x] Add security checks to code, I have seen some input required in the code\n",
    "- [ ] Try to solve some easy task with independent sampling\n",
    "  - [x] How frequently is the dsl used?\n",
    "  - [x] Influence of the model\n",
    "  - [x] Implement output validation, and simplify the metric. Those are different responsabilities\n",
    "  - [x] Correct grids\n",
    "  - [x] Unique outputs\n",
    "  - [x] Everything into a dataframe\n",
    "  - [ ] Metrics distribution\n",
    "  - [ ] Visualization of the predictions\n",
    "  - [ ] Global metrics vs task specific analysis\n",
    "- [ ] Create a refine prompt\n",
    "- [ ] Make a more complex tree search\n",
    "- [x] Validate the tokenizer on the input grids\n",
    "\n",
    "```python\n",
    "for i in range(10):\n",
    "    print(i, {word for word in tokenizer.vocab if str(i) in word})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd122fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
