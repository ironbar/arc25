{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5ef70f",
   "metadata": {},
   "source": [
    "# Search with base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259d",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dabfd",
   "metadata": {},
   "source": [
    "Can we solve ARC tasks using base models with access to a DSL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9df7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import BeamSearchParams\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import pretty_print_prompt, Template\n",
    "from arc25.metrics import pixel_similarity_score, correct_grids_score\n",
    "import arc25.BARC_dsl as dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2076585",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ec071",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f12f27",
   "metadata": {},
   "source": [
    "https://github.com/flowersteam/SOAR/blob/main/soar/prompt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f62ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_footprint(module_name: str, show_types: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Load a module by name, then return a newline-separated list of all\n",
    "    top-level functions in it, in the form:\n",
    "\n",
    "      def func_name(arg1, arg2) -> return\n",
    "\n",
    "    If show_types=True, annotations are included; otherwise only names.\n",
    "    \"\"\"\n",
    "    mod = importlib.import_module(module_name)\n",
    "    footprints = []\n",
    "\n",
    "    for name, fn in inspect.getmembers(mod, inspect.isfunction):\n",
    "        # skip imports from elsewhere\n",
    "        if fn.__module__ != module_name or name.startswith(\"_\"):\n",
    "            continue\n",
    "\n",
    "        sig = inspect.signature(fn)\n",
    "        if not show_types:\n",
    "            # strip type info\n",
    "            params = [p.name for p in sig.parameters.values()]\n",
    "            sig_text = f\"({', '.join(params)})\"\n",
    "        else:\n",
    "            sig_text = str(sig)\n",
    "\n",
    "        footprints.append(f\"- dsl.{name}{sig_text}\")\n",
    "\n",
    "    return \"\\n\".join(footprints)\n",
    "\n",
    "print(extract_footprint('arc25.BARC_dsl', show_types=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json', 'r') as f:\n",
    "    training_challenges = json.load(f)\n",
    "\n",
    "def get_task(task_name):\n",
    "    if task_name in training_challenges:\n",
    "        task_data = training_challenges[task_name]\n",
    "        inputs = [Img(sample['input']) for sample in task_data['train']]\n",
    "        outputs = [Img(sample['output']) for sample in task_data['train']]\n",
    "        return Task(inputs=inputs, outputs=outputs, code='', name=task_name)\n",
    "    raise ValueError(f\"Task {task_name} not found in training challenges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f42cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an advanced AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks.\"\"\"\n",
    "\n",
    "prompt_template_text =\"\"\"You are tasked with solving a transformation problem from the Abstraction and Reasoning Challenge (ARC).\n",
    "Implement the transformation rules as a Python function.\n",
    "You should only write the implemented the transformation in code.\n",
    "You must write code in triple backticks (```python and then ). You must write a function called `transform` which takes a single argument, the input grid as `list[list[int]]`, and returns the transformed grid (also as `list[list[int]]`).\n",
    "\n",
    "## Key Priors:\n",
    "\n",
    "- **Objectness**: Consider the grid as containing objects (groups of connected cells) rather than just individual pixels.\n",
    "- **Goal-Directed**: The transformation should achieve a specific goal, such as creating symmetry or changing the color of specific objects.\n",
    "- **Numbers & Counting**: Keep track of the number of objects, sizes, and their relative positions.\n",
    "- **Geometry & Topology**: Use spatial relationships such as adjacency, enclosure, or symmetry.\n",
    "\n",
    "Carefully analyze the examples and find the underlying transformation logic.\n",
    "\n",
    "## Domain Specific Primitive Functions\n",
    "\n",
    "You can use the already implemented following functions to manipulate the grid:\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "The dsl has been already imported, so just simply call the functions as needed. F.e. dsl.foo()\n",
    "Do not import the dsl again, just use it directly.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are several input-output examples that illustrate the transformation.\n",
    "Your function should generalize the pattern from these examples to solve any input following the same logic.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "### Example {{ loop.index }}\n",
    "\n",
    "#### Input\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "#### Output\n",
    "\n",
    "{{ sample.output }}\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = Template(prompt_template_text)\n",
    "\n",
    "\n",
    "def create_prompt_from_task(task, grid_encoder, tokenizer, shuffle_train_samples=False, remove_last_train_sample=False):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    if remove_last_train_sample and len(train_samples) > 1:\n",
    "        train_samples = train_samples[:-1]\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc2a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1. One problem of v1 is that 13% of the tasks have predicted the exact same code. So let's rewrite a prompt to avoid that\n",
    "prompt_novel_tasks_text_v1 = prompt_template_text + \"\"\"\n",
    "## Already generated code\n",
    "\n",
    "Below are python functions already generated to try to solve the task. Take them into account when generating the new code.\n",
    "Your code should be different from the already generated code, it should generate a new and different solution to the task.\n",
    "The functions are given to avoid repeating the same code. Your code should be a new and different solution to the task.\n",
    "\n",
    "{% for prediction in previous_predictions %}\n",
    "### Code {{ loop.index }}\n",
    "\n",
    "python\n",
    "{{ prediction }}\n",
    "```\n",
    "\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "# v2, 8% exact repetitions\n",
    "prompt_novel_tasks_text_v2 = prompt_template_text + \"\"\"\n",
    "## Already generated code\n",
    "\n",
    "Below are python functions already generated to try to solve the task. \n",
    "Do not repeat them, make sure to generate a new and different solution to the task.\n",
    "\n",
    "{% for prediction in previous_predictions %}\n",
    "### Code sample{{ loop.index }}\n",
    "\n",
    "```python\n",
    "{{ prediction }}\n",
    "```\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "## New solution\n",
    "\n",
    "Now implement a new and original solution to the task.\n",
    "\"\"\"\n",
    "\n",
    "# v3, 5% of repeated code\n",
    "prompt_novel_tasks_text_v3 = prompt_template_text + \"\"\"\n",
    "## Already generated code\n",
    "\n",
    "Below are python functions already generated to try to solve the task. \n",
    "Do not repeat them, make sure to generate a new and different solution to the task.\n",
    "\n",
    "{% for prediction in previous_predictions %}\n",
    "### Code sample{{ loop.index }}\n",
    "\n",
    "DO NOT REPEAT THIS CODE, generate a new and different solution to the task.\n",
    "\n",
    "```python\n",
    "{{ prediction }}\n",
    "\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "## New solution\n",
    "\n",
    "Now implement a new and original solution to the task.\n",
    "Write some code that solves the task using a different approach than the previous ones.\n",
    "\"\"\"\n",
    "\n",
    "# system_prompt = \"\"\"You are an advanced AI assistant specialized in solving Abstract Reasoning Corpus (ARC-AGI) tasks.\n",
    "# Do not repeat any of the code snippets provided in the previous predictions. Always generate a new and different solution to the task.\"\"\"\n",
    "\n",
    "prompt_novel_tasks_template = Template(prompt_novel_tasks_text_v1)\n",
    "# TODO: add scores to the prompt\n",
    "#Pixel similarity scores: {{ prediction.pixel_similarity_scores }}\n",
    "#Correct grids scores: {{ prediction.correct_grids_scores }}\n",
    "\n",
    "def create_prompt_for_novel_solutions(task, grid_encoder, tokenizer, previous_predictions):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True),\n",
    "                         previous_predictions=previous_predictions)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_novel_tasks_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb619bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_prompt_v1 = prompt_template_text + \"\"\"\n",
    "## Code to be refined\n",
    "\n",
    "Below are python functions already generated to try to solve the task.\n",
    "Those functions do not solve the task, but the direction of the solution is correct.\n",
    "Refine them into a new function that solves the task.\n",
    "\n",
    "{% for prediction in previous_predictions %}\n",
    "### Code sample {{ loop.index }}\n",
    "\n",
    "```python\n",
    "{{ prediction }}\n",
    "```\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "## Refined solution\n",
    "\n",
    "Now implement a new function that refines the previous code and solves the task.\n",
    "\"\"\"\n",
    "\n",
    "refine_prompt_template = Template(refine_prompt_v1)\n",
    "\n",
    "\n",
    "def create_refine_prompt(task, grid_encoder, tokenizer, previous_predictions):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True),\n",
    "                         previous_predictions=previous_predictions)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": refine_prompt_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b610e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_variations = [\n",
    "\"\"\"You are tasked with solving a transformation problem from the Abstraction and Reasoning Challenge (ARC).\n",
    "Implement the transformation rules as a Python function.\n",
    "You should only write the implemented the transformation in code.\n",
    "You must write code in triple backticks (```python and then ```). You must write a function called `transform` which takes a single argument, the input grid as `list[list[int]]`, and returns the transformed grid (also as `list[list[int]]`).\n",
    "\n",
    "## Key Priors:\n",
    "\n",
    "- **Objectness**: Consider the grid as containing objects (groups of connected cells) rather than just individual pixels.\n",
    "- **Goal-Directed**: The transformation should achieve a specific goal, such as creating symmetry or changing the color of specific objects.\n",
    "- **Numbers & Counting**: Keep track of the number of objects, sizes, and their relative positions.\n",
    "- **Geometry & Topology**: Use spatial relationships such as adjacency, enclosure, or symmetry.\n",
    "\n",
    "Carefully analyze the examples and find the underlying transformation logic.\n",
    "\n",
    "## Domain Specific Primitive Functions\n",
    "\n",
    "You can use the already implemented following functions to manipulate the grid:\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "The dsl has been already imported, so just simply call the functions as needed. F.e. dsl.foo()\n",
    "Do not import the dsl again, just use it directly.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are several input-output examples that illustrate the transformation.\n",
    "Your function should generalize the pattern from these examples to solve any input following the same logic.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "### Example {{ loop.index }}\n",
    "\n",
    "#### Input\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "#### Output\n",
    "\n",
    "{{ sample.output }}\n",
    "{% endfor %}\n",
    "\"\"\",\n",
    "\"\"\"You are an expert ARC solver. Your goal is to deduce the underlying transformation logic from the provided examples and implement it in a Python function.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Your response must have two parts:\n",
    "\n",
    "1.  **Analysis**: First, explain your reasoning. Describe the pattern you've identified, the key observations, and the step-by-step logic for the transformation.\n",
    "2.  **Implementation**: Second, provide the Python code.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Analysis and Reasoning ðŸ§ \n",
    "\n",
    "Carefully observe the input-output pairs. Based on your analysis, describe the core transformation logic. Answer questions like:\n",
    "\n",
    "* What objects or patterns are present in the inputs?\n",
    "* How do these objects or patterns change to produce the outputs?\n",
    "* What is the consistent rule that applies to all examples?\n",
    "* What is your step-by-step plan to implement this rule?\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Python Implementation ðŸ’¡\n",
    "\n",
    "Now, implement the logic you described above.\n",
    "\n",
    "* Wrap your code in a single Python block (```python ... ```).\n",
    "* Create a function `transform(grid: list[list[int]]) -> list[list[int]]`.\n",
    "* Do not write any code outside of this function.\n",
    "\n",
    "---\n",
    "\n",
    "### Guiding Principles & Resources\n",
    "\n",
    "To help you, keep these concepts in mind and use the provided functions.\n",
    "\n",
    "* **Core Concepts**: Look for principles like object detection, counting, symmetry, rotation, color manipulation, and geometric relationships (e.g., containment, adjacency).\n",
    "* **Domain-Specific Functions (DSL)**: You have access to a pre-loaded library of helper functions called `dsl`. Use them directly (e.g., `dsl.some_function()`) without importing them.\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "---\n",
    "\n",
    "### Training Examples\n",
    "\n",
    "These examples demonstrate the transformation rule you must implement.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "#### Example {{ loop.index }}\n",
    "\n",
    "**Input:**\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "**Output:**\n",
    "\n",
    "{{ sample.output }}\n",
    "\n",
    "{% endfor %}\n",
    "\"\"\",\n",
    "\"\"\"You are a research AI specializing in abstract pattern recognition. Your task is to solve the provided ARC puzzle by first deducing the transformation rule and then implementing it.\n",
    "\n",
    "Your final output should be a single Python code block containing the solution. Before you write the code, you must outline your thinking process.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part 1: Formulate Your Hypothesis**\n",
    "\n",
    "Analyze the training examples to develop a theory of the transformation. Please structure your analysis as follows:\n",
    "\n",
    "1.  **General Observations:**\n",
    "    * Briefly describe the common elements and changes across all `input` -> `output` pairs. Consider colors, shapes, sizes, counts, and spatial arrangements.\n",
    "\n",
    "2.  **Core Hypothesis:**\n",
    "    * In one or two sentences, state the specific rule that you believe governs the transformation. This rule must be general enough to work for all examples.\n",
    "\n",
    "3.  **Algorithm Outline:**\n",
    "    * Provide a high-level, step-by-step plan or pseudocode that describes how to implement your hypothesis. This should be a clear blueprint for your code.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part 2: Implement Your Solution**\n",
    "\n",
    "Translate your algorithm into a Python function.\n",
    "\n",
    "* You must define a function: `transform(grid: list[list[int]]) -> list[list[int]]`.\n",
    "* Enclose your complete solution in a single ` ```python ` code block.\n",
    "* Do not include any other text or explanation outside the code block in this final part.\n",
    "\n",
    "---\n",
    "\n",
    "### **Reference Materials**\n",
    "\n",
    "* **Available Tools (`dsl`)**: You can use the functions from the `dsl` library provided below. They are already imported for your use. Just call them as `dsl.function_name()`.\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "* **Case Studies (Training Data)**: Use these input-output pairs to derive your hypothesis.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "**Case Study {{ loop.index }}**\n",
    "\n",
    "*Input Grid:*\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "\n",
    "\n",
    "*Resulting Grid:*\n",
    "\n",
    "\n",
    "{{ sample.output }}\n",
    "\n",
    "\n",
    "{% endfor %}\"\"\",\n",
    "\"\"\"You are tasked with solving a transformation problem from the Abstraction and Reasoning Challenge (ARC).\n",
    "\n",
    "Implement the transformation logic as a Python function called `transform`.\n",
    "\n",
    "Only output the code. The code must be enclosed in triple backticks like this:\n",
    "\n",
    "```python\n",
    "def transform(grid: list[list[int]]) -> list[list[int]]:\n",
    "    ...\n",
    "````\n",
    "\n",
    "Your function should take a single argument, `grid`, and return a transformed grid of the same type.\n",
    "\n",
    "## Key Priors\n",
    "\n",
    "Keep in mind the following principles to guide your implementation:\n",
    "\n",
    "* **Objectness**: Treat the grid as composed of objectsâ€”groups of connected cellsâ€”rather than individual pixels.\n",
    "* **Goal-Directedness**: Assume the transformation pursues a goal, such as restoring symmetry, recoloring specific objects, or rearranging structures.\n",
    "* **Counting & Structure**: Pay attention to the number, size, and spatial arrangement of objects.\n",
    "* **Geometry & Topology**: Consider relationships like adjacency, symmetry, containment, alignment, or enclosure.\n",
    "\n",
    "Analyze the examples carefully to uncover the general transformation logic.\n",
    "\n",
    "## Available Tools\n",
    "\n",
    "You may use the following domain-specific primitive functions, already available in the `dsl` module:\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "These have been imported for you. Call them directly as needed, e.g., `dsl.some_function(...)`.\n",
    "\n",
    "Do **not** re-import the module.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Study the input-output examples below. Your function should generalize from these to handle unseen inputs that follow the same transformation rule.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "\n",
    "### Example {{ loop.index }}\n",
    "\n",
    "#### Input\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "#### Output\n",
    "\n",
    "{{ sample.output }}\n",
    "\n",
    "{% endfor %}\n",
    "\"\"\",\n",
    "\"\"\"You are a reasoning agent solving an ARC (Abstraction and Reasoning Corpus) task.\n",
    "\n",
    "Your job is to discover the transformation rule underlying several input-output grid pairs, and implement this rule in Python.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Write a function named `transform(grid: list[list[int]]) -> list[list[int]]` that implements the inferred rule.\n",
    "\n",
    "- Output **only the code**, inside triple backticks like so:\n",
    "\n",
    "```python\n",
    "def transform(grid: list[list[int]]) -> list[list[int]]:\n",
    "    ...\n",
    "````\n",
    "\n",
    "* The code should work for any input grid that follows the same transformation logic.\n",
    "\n",
    "## Reasoning Heuristics\n",
    "\n",
    "Use the following principles to guide your analysis:\n",
    "\n",
    "* **Objects, not pixels**: Identify groups of connected cells that behave as coherent entities.\n",
    "* **Functional transformation**: The output is not arbitraryâ€”it results from applying a consistent operation or goal to the input.\n",
    "* **Count, compare, align**: Track quantities, positions, symmetries, or relative motion between elements.\n",
    "* **Local vs global**: Consider whether the rule acts on each object individually or on the grid as a whole.\n",
    "\n",
    "## Tools\n",
    "\n",
    "You are allowed to use predefined helper functions provided via the `dsl` module:\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "These are already available in the environment. Call them directly as needed, e.g., `dsl.some_function(...)`.\n",
    "\n",
    "Do **not** re-import or redefine the `dsl`.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Analyze the following training examples to understand the transformation. Your function should generalize the rule that maps each input to its corresponding output.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "\n",
    "### Example {{ loop.index }}\n",
    "\n",
    "#### Input\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "#### Output\n",
    "\n",
    "{{ sample.output }}\n",
    "\n",
    "{% endfor %}\n",
    "\"\"\",\n",
    "\"\"\"You are an expert AI programmer specializing in solving Abstraction and Reasoning Challenge (ARC) problems.\n",
    "\n",
    "Your goal is to deduce the transformation logic from the provided examples and implement it in a single Python function.\n",
    "\n",
    "## 1. Analyze the Examples\n",
    "\n",
    "Carefully study the input-output pairs below. Identify the underlying pattern, rule, or transformation that connects each input grid to its corresponding output grid.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "### Example {{ loop.index }}\n",
    "**Input:**\n",
    "\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "\n",
    "**Output:**\n",
    "\n",
    "\n",
    "{{ sample.output }}\n",
    "\n",
    "\n",
    "{% endfor %}\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Key Concepts for Reasoning\n",
    "\n",
    "As you analyze, consider the following principles to uncover the solution:\n",
    "- **Object-Based Analysis:** Treat connected groups of same-colored pixels as distinct objects. Analyze their properties (size, shape, color) and relationships (position, containment, adjacency).\n",
    "- **Symmetry & Repetition:** Look for patterns involving symmetry (reflection, rotation), repetition, or fractals.\n",
    "- **Counting & Logic:** The solution may depend on the number of objects, the count of pixels of a certain color, or other numerical properties.\n",
    "- **Geometric Transformations:** Think about transformations like moving, scaling, rotating, cropping, or copying objects.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Available Tools\n",
    "\n",
    "You have access to a pre-loaded library of helper functions available through the `dsl` object. You can and should use these functions to implement the transformation logic.\n",
    "\n",
    "**Available `dsl` functions:**\n",
    "\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "\n",
    "**Note:** The `dsl` is already imported. Call functions directly, for example: `dsl.some_function(...)`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Implementation\n",
    "\n",
    "Now, write the Python function that solves the task.\n",
    "\n",
    "### Requirements:\n",
    "- The function must be named `transform`.\n",
    "- It must accept one argument: `grid` (`list[list[int]]`).\n",
    "- It must return the transformed grid (`list[list[int]]`).\n",
    "- **Your entire response must be a single Python code block.** Do not write any explanations, comments, or text outside of the ```python ... ``` block.\n",
    "\"\"\",\n",
    "\"\"\"You are solving a transformation task from the Abstraction and Reasoning Challenge (ARC).\n",
    "\n",
    "Write a Python function to implement the transformation. Provide only the function code, wrapped in triple backticks:\n",
    "\n",
    "```python\n",
    "def transform(grid: list[list[int]]) -> list[list[int]]:\n",
    "    # your implementation here\n",
    "````\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "* **Objectness**: Treat connected cells as objects, not isolated pixels.\n",
    "* **Goalâ€‘Directed**: Aim for a specific outcome (e.g., symmetry, color change).\n",
    "* **Counting**: Track object counts, sizes, and positions.\n",
    "* **Spatial Reasoning**: Leverage adjacency, enclosure, and symmetry.\n",
    "\n",
    "Analyze the examples to infer the underlying rule.\n",
    "\n",
    "### Available Primitives\n",
    "\n",
    "You may call any of these preâ€‘imported DSL functions directly (e.g., `dsl.foo()`):\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "> **Do not** import the DSL module again.\n",
    "\n",
    "### Examples\n",
    "\n",
    "Use the following I/O pairs to generalize the rule. Your `transform` must handle any input that follows the same pattern.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "\n",
    "#### Example {{ loop.index }}\n",
    "\n",
    "**Input**\n",
    "\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "\n",
    "{{ sample.output }}\n",
    "\n",
    "\n",
    "{% endfor %}\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "You are solving an Abstraction and Reasoning Challenge (ARC) transformation task.\n",
    "\n",
    "Implement the grid transformation as a Python function.\n",
    "\n",
    "**Requirements**  \n",
    "- Provide only the code for your transformation.  \n",
    "- Enclose your code in triple backticks with a Python syntax hint:  \n",
    "  ```python  \n",
    "  # your solution  \n",
    "````\n",
    "\n",
    "* Define a function `transform(input_grid: list[list[int]]) -> list[list[int]]`\n",
    "  that returns the correctly transformed grid.\n",
    "\n",
    "## Key Priors\n",
    "\n",
    "1. **Objectness**\n",
    "   Treat contiguous regions of the same color as objects, not isolated pixels.\n",
    "2. **Goal-Directed**\n",
    "   Identify and apply the specific transformation goal (e.g., symmetry, color-change).\n",
    "3. **Counting & Numbers**\n",
    "   Count objects, compare sizes, and use relative positioning.\n",
    "4. **Geometry & Topology**\n",
    "   Leverage spatial relationships (adjacency, enclosure, symmetry).\n",
    "\n",
    "Analyze the examples to infer the underlying rule and generalize it.\n",
    "\n",
    "## Domain-Specific Primitives\n",
    "\n",
    "You may call any of the imported DSL functions directly.\n",
    "Example usage:\n",
    "\n",
    "```python\n",
    "dsl.foo(...)\n",
    "```\n",
    "\n",
    "*Do not* re-import the DSL module.\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are training examples illustrating the desired transformation. Your solution should generalize from these.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "\n",
    "### Example {{ loop.index }}\n",
    "\n",
    "**Input**\n",
    "\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "\n",
    "**Output**\n",
    "\n",
    "\n",
    "{{ sample.output }}\n",
    "\n",
    "\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "]\n",
    "len(prompt_variations)\n",
    "def create_prompt_variations_from_task(idx, task, grid_encoder, tokenizer, shuffle_train_samples=False):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    prompt_template = Template(prompt_variations[idx])\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_suggestions_for_prompt(module_name: str = 'arc25.BARC_dsl') -> str:\n",
    "    mod = importlib.import_module(module_name)\n",
    "    func_names = []\n",
    "    for name, fn in inspect.getmembers(mod, inspect.isfunction):\n",
    "        if fn.__module__ != module_name or name.startswith(\"_\"):\n",
    "            continue\n",
    "        func_names.append(name)\n",
    "    selection = random.sample(func_names, min(5, len(func_names)))\n",
    "    suggestion = '\\n\\n**Bonus**: You will get bonus points if you use any of the following dsl functions in your solution:\\n\\n'\n",
    "    suggestion += '\\n'.join(f\"- dsl.{name}\" for name in selection)\n",
    "    return suggestion\n",
    "\n",
    "\n",
    "def create_prompt_with_dsl_suggestions(task, grid_encoder, tokenizer, shuffle_train_samples=False):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    prompt_template = Template(prompt_template_text + create_random_suggestions_for_prompt())\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae964737",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1, max_model_len=32000):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=0.9,  # Use less GPU memory\n",
    "        # max_model_len=4096,  # Limit context length\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea848918",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae210ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_python_code(text):\n",
    "    # Extract Python code from the text\n",
    "    if '```python' not in text:\n",
    "        return ''\n",
    "    code = text.split('```python')[1]\n",
    "    if not '```' in code:\n",
    "        return ''\n",
    "\n",
    "    code = code.split('```')[0].strip()\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_python_code(code):\n",
    "    remove_line_keywords = ['import dsl', 'from dsl import ', 'print(']\n",
    "    code = '\\n'.join(line for line in code.split('\\n') if not any(keyword in line for keyword in remove_line_keywords))\n",
    "    return code.strip()\n",
    "\n",
    "def add_additional_imports(code):\n",
    "    additional_imports = [\n",
    "        'from typing import List, Tuple',\n",
    "        'import numpy as np',\n",
    "        'import numpy'\n",
    "    ]\n",
    "    imports = '\\n'.join(additional_imports)\n",
    "    return imports + '\\n' + code if code else imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee204f",
   "metadata": {},
   "source": [
    "### Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_outputs(outputs):\n",
    "    if not outputs:\n",
    "        raise ValueError(\"Outputs list is empty\")\n",
    "    return [_validate_output(output) for output in outputs]\n",
    "\n",
    "def _validate_output(output):\n",
    "    if output is None:\n",
    "        raise ValueError(\"Output is None\")\n",
    "    output = np.array(output) # otherwise I see weird outputs that mix list and numpy arrays\n",
    "    if output.ndim != 2:\n",
    "        raise ValueError(f\"Output is not a 2D array. Output shape: {output.shape}\")\n",
    "    if max(output.shape) > 35:\n",
    "        raise ValueError(f\"Output is too large, the maximum allowed shape is 30x30. Output shape: {output.shape}\")\n",
    "    if min(output.shape) == 0:\n",
    "        raise ValueError(f\"Output has zero dimension, it is empty. Output shape: {output.shape}\")\n",
    "    if np.max(output) > 9 or np.min(output) < 0:\n",
    "        raise ValueError(f\"Output contains invalid values, expected values in range [0, 9]. Output max: {np.max(output)}, min: {np.min(output)}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee616990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def fingerprint(prediction):\n",
    "    \"\"\"\n",
    "    Create a compact hash for a list of matrices.\n",
    "    Includes shape & dtype to distinguish e.g. (2Ã—2) from (4Ã—1).\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    for m in prediction:\n",
    "        # incorporate shape and dtype in a reproducible way\n",
    "        h.update(str(m.shape).encode())\n",
    "        h.update(m.dtype.str.encode())\n",
    "        # raw data bytes\n",
    "        h.update(m.tobytes())\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b4bd9",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds):\n",
    "    df = pd.DataFrame(columns=['valid code', 'valid outputs', 'unique outputs', 'dsl usage', 'pixel similarity', 'correct grids', 'solved task'])\n",
    "    for task_id in task_ids:\n",
    "        df.loc[task_id, 'valid code'] = len(predicted_code[task_id])/n_preds\n",
    "        df.loc[task_id, 'valid outputs'] = len(predicted_outputs[task_id])/n_preds\n",
    "        df.loc[task_id, 'unique outputs'] = len(set(fingerprint(output) for output in predicted_outputs[task_id]))/n_preds\n",
    "        df.loc[task_id, 'dsl usage'] = sum(1 for code in predicted_code[task_id] if 'dsl.' in code)/n_preds\n",
    "\n",
    "        task = get_task(task_id)\n",
    "        task_predicted_outputs = predicted_outputs[task_id]\n",
    "        scores = sorted([np.mean([pixel_similarity_score(output, pred) for output, pred in zip(task.outputs, predictions)]) for predictions in task_predicted_outputs])\n",
    "        df.loc[task_id, 'pixel similarity'] = np.mean(scores) if scores else 0.0\n",
    "\n",
    "        task_outputs = [np.array(output) for output in task.outputs]\n",
    "        scores = sorted([correct_grids_score(task_outputs, predictions) for predictions in task_predicted_outputs])\n",
    "        df.loc[task_id, 'correct grids'] = np.mean(scores) if scores else 0.0\n",
    "        df.loc[task_id, 'solved task'] = int(np.max(scores) == 1) if scores else 0\n",
    "\n",
    "    df.loc['MEAN'] = df.mean(axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6171a",
   "metadata": {},
   "source": [
    "## Independent search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aefc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Qwen2.5-Coder-7B-Instruct\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)\n",
    "\n",
    "# model_path = \"/home/gbarbadillo/models/Qwen3-4B\"\n",
    "# llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)\n",
    "\n",
    "# model_path = '/home/gbarbadillo/models/Qwen2.5-Coder-14B-Instruct-GGUF/qwen2.5-coder-14b-instruct-q4_k_m.gguf' # Needs 2 GPUs\n",
    "# llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=2, max_model_len=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = list(training_challenges.keys())\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "prompts = [create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer) for task_id in task_ids]\n",
    "if 'Qwen3' in model_path:\n",
    "    # disable thinking without using the chat template\n",
    "    prompts = [prompt + '<think>\\n\\n</think>\\n\\n' for prompt in prompts]\n",
    "\n",
    "t0 = time.time()\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in outputs)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(outputs):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(outputs) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            code = curate_python_code(code)\n",
    "            predicted_code[task_id].append(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58674fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, sampling_params.n)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2430fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'{os.path.basename(model_path)}_{len(task_ids)}tasks_{sampling_params.n}preds_{int(inference_time)}runtime.csv'\n",
    "df.to_csv(output_path, index_label='task_id')\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816116aa",
   "metadata": {},
   "source": [
    "## Sequential search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d5573",
   "metadata": {},
   "source": [
    "Let's give the model the context of all the functions generated so far to try induce more diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9404b6ff",
   "metadata": {},
   "source": [
    "### Prompt tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cafa907",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Qwen2.5-Coder-7B-Instruct\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6dcd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = list(training_challenges.keys())\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258b508",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb484477",
   "metadata": {},
   "source": [
    "As a first step let's make sure that we have a single prediction that predicts a valid output for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb64856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first prediction initialization\n",
    "while True:\n",
    "    prompts, epoch_task_ids = [], []\n",
    "    for task_id in task_ids:\n",
    "        if not predicted_code[task_id]:  # only create prompt if no code has been predicted yet\n",
    "            prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer)\n",
    "            prompts.append(prompt)\n",
    "            epoch_task_ids.append(task_id)\n",
    "    if not prompts:\n",
    "        break\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    for task_id, responses in zip(epoch_task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    predicted_code[task_id].append(code)\n",
    "                    predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea5d29",
   "metadata": {},
   "source": [
    "#### Independent search Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b259554",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs = {key: values[:1] for key, values in predicted_outputs.items()}\n",
    "predicted_code = {key: values[:1] for key, values in predicted_code.items()}\n",
    "\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e334519",
   "metadata": {},
   "source": [
    "#### Compare with sequential search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ae140",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs = {key: values[:1] for key, values in predicted_outputs.items()}\n",
    "predicted_code = {key: values[:1] for key, values in predicted_code.items()}\n",
    "\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048, repetition_penalty=1.0)\n",
    "\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_for_novel_solutions(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer, previous_predictions=predicted_code[task_id])\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, equal = 0, 0\n",
    "repeated_code_task_ids = []\n",
    "for task_id in task_ids:\n",
    "    if len(predicted_code[task_id]) > 1:\n",
    "        total += 1\n",
    "        if predicted_code[task_id][0] == predicted_code[task_id][1]:\n",
    "            repeated_code_task_ids.append(task_id)\n",
    "            equal += 1\n",
    "print(f\"Total tasks with multiple predictions: {total}\")\n",
    "print(f\"Tasks with exactly equal predictions: {equal} ({equal/total:.2%})\")\n",
    "print(f\"Tasks with repeated code: {repeated_code_task_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc35777",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_prompt(prompts[task_ids.index(repeated_code_task_ids[0])], default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afafbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code[repeated_code_task_ids[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2651f",
   "metadata": {},
   "source": [
    "#### Compare with refine prompt approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_outputs = {key: values[:1] for key, values in predicted_outputs.items()}\n",
    "predicted_code = {key: values[:1] for key, values in predicted_code.items()}\n",
    "\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048, repetition_penalty=1.0)\n",
    "\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_refine_prompt(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer, previous_predictions=predicted_code[task_id])\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total, equal = 0, 0\n",
    "repeated_code_task_ids = []\n",
    "for task_id in task_ids:\n",
    "    if len(predicted_code[task_id]) > 1:\n",
    "        total += 1\n",
    "        if predicted_code[task_id][0] == predicted_code[task_id][1]:\n",
    "            repeated_code_task_ids.append(task_id)\n",
    "            equal += 1\n",
    "print(f\"Total tasks with multiple predictions: {total}\")\n",
    "print(f\"Tasks with exactly equal predictions: {equal} ({equal/total:.2%})\")\n",
    "print(f\"Tasks with repeated code: {repeated_code_task_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b03175",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_prompt(prompts[task_ids.index(repeated_code_task_ids[0])], default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b68fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code[repeated_code_task_ids[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271fac9",
   "metadata": {},
   "source": [
    "## Increase search diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15f6bb",
   "metadata": {},
   "source": [
    "I'm going to make experiments with 8 predictions per task, that should take around 30 minutes per experiment. Hopefully I will have enough resolution to measure changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab0568",
   "metadata": {},
   "source": [
    "### Default Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f640c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Qwen2.5-Coder-7B-Instruct\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = 8\n",
    "task_ids = list(training_challenges.keys())\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8b10f",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25d871",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=n_preds, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                        tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7daf5",
   "metadata": {},
   "source": [
    "### Shuffle train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92cf08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "print(f\"Generating {n_preds} predictions for {len(task_ids)} tasks...\")\n",
    "for epoch in tqdm(range(n_preds), smoothing=0, desc=\"Generating predictions\"):\n",
    "    prompts = []\n",
    "    for task_id in task_ids:\n",
    "        prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                         tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for task_id, responses in zip(task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                predicted_code[task_id].append(code)\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9933b",
   "metadata": {},
   "source": [
    "### Effect of temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = 16\n",
    "sampling_params = SamplingParams(n=n_preds, temperature=1.2, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                        tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf87be91",
   "metadata": {},
   "source": [
    "### Prompt variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "print(f\"Generating {n_preds} predictions for {len(task_ids)} tasks...\")\n",
    "for epoch in tqdm(range(n_preds), smoothing=0, desc=\"Generating predictions\"):\n",
    "    prompts = []\n",
    "    for task_id in task_ids:\n",
    "        prompt = create_prompt_variations_from_task(\n",
    "            epoch, get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "        prompts.append(prompt)\n",
    "    # pretty_print_prompt(prompts[0], default_color='white')\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for task_id, responses in zip(task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                predicted_code[task_id].append(code)\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535e876",
   "metadata": {},
   "source": [
    "Would this be faster if I give all the prompts at once? I have tested it and no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b647e56",
   "metadata": {},
   "source": [
    "### Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527361a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = BeamSearchParams(beam_width=n_preds, temperature=1.0, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                        tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "    prompts.append(dict(prompt=prompt))\n",
    "\n",
    "outputs = llm.beam_search(prompts, params=sampling_params)\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0660bf8",
   "metadata": {},
   "source": [
    "GPU usage is not constant, average should be around 50%.  \n",
    "After 8 minutes it has not ended the prediction for just 5 tasks. Making predictions for 400 tasks will take more than 10 hours.\n",
    "I have stopped it after 11 minutes without response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85d6af",
   "metadata": {},
   "source": [
    "### Add suggestion to use some of the DSL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "print(f\"Generating {n_preds} predictions for {len(task_ids)} tasks...\")\n",
    "for epoch in tqdm(range(n_preds), smoothing=0, desc=\"Generating predictions\"):\n",
    "    prompts = []\n",
    "    for task_id in task_ids:\n",
    "        prompt = create_prompt_with_dsl_suggestions(\n",
    "            get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "        prompts.append(prompt)\n",
    "    # pretty_print_prompt(prompts[0], default_color='white')\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for task_id, responses in zip(task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                predicted_code[task_id].append(code)\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ce248",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44546e4",
   "metadata": {},
   "source": [
    "The challenge of data augmentation is that we have to be able to undo the data augmentation to see if the solution was correct or unique.\n",
    "\n",
    "I'm going to try to find the code of the previous year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cbe27",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa78893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_data_augmentation(task, hflip, n_rot90, color_map=None):\n",
    "    augmented_task = Task(\n",
    "        inputs = [geometric_augmentation(grid, hflip, n_rot90) for grid in task.inputs],\n",
    "        outputs = [geometric_augmentation(grid, hflip, n_rot90) for grid in task.outputs],\n",
    "        code = '',\n",
    "        name = task.name,\n",
    "    )\n",
    "    if color_map is not None:\n",
    "        augmented_task = swap_task_colors(augmented_task, color_map)\n",
    "    return augmented_task\n",
    "\n",
    "\n",
    "def revert_data_augmentation(grid, hflip, n_rot90, color_map=None):\n",
    "    grid = revert_geometric_augmentation(grid, hflip, n_rot90)\n",
    "    if color_map is not None:\n",
    "        grid = revert_color_swap(grid, color_map)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def geometric_augmentation(grid, hflip, n_rot90):\n",
    "    grid = np.array(grid)\n",
    "    if hflip:\n",
    "        grid = np.flip(grid, axis=1)\n",
    "    grid = np.rot90(grid, k=n_rot90)\n",
    "    return grid.tolist()\n",
    "\n",
    "\n",
    "def revert_geometric_augmentation(grid, hflip, n_rot90):\n",
    "    grid = np.array(grid)\n",
    "    grid = np.rot90(grid, k=-n_rot90)\n",
    "    if hflip:\n",
    "        grid = np.flip(grid, axis=1)\n",
    "    return grid\n",
    "\n",
    "def revert_color_swap(grid, color_map):\n",
    "    reverse_color_map = {v: k for k, v in color_map.items()}\n",
    "    vectorized_mapping = np.vectorize(reverse_color_map.get)\n",
    "    return vectorized_mapping(grid)\n",
    "\n",
    "\n",
    "def swap_task_colors(task, color_map=None, change_background_probability=0.1):\n",
    "    if color_map is None:\n",
    "        color_map = get_random_color_map(change_background_probability)\n",
    "    vectorized_mapping = np.vectorize(color_map.get)\n",
    "    new_task = Task(\n",
    "        inputs = [vectorized_mapping(grid) for grid in task.inputs],\n",
    "        outputs = [vectorized_mapping(grid) for grid in task.outputs],\n",
    "        code = '',\n",
    "        name = task.name,)\n",
    "    return new_task\n",
    "\n",
    "\n",
    "def get_random_data_augmentation_params():\n",
    "    params = get_random_geometric_augmentation_params()\n",
    "    params['color_map'] = get_random_color_map()\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_random_geometric_augmentation_params():\n",
    "    return dict(hflip=random.choice([True, False]), n_rot90=random.choice([0, 1, 2, 3]))\n",
    "\n",
    "\n",
    "def get_random_color_map(change_background_probability=0.1):\n",
    "    colors = list(range(10))\n",
    "    if random.random() < change_background_probability:\n",
    "        new_colors = list(range(10))\n",
    "        random.shuffle(new_colors)\n",
    "    else:\n",
    "        new_colors = list(range(1, 10))\n",
    "        random.shuffle(new_colors)\n",
    "        new_colors = [0] + new_colors\n",
    "\n",
    "    color_map = {x: y for x, y in zip(colors, new_colors)}\n",
    "    return color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15233233",
   "metadata": {},
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "print(f\"Generating {n_preds} predictions for {len(task_ids)} tasks...\")\n",
    "for epoch in tqdm(range(n_preds), smoothing=0, desc=\"Generating predictions\"):\n",
    "    prompts, data_augmentation_params = [], []\n",
    "    for task_id in task_ids:\n",
    "        params = get_random_data_augmentation_params()\n",
    "        data_augmentation_params.append(params)\n",
    "        task = get_task(task_id)\n",
    "        task = apply_data_augmentation(task, **params)\n",
    "        prompt = create_prompt_from_task(\n",
    "            task, grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "        prompts.append(prompt)\n",
    "    # pretty_print_prompt(prompts[0], default_color='white')\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for task_id, responses, params in zip(task_ids, outputs, data_augmentation_params):\n",
    "        task = get_task(task_id)\n",
    "        task = apply_data_augmentation(task, **params)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                predicted_code[task_id].append(code)\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    task_predicted_outputs = [revert_data_augmentation(output, **params) for output in task_predicted_outputs]\n",
    "                    predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c17911",
   "metadata": {},
   "source": [
    "### Remove one training sample (constraint relaxation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "print(f\"Generating {n_preds} predictions for {len(task_ids)} tasks...\")\n",
    "for epoch in tqdm(range(n_preds), smoothing=0, desc=\"Generating predictions\"):\n",
    "    prompts = []\n",
    "    for task_id in task_ids:\n",
    "        prompt = create_prompt_from_task(\n",
    "            get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer,\n",
    "            shuffle_train_samples=True, remove_last_train_sample=epoch != 0)\n",
    "        prompts.append(prompt)\n",
    "    # pretty_print_prompt(prompts[0], default_color='white')\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for task_id, responses in zip(task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                predicted_code[task_id].append(code)\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f7157",
   "metadata": {},
   "source": [
    "### Multiple functions per prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3318fd5",
   "metadata": {},
   "source": [
    "Maybe if I ask to generate multiple functions on the same prompt, the model will have the intelligence to take different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeffd85d",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_multiple_functions_template_text = \"\"\"You are tasked with solving a transformation problem from the Abstraction and Reasoning Challenge (ARC).\n",
    "Implement the transformation rules as a Python function.\n",
    "You should only write the implemented the transformation in code.\n",
    "You must write code in triple backticks (```python and then ). You must write a function called `transform` which takes a single argument, the input grid as `list[list[int]]`, and returns the transformed grid (also as `list[list[int]]`).\n",
    "\n",
    "## Key Priors:\n",
    "\n",
    "- **Objectness**: Consider the grid as containing objects (groups of connected cells) rather than just individual pixels.\n",
    "- **Goal-Directed**: The transformation should achieve a specific goal, such as creating symmetry or changing the color of specific objects.\n",
    "- **Numbers & Counting**: Keep track of the number of objects, sizes, and their relative positions.\n",
    "- **Geometry & Topology**: Use spatial relationships such as adjacency, enclosure, or symmetry.\n",
    "\n",
    "Carefully analyze the examples and find the underlying transformation logic.\n",
    "\n",
    "## Domain Specific Primitive Functions\n",
    "\n",
    "You can use the already implemented following functions to manipulate the grid:\n",
    "\n",
    "{{ dsl }}\n",
    "\n",
    "The dsl has been already imported, so just simply call the functions as needed. F.e. dsl.foo()\n",
    "Do not import the dsl again, just use it directly.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Below are several input-output examples that illustrate the transformation.\n",
    "Your function should generalize the pattern from these examples to solve any input following the same logic.\n",
    "\n",
    "{% for sample in train_samples %}\n",
    "### Example {{ loop.index }}\n",
    "\n",
    "#### Input\n",
    "\n",
    "{{ sample.input }}\n",
    "\n",
    "#### Output\n",
    "\n",
    "{{ sample.output }}\n",
    "{% endfor %}\n",
    "\n",
    "## Code\n",
    "\n",
    "Implement the transformation rules as a Python function.\n",
    "Please write {{ n }} distinct implementations of the `transform` function, each solving the same problem in a different way.\n",
    "Place each implementation in a separate code block, and ensure that each implementation is unique.\n",
    "\"\"\"\n",
    "prompt_multiple_functions_template = Template(prompt_multiple_functions_template_text)\n",
    "\n",
    "def create_prompt_requesting_multiple_functions_from_task(\n",
    "        n_output_functions, task, grid_encoder, tokenizer, shuffle_train_samples=False, remove_last_train_sample=False):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    if remove_last_train_sample and len(train_samples) > 1:\n",
    "        train_samples = train_samples[:-1]\n",
    "    render_kwargs = dict(train_samples=train_samples, n=n_output_functions,\n",
    "                         dsl=extract_footprint('arc25.BARC_dsl', show_types=True))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_multiple_functions_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6945a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_multiple_python_code_snippets(text):\n",
    "    # Extract Python code from the text\n",
    "    if '```python' not in text:\n",
    "        return\n",
    "    for code in text.split('```python')[1:]:\n",
    "        if '```' in code:\n",
    "            code = code.split('```')[0].strip()\n",
    "            if code:\n",
    "                yield code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc5cc83",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_functions_per_prediction = 8\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=8192)\n",
    "predictions = {key: [] for key in task_ids}\n",
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "output_tokens = []\n",
    "print(f\"Generating {n_preds} predictions for {len(task_ids)} tasks...\")\n",
    "for epoch in tqdm(range(n_preds//n_functions_per_prediction), smoothing=0, desc=\"Generating predictions\"):\n",
    "    prompts = []\n",
    "    for task_id in task_ids:\n",
    "        prompt = create_prompt_requesting_multiple_functions_from_task(\n",
    "            n_output_functions = n_functions_per_prediction,\n",
    "            task=get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer,\n",
    "            shuffle_train_samples=True, remove_last_train_sample=False)\n",
    "        prompts.append(prompt)\n",
    "    # pretty_print_prompt(prompts[0], default_color='white')\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    for task_id, responses in zip(task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        output_tokens.append(len(responses.outputs[0].token_ids))\n",
    "        predictions[task_id].append(responses.outputs[0].text)\n",
    "        for code in parse_multiple_python_code_snippets(responses.outputs[0].text):\n",
    "            predicted_code[task_id].append(code)\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}: {type(e)} {e}\")\n",
    "    print([len(codes) for codes in predicted_code.values()])\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "print(f'Max tokens per output: {max(output_tokens)}')\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8623cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction in predictions[task_ids[np.argmin([len(codes) for codes in predicted_code.values()])]]:\n",
    "    print(prediction)\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63275072",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code['007bbfb7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e1e20",
   "metadata": {},
   "source": [
    "## Solution refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3964a01a",
   "metadata": {},
   "source": [
    "### Default Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ce3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Qwen2.5-Coder-7B-Instruct\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9fc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_preds = 16\n",
    "task_ids = list(training_challenges.keys())\n",
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff396eb",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15895f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction = namedtuple(\"Prediction\", ['code', 'outputs', 'exception', 'scores'])\n",
    "\n",
    "def format_exception(e):\n",
    "    message = str(e).replace('arc25.BARC_dsl', 'dsl')\n",
    "    return f\"{e.__class__.__name__}: {message}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42595ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=n_preds, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "predictions = {key: [] for key in task_ids}\n",
    "t0 = time.time()\n",
    "prompts = []\n",
    "for task_id in task_ids:\n",
    "    prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                        tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for task_id, responses in zip(task_ids, outputs):\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(responses.outputs):\n",
    "        code = parse_python_code(output.text)\n",
    "        if code:\n",
    "            code = curate_python_code(code)\n",
    "            try:\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                prediction = Prediction(\n",
    "                    code=code, outputs=task_predicted_outputs, exception=None,\n",
    "                    scores=[pixel_similarity_score(output, pred) for output, pred in zip(task.outputs, task_predicted_outputs)])\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {format_exception(e)}\")\n",
    "                prediction = Prediction(code=code, outputs=None, exception=format_exception(e),\n",
    "                                        scores=None)\n",
    "            predictions[task_id].append(prediction)\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "max_output_tokens = max(max(len(output.token_ids) for output in responses.outputs) for responses in outputs)\n",
    "print(f'Max output tokens: {max_output_tokens}')\n",
    "predicted_code = {key: [pred.code for pred in values] for key, values in predictions.items()}\n",
    "predicted_outputs = {key: [pred.outputs for pred in values if pred.outputs is not None] for key, values in predictions.items()}\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2295b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_execution_feedback_template_text = \"\"\"## Execution feedback\n",
    "\n",
    "Please refine the code to ensure it produces the expected outputs for all examples.\n",
    "\n",
    "### Execution outputs\n",
    "\n",
    "These are the outputs of the code execution for each example. Compare them with the expected\n",
    "outputs and refine the code if necessary to achieve the desired transformation.\n",
    "{% for output in outputs %}\n",
    "#### Example {{ loop.index }} execution output\n",
    "\n",
    "{{ output }}\n",
    "{% endfor %}\n",
    "\n",
    "### Metrics\n",
    "\n",
    "These are the pixel similarity scores for each sample: {{ scores}}\n",
    "All scores need to be 1.0 to consider the code correct.\n",
    "\"\"\"\n",
    "successful_execution_feedback_template = Template(successful_execution_feedback_template_text)\n",
    "\n",
    "failed_execution_feedback_template_text = \"\"\"## Execution feedback\n",
    "\n",
    "Execution of the code failed with the following exception:\n",
    "```\n",
    "{{ exception }}\n",
    "```\n",
    "\n",
    "Please refine the code to ensure it executes successfully and produces the expected outputs for all examples.\n",
    "\"\"\"\n",
    "failed_execution_feedback_template = Template(failed_execution_feedback_template_text)\n",
    "\n",
    "def generate_execution_feedback_message(prediction, grid_encoder=grid_encoder):\n",
    "    if prediction.outputs is None:\n",
    "        return failed_execution_feedback_template.render(exception=prediction.exception)\n",
    "    outputs = [grid_encoder.to_text(output) for output in prediction.outputs]\n",
    "    scores = np.array(prediction.scores).round(2).tolist()\n",
    "    feedback_message = successful_execution_feedback_template.render(outputs=outputs, scores=scores)\n",
    "    return feedback_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a404d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_refine_prompt_from_task(task, prediction, grid_encoder, tokenizer, shuffle_train_samples=False, remove_last_train_sample=False):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs, task.outputs)]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    if remove_last_train_sample and len(train_samples) > 1:\n",
    "        train_samples = train_samples[:-1]\n",
    "    render_kwargs = dict(train_samples=train_samples, dsl=extract_footprint('arc25.BARC_dsl', show_types=True))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)},\n",
    "                {'role': 'assistant', 'content': f'```python\\n{prediction.code}\\n```'},\n",
    "                {'role': 'user', 'content': generate_execution_feedback_message(prediction, grid_encoder=grid_encoder)},\n",
    "               ]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3659672",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in tqdm(range(n_preds), smoothing=0):\n",
    "    print(f\"Generating {n_preds} predictions for {len(task_ids)} tasks, epoch {epoch + 1}/{n_preds}...\")\n",
    "    prompts = []\n",
    "    for task_id in task_ids:\n",
    "        if predictions[task_id]:\n",
    "            # prediction = random.choice(predictions[task_id]) # random choose\n",
    "            prediction = predictions[task_id][epoch % len(predictions[task_id])]  # loop over the predictions\n",
    "            prompt = create_refine_prompt_from_task(get_task(task_id), prediction, grid_encoder=grid_encoder,\n",
    "                                                    tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "        else:\n",
    "            # If no previous prediction, use the original task prompt\n",
    "            logging.warning(f\"No previous prediction for task {task_id}, using original task prompt.\")\n",
    "            prompt = create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder,\n",
    "                                                tokenizer=tokenizer, shuffle_train_samples=False)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    max_output_tokens = max(max(len(output.token_ids) for output in responses.outputs) for responses in outputs)\n",
    "    print(f'Max output tokens: {max_output_tokens}')\n",
    "    for task_id, responses in zip(task_ids, outputs):\n",
    "        task = get_task(task_id)\n",
    "        for i, output in enumerate(responses.outputs):\n",
    "            code = parse_python_code(output.text)\n",
    "            if code:\n",
    "                code = curate_python_code(code)\n",
    "                try:\n",
    "                    task_predicted_outputs = safe_code_execution(add_additional_imports(code), task.inputs, func_name='transform', dsl=dsl)\n",
    "                    task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                    prediction = Prediction(\n",
    "                        code=code, outputs=task_predicted_outputs, exception=None,\n",
    "                        scores=[pixel_similarity_score(output, pred) for output, pred in zip(task.outputs, task_predicted_outputs)])\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing code for task {task_id}, response {i}: {format_exception(e)}\")\n",
    "                    prediction = Prediction(code=code, outputs=None, exception=format_exception(e),\n",
    "                                            scores=None)\n",
    "                predictions[task_id].append(prediction)\n",
    "\n",
    "inference_time = time.time() - t0\n",
    "print(f'Inference time: {inference_time:.1f} seconds')\n",
    "predicted_code = {key: [pred.code for pred in values] for key, values in predictions.items()}\n",
    "predicted_outputs = {key: [pred.outputs for pred in values if pred.outputs is not None] for key, values in predictions.items()}\n",
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds*2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd44e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id, preds in predictions.items():\n",
    "    for pred in preds:\n",
    "        if pred.outputs is not None:\n",
    "            try:\n",
    "                validate_outputs(pred.outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Validation error for task {task_id}: {format_exception(e)}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    if len(tokenizer.tokenize(prompt)) > 32000:\n",
    "        pretty_print_prompt(prompt, default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions['007bbfb7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78756bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_prompt(prompts[0], default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93dd606",
   "metadata": {},
   "source": [
    "- I would like to see the distribution of output tokens.\n",
    "- Problems with `arc25.code_execution.TimeoutException: Code execution exceeded time limit!`\n",
    "- `Token indices sequence length is longer than the specified maximum sequence length for this model (34076 > 32768). Running this sequence through the model will result in indexing errors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3166f6e",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1370bb",
   "metadata": {},
   "source": [
    "- [x] Create a prompt with the available DSL functions and the training ARC task\n",
    "- [x] Fix VLLM initialization issues with proper memory management\n",
    "- [x] Verify the effect of caching\n",
    "- [x] Generate some code that can be used to test the new BARC dsl\n",
    "- [x] Update the library to be able to select which DSL to use when executing code\n",
    "- [x] Verify that I can execute the code generated with the BARC dsl\n",
    "- [x] Add security checks to code, I have seen some input required in the code\n",
    "- [ ] Try to solve some easy task with independent sampling\n",
    "  - [x] How frequently is the dsl used?\n",
    "  - [x] Influence of the model\n",
    "  - [x] Implement output validation, and simplify the metric. Those are different responsabilities\n",
    "  - [x] Correct grids\n",
    "  - [x] Unique outputs\n",
    "  - [x] Everything into a dataframe\n",
    "  - [ ] Metrics distribution\n",
    "  - [ ] Visualization of the predictions\n",
    "  - [ ] Global metrics vs task specific analysis\n",
    "- [ ] Create a refine prompt\n",
    "- [ ] Make a more complex tree search\n",
    "- [x] Validate the tokenizer on the input grids\n",
    "\n",
    "```python\n",
    "for i in range(10):\n",
    "    print(i, {word for word in tokenizer.vocab if str(i) in word})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd122fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
