{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5ef70f",
   "metadata": {},
   "source": [
    "# Search with BARC induction models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259d",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dabfd",
   "metadata": {},
   "source": [
    "Can we solve ARC tasks using base models with access to a DSL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9df7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import BeamSearchParams\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import pretty_print_prompt, Template\n",
    "from arc25.metrics import pixel_similarity_score, correct_grids_score\n",
    "from arc25.utils import get_timestamp\n",
    "from arc25.plot import plot_task\n",
    "import arc25.BARC_dsl as dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2076585",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd75ce",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arc_data_with_solutions(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    solutions_filepath = filepath.replace('challenges.json', 'solutions.json')\n",
    "    if filepath != solutions_filepath and os.path.exists(solutions_filepath):\n",
    "        with open(solutions_filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        for sample_id, task in data.items():\n",
    "            for idx, sample in enumerate(task['test']):\n",
    "                sample['output'] = solutions[sample_id][idx]\n",
    "    verify_that_all_samples_have_output(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def verify_that_all_samples_have_output(data):\n",
    "    for task in data.values():\n",
    "        if isinstance(task, dict):\n",
    "            verify_that_task_has_outputs(task)\n",
    "        elif isinstance(task, list):\n",
    "            for subtask in task:\n",
    "                verify_that_task_has_outputs(subtask)\n",
    "\n",
    "\n",
    "def verify_that_task_has_outputs(task):\n",
    "    for partition, samples in task.items():\n",
    "        if partition not in ['train', 'test']:\n",
    "            continue\n",
    "        for sample in samples:\n",
    "            if 'output' not in sample:\n",
    "                raise ValueError('Not all samples have output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_challenges = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "evaluation_challenges = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "evaluation_challenges_2025 = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2025/arc-agi_evaluation_challenges.json')\n",
    "all_challenges = {**training_challenges, **evaluation_challenges, **evaluation_challenges_2025}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfba8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(task_id):\n",
    "    if task_id in all_challenges:\n",
    "        task = all_challenges[task_id]\n",
    "        task = {partition: [{key: np.array(value) for key, value in sample.items()} for sample in samples] for partition, samples in task.items()}\n",
    "        return task\n",
    "    else:\n",
    "        raise ValueError(f'Task ID {task_id} not found in challenges')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ec071",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f12f27",
   "metadata": {},
   "source": [
    "https://github.com/flowersteam/SOAR/blob/main/soar/prompt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f42cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/barc0/Llama-3.1-ARC-Potpourri-Induction-8B\n",
    "system_prompt = \"\"\"You are a world-class puzzle solver with exceptional pattern recognition skills and expertise in Python programming. Your task is to analyze puzzles and provide Python solutions.\"\"\"\n",
    "\n",
    "prompt_template_text = \"\"\"Given input-output grid pairs as reference examples, carefully observe the patterns to predict the output grid for new test input. Each pair follows the same transformation rule. Grids are 2D arrays represented as strings, with cells (colors) separated by spaces and rows by newlines.\n",
    "Here are the input and output grids for the reference examples:\n",
    "{% for sample in train_samples %}Example {{ loop.index }}\n",
    "Input:\n",
    "{{ sample.input }}\n",
    "\n",
    "Output:\n",
    "{{ sample.output }}\n",
    "\n",
    "{% endfor %}\n",
    "Here is the input grid for the test example:\n",
    "{{ test }}\n",
    "\n",
    "Write a Python function `transform` that can convert any given input grid to its corresponding output grid based on the pattern observed in the reference examples.\n",
    "\"\"\"\n",
    "\n",
    "# I have verified that all responses start with this prefix\n",
    "common_prefix = \"Let's solve this puzzle using Python code with the common library functions. We'll first reason about the problem and then write the code to solve it. The `transform` function will take the input grid and return the output grid. Here is the Python code with the comments describing how to solve the problem:\\n\" #```python\\nfrom common import *\\n\"\n",
    "\n",
    "prompt_template = Template(prompt_template_text)\n",
    "\n",
    "def create_prompt_from_task(task, grid_encoder, tokenizer, shuffle_train_samples=True):\n",
    "    train_samples = [{'input': grid_encoder.to_text(sample['input']), 'output': grid_encoder.to_text(sample['output'])} for sample in task['train']]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    test_sample = random.choice(task['test'])\n",
    "    render_kwargs = dict(train_samples=train_samples, test=grid_encoder.to_text(test_sample['input']))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)},\n",
    "                {\"role\": \"assistant\", \"content\": common_prefix}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=False,\n",
    "                                            continue_final_message=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae964737",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1, max_model_len=32000):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=0.92,  # Use less GPU memory\n",
    "        # max_model_len=4096,  # Limit context length\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea848918",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae210ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_python_code(text):\n",
    "    # Extract Python code from the text\n",
    "    if '```python' not in text:\n",
    "        return ''\n",
    "    code = text.split('```python')[1]\n",
    "    if not '```' in code:\n",
    "        return ''\n",
    "\n",
    "    code = code.split('```')[0].strip()\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_python_code(code):\n",
    "    remove_line_keywords = ['import dsl', 'from dsl import ', 'print(', 'from common import *']\n",
    "    code = '\\n'.join(line for line in code.split('\\n') if not any(keyword in line for keyword in remove_line_keywords))\n",
    "    # code = 'from arc25.BARC_dsl import *\\n' + code  # Ensure BARC_dsl is imported\n",
    "    return code.strip()\n",
    "\n",
    "def add_additional_imports(code):\n",
    "    additional_imports = [\n",
    "        'from typing import List, Tuple',\n",
    "        'import numpy as np',\n",
    "        'import numpy'\n",
    "    ]\n",
    "    imports = '\\n'.join(additional_imports)\n",
    "    return imports + '\\n' + code if code else imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_from_predictions(predictions: dict[str, list[str]], log_errors: bool = True):\n",
    "    # Precompute inputs per task once\n",
    "    # task_inputs = {tid: [np.array(g) for g in get_task(tid).inputs] for tid in predictions}\n",
    "\n",
    "    task_inputs = dict()\n",
    "    for task_id in predictions:\n",
    "        task = get_task(task_id)\n",
    "        task_inputs[task_id] = [sample['input'] for sample in task['train']] + [sample['input'] for sample in task['test']]\n",
    "\n",
    "    # Flatten all predictions into a work list\n",
    "    work = [\n",
    "        (tid, i, text_pred, task_inputs[tid], prediction_data['data_augmentation_params'][i])\n",
    "        for tid, prediction_data in predictions.items()\n",
    "        for i, text_pred in enumerate(prediction_data['text_predictions'])\n",
    "    ]\n",
    "    # print(work[0])\n",
    "\n",
    "    predicted_code = {tid: [] for tid in predictions}\n",
    "    predicted_outputs = {tid: [] for tid in predictions}\n",
    "\n",
    "    n_jobs = -1  # all cores; set to an int to cap\n",
    "\n",
    "    # with tqdm_joblib(tqdm(total=len(work), desc=\"Executing predictions\", unit=\"pred\")):\n",
    "    with tqdm_joblib(total=len(work), desc=\"Executing predictions\", unit=\"pred\", smoothing=0):\n",
    "        results = Parallel(\n",
    "            n_jobs=n_jobs,\n",
    "            backend=\"loky\",\n",
    "            prefer=\"processes\",\n",
    "            batch_size=\"auto\",\n",
    "        )(delayed(_run_one)(*args) for args in work)\n",
    "\n",
    "    # Rebuild per-task outputs, preserving your original behavior (code appended even on exec error)\n",
    "    for task_id, i, code, outs, err in results:\n",
    "        predicted_code[task_id].append(code)\n",
    "        predicted_outputs[task_id].append(outs)\n",
    "        if err and log_errors:\n",
    "            logging.error(f\"Error executing code for task {task_id}, response {i}: {err}\")\n",
    "\n",
    "    return predicted_code, predicted_outputs\n",
    "\n",
    "\n",
    "def _run_one(task_id, i, text_prediction, input_grids, data_augmentation_params=None):\n",
    "    code = parse_python_code(text_prediction)\n",
    "    if not code:\n",
    "        return (task_id, i, None, None, \"parse_failed\")\n",
    "    try:\n",
    "        if data_augmentation_params is not None:\n",
    "            # Apply data augmentation to the input grids\n",
    "            input_grids = [apply_data_augmentation_to_grid(grid, **data_augmentation_params) for grid in input_grids]\n",
    "        outs = safe_code_execution(\n",
    "            add_additional_imports(curate_python_code(code)),\n",
    "            input_grids,\n",
    "            func_name=\"transform\",\n",
    "        )\n",
    "        outs = validate_outputs(outs)\n",
    "        # print(outs)\n",
    "        if data_augmentation_params is not None:\n",
    "            outs = [np.array(revert_data_augmentation(output, **data_augmentation_params)) for output in outs]\n",
    "        # print(outs)\n",
    "        return (task_id, i, code, outs, None)\n",
    "    except Exception as e:\n",
    "        return (task_id, i, code, None, f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "# tiny_predictions = {'00576224': predictions['00576224']}\n",
    "# predicted_code, predicted_outputs = run_code_from_predictions(tiny_predictions, log_errors=True)\n",
    "# df = compute_search_metrics(list(tiny_predictions.keys()), predicted_code, predicted_outputs, n_preds)\n",
    "# df.round(3)\n",
    "# predicted_code, predicted_outputs = run_code_from_predictions(predictions, log_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee204f",
   "metadata": {},
   "source": [
    "### Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_outputs(outputs):\n",
    "    if not outputs:\n",
    "        raise ValueError(\"Outputs list is empty\")\n",
    "    return [_validate_output(output) for output in outputs]\n",
    "\n",
    "def _validate_output(output):\n",
    "    if output is None:\n",
    "        raise ValueError(\"Output is None\")\n",
    "    output = np.array(output) # otherwise I see weird outputs that mix list and numpy arrays\n",
    "    if output.ndim != 2:\n",
    "        raise ValueError(f\"Output is not a 2D array. Output shape: {output.shape}\")\n",
    "    if max(output.shape) > 35:\n",
    "        raise ValueError(f\"Output is too large, the maximum allowed shape is 30x30. Output shape: {output.shape}\")\n",
    "    if min(output.shape) == 0:\n",
    "        raise ValueError(f\"Output has zero dimension, it is empty. Output shape: {output.shape}\")\n",
    "    if np.max(output) > 9 or np.min(output) < 0:\n",
    "        raise ValueError(f\"Output contains invalid values, expected values in range [0, 9]. Output max: {np.max(output)}, min: {np.min(output)}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee616990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def fingerprint(prediction):\n",
    "    \"\"\"\n",
    "    Create a compact hash for a list of matrices.\n",
    "    Includes shape & dtype to distinguish e.g. (2×2) from (4×1).\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    for m in prediction:\n",
    "        # incorporate shape and dtype in a reproducible way\n",
    "        h.update(str(m.shape).encode())\n",
    "        h.update(m.dtype.str.encode())\n",
    "        # raw data bytes\n",
    "        h.update(m.tobytes())\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b4bd9",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds):\n",
    "    df = pd.DataFrame(columns=['n_preds', 'valid code', 'valid outputs', 'unique outputs', 'pixel similarity', 'correct grids', \n",
    "                               'train_pass_rate', 'train_pass@n', 'pass_rate', 'pass@n'])\n",
    "    for task_id in task_ids:\n",
    "        df.loc[task_id, 'n_preds'] = n_preds\n",
    "        valid_code = [code for code in predicted_code[task_id] if code is not None]\n",
    "        df.loc[task_id, 'valid code'] = len(valid_code)/n_preds\n",
    "        valid_outputs = [output for output in predicted_outputs[task_id] if output is not None]\n",
    "        df.loc[task_id, 'valid outputs'] = len(valid_outputs)/n_preds\n",
    "        df.loc[task_id, 'unique outputs'] = len(set(fingerprint(output) for output in valid_outputs))/n_preds\n",
    "\n",
    "        task = get_task(task_id)\n",
    "        task_outputs = [sample['output'] for sample in task['train']] + [sample['output'] for sample in task['test']]\n",
    "        scores = sorted([np.mean([pixel_similarity_score(output, pred) for output, pred in zip(task_outputs, predictions)]) for predictions in valid_outputs])\n",
    "        df.loc[task_id, 'pixel similarity'] = np.mean(scores) if scores else 0.0\n",
    "        \n",
    "        scores = sorted([correct_grids_score(task_outputs, predictions) for predictions in valid_outputs])\n",
    "        df.loc[task_id, 'correct grids'] = np.mean(scores) if scores else 0.0\n",
    "        df.loc[task_id, 'pass_rate'] = np.mean(np.array(scores) == 1) if scores else 0\n",
    "        df.loc[task_id, 'pass@n'] = int(np.max(scores) == 1) if scores else 0\n",
    "\n",
    "        train_outputs = [sample['output'] for sample in task['train']]\n",
    "        train_scores = sorted([correct_grids_score(train_outputs, predictions[:len(train_outputs)]) for predictions in valid_outputs])\n",
    "        df.loc[task_id, 'train_pass_rate'] = np.mean(np.array(train_scores) == 1) if train_scores else 0\n",
    "        df.loc[task_id, 'train_pass@n'] = int(np.max(train_scores) == 1) if train_scores else 0\n",
    "\n",
    "    df.loc['MEAN'] = df.mean(axis=0)\n",
    "    return df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99333e2",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429556e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_data_augmentation(task, hflip, n_rot90, color_map=None):\n",
    "    augmented_task = {partition: [{key: apply_data_augmentation_to_grid(grid, hflip, n_rot90, color_map) for key, grid in sample.items()} \\\n",
    "                 for sample in samples] for partition, samples in task.items()}\n",
    "    return augmented_task\n",
    "\n",
    "\n",
    "def apply_data_augmentation_to_grid(grid, hflip, n_rot90, color_map=None):\n",
    "    grid = geometric_augmentation(grid, hflip, n_rot90)\n",
    "    if color_map is not None:\n",
    "        grid = apply_colormap(grid, color_map)\n",
    "    return np.array(grid)\n",
    "\n",
    "\n",
    "def revert_data_augmentation(grid, hflip, n_rot90, color_map=None):\n",
    "    grid = revert_geometric_augmentation(grid, hflip, n_rot90)\n",
    "    if color_map is not None:\n",
    "        grid = revert_color_swap(grid, color_map)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def geometric_augmentation(grid, hflip, n_rot90):\n",
    "    grid = np.array(grid)\n",
    "    if hflip:\n",
    "        grid = np.flip(grid, axis=1)\n",
    "    grid = np.rot90(grid, k=n_rot90)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def revert_geometric_augmentation(grid, hflip, n_rot90):\n",
    "    grid = np.array(grid)\n",
    "    grid = np.rot90(grid, k=-n_rot90)\n",
    "    if hflip:\n",
    "        grid = np.flip(grid, axis=1)\n",
    "    return grid\n",
    "\n",
    "\n",
    "def revert_color_swap(grid, color_map):\n",
    "    reverse_color_map = {v: int(k) for k, v in color_map.items()}\n",
    "    vectorized_mapping = np.vectorize(reverse_color_map.get)\n",
    "    return vectorized_mapping(grid)\n",
    "\n",
    "\n",
    "def swap_task_colors(task, color_map=None, change_background_probability=0.1):\n",
    "    if color_map is None:\n",
    "        color_map = get_random_color_map(change_background_probability)\n",
    "    vectorized_mapping = np.vectorize(color_map.get)\n",
    "    new_task = Task(\n",
    "        inputs = [vectorized_mapping(grid) for grid in task.inputs],\n",
    "        outputs = [vectorized_mapping(grid) for grid in task.outputs],\n",
    "        code = '',\n",
    "        name = task.name,)\n",
    "    return new_task\n",
    "\n",
    "\n",
    "def apply_colormap(grid, color_map):\n",
    "    vectorized_mapping = np.vectorize(color_map.get)\n",
    "    return vectorized_mapping(grid)\n",
    "\n",
    "\n",
    "def get_random_data_augmentation_params():\n",
    "    params = get_random_geometric_augmentation_params()\n",
    "    params['color_map'] = get_random_color_map()\n",
    "    return params\n",
    "\n",
    "\n",
    "def get_random_geometric_augmentation_params():\n",
    "    return dict(hflip=random.choice([True, False]), n_rot90=random.choice([0, 1, 2, 3]))\n",
    "\n",
    "\n",
    "def get_random_color_map(change_background_probability=0.1):\n",
    "    colors = list(range(10))\n",
    "    if random.random() < change_background_probability:\n",
    "        new_colors = list(range(10))\n",
    "        random.shuffle(new_colors)\n",
    "    else:\n",
    "        new_colors = list(range(1, 10))\n",
    "        random.shuffle(new_colors)\n",
    "        new_colors = [0] + new_colors\n",
    "\n",
    "    color_map = {x: y for x, y in zip(colors, new_colors)}\n",
    "    return color_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fed80",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca60f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_predictions(path_pattern):\n",
    "    filepaths = glob.glob(path_pattern)\n",
    "    predictions = dict()\n",
    "    for filepath in tqdm(filepaths, desc=\"Loading predictions\"):\n",
    "        with open(filepath, 'r') as f:\n",
    "            preds = json.load(f)\n",
    "        for task_id, outputs in preds.items():\n",
    "            if task_id not in predictions:\n",
    "                predictions[task_id] = dict(text_predictions=[], data_augmentation_params=[])\n",
    "            if isinstance(outputs, dict):\n",
    "                predictions[task_id]['text_predictions'].extend(outputs['text_predictions'])\n",
    "                data_augmentation_params = outputs.get('data_augmentation_params', None)\n",
    "                if data_augmentation_params['color_map'] is not None:\n",
    "                    data_augmentation_params['color_map'] = {int(k): int(v) for k, v in data_augmentation_params['color_map'].items()}\n",
    "                predictions[task_id]['data_augmentation_params'].extend([data_augmentation_params]*len(outputs['text_predictions']))\n",
    "            else:\n",
    "                predictions[task_id]['text_predictions'].extend(outputs)\n",
    "                predictions[task_id]['data_augmentation_params'].extend([None] * len(outputs))  # Assuming no params for old format\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ba732",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a678a",
   "metadata": {},
   "source": [
    "Does using data augmentation increases the diversity of the predictions and improves the pass@n metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c0da6",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_task_ids = {\n",
    "    'training': list(training_challenges.keys()),\n",
    "    'evaluation': list(evaluation_challenges.keys()),\n",
    "    'evaluation-2025': list(evaluation_challenges_2025.keys())\n",
    "}\n",
    "\n",
    "experiment_name = '2025-08-22_add-common-prefix'\n",
    "dataset = 'training'\n",
    "task_ids = dataset_to_task_ids[dataset]\n",
    "grid_encoder = create_grid_encoder('ColorNameEncoder()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd9fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = create_prompt_from_task(get_task(task_ids[0]), grid_encoder, tokenizer)\n",
    "pretty_print_prompt(sample_prompt, default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [8]:\n",
    "    sampling_params = SamplingParams(n=n, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "\n",
    "    prompts, data_augmentation_params = [], []\n",
    "    for task_id in task_ids:\n",
    "        params = get_random_data_augmentation_params()\n",
    "        data_augmentation_params.append(params)\n",
    "        task = get_task(task_id)\n",
    "        task = apply_data_augmentation(task, **params)\n",
    "        prompt = create_prompt_from_task(\n",
    "            task, grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    t0 = time.time()\n",
    "    text_predictions = llm.generate(prompts, sampling_params)\n",
    "    total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "    inference_time = time.time() - t0\n",
    "    print(f\"Total tokens generated: {total_tokens}\")\n",
    "    print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "    print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "    print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "    print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "\n",
    "    predictions = dict()\n",
    "    for task_id, output, params in zip(task_ids, text_predictions, data_augmentation_params):\n",
    "        predictions[task_id] = {\n",
    "            'text_predictions': [output.text for output in output.outputs],\n",
    "            'data_augmentation_params': params,\n",
    "        }\n",
    "\n",
    "    output_filepath = f'/mnt/hdd0/Kaggle/arc25/predictions/{experiment_name}/{dataset}_{sampling_params.n}preds_{get_timestamp()}_predictions.json'\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(f\"Predictions saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a2167",
   "metadata": {},
   "source": [
    "```\n",
    "training\n",
    "8 preds, Average time per task: 3.25 seconds\n",
    "Average time per task: 2.88 seconds, when adding the common prefix\n",
    "\n",
    "evaluation\n",
    "8 preds, Average time per task: 4.17 seconds\n",
    "(previously it was 3.64 when using n-1 training samples)\n",
    "\n",
    "evaluation-2025\n",
    "8 preds, Average time per task: 5.57 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91602ec",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdccaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_fix-bug/evaluation_*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_fix-bug/training_*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-18_barc-first-steps/evaluation-2025_*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-21_data-augmentation/evaluation_8preds_2025_08_21_13_58_47_predictions.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code, predicted_outputs = run_code_from_predictions(predictions, log_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_search_metrics(list(predictions.keys()), predicted_code, predicted_outputs, n_preds)\n",
    "df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46931f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "n_preds_range = 2**np.arange(0, int(np.log2(n_preds)) + 2)\n",
    "fail_prob = 1 - df['pass_rate'].values[:-1]\n",
    "for n in n_preds_range:\n",
    "    scores.append(float(np.mean(1 - fail_prob**n)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(n_preds_range, scores, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('pass@n vs Number of Predictions')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "dict(evaluation_data_augmentation=(n_preds_range.tolist(), scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda96f6",
   "metadata": {},
   "source": [
    "### Compare with and without data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630dca40",
   "metadata": {},
   "source": [
    "#### evaluation ARC-AGI-1 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "'baseline': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.01996212663472896,\n",
    "   0.02831747138692302,\n",
    "   0.03977086910606171,\n",
    "   0.05542900033336087,\n",
    "   0.07674247281753263,\n",
    "   0.10320479610157163,\n",
    "   0.13093643779484748,\n",
    "   0.15594708997780551,\n",
    "   0.1770796829272273,\n",
    "   0.19471691227422966,\n",
    "   0.20712048546783934]),\n",
    "'+ data augmentation': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.019813790637075775,\n",
    "   0.029064191088443207,\n",
    "   0.04100375897586597,\n",
    "   0.05671308781660802,\n",
    "   0.07779377587518771,\n",
    "   0.10484135687838109,\n",
    "   0.13591735274813102,\n",
    "   0.1675520359797603,\n",
    "   0.19696420491620842,\n",
    "   0.22203487375136557,\n",
    "   0.23821892651950458])\n",
    "}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    plt.plot(n_preds_range, scores, marker='o', label=key)\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-1')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf42afb",
   "metadata": {},
   "source": [
    "#### Bias of the number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4baf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "'64 preds': ([1, 2, 4, 8, 16, 32, 64, 128],\n",
    "  [0.019106336951575514,\n",
    "   0.028082339121390357,\n",
    "   0.039026953359591325,\n",
    "   0.05221714653327929,\n",
    "   0.06785124181952008,\n",
    "   0.08534713350917737,\n",
    "   0.10178389655716932,\n",
    "   0.11195929946811921]),\n",
    "'112 preds': ([1, 2, 4, 8, 16, 32, 64, 128],\n",
    "  [0.019387411477336133,\n",
    "   0.02887665434462053,\n",
    "   0.04103250559571824,\n",
    "   0.05662528637189867,\n",
    "   0.07657867399726664,\n",
    "   0.1006969445110981,\n",
    "   0.12613859060350577,\n",
    "   0.14808143720453426]),\n",
    "'184 preds': ([1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "  [0.019316797975629206,\n",
    "   0.02854922966041234,\n",
    "   0.0405175566715264,\n",
    "   0.056039023485551356,\n",
    "   0.07638928013313853,\n",
    "   0.10206766052179019,\n",
    "   0.13087569581761588,\n",
    "   0.15844583931075643,\n",
    "   0.1786899038965614]),\n",
    "'584 preds': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.019813790637075775,\n",
    "   0.029064191088443207,\n",
    "   0.04100375897586597,\n",
    "   0.05671308781660802,\n",
    "   0.07779377587518771,\n",
    "   0.10484135687838109,\n",
    "   0.13591735274813102,\n",
    "   0.1675520359797603,\n",
    "   0.19696420491620842,\n",
    "   0.22203487375136557,\n",
    "   0.23821892651950458])\n",
    "}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    plt.plot(n_preds_range, scores, marker='o', label=key)\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-1')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62a70b",
   "metadata": {},
   "source": [
    "#### evaluation arc-agi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'baseline': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024,\n",
    "   2048],\n",
    "  [0.000525120200263568,\n",
    "   0.0010290649730527559,\n",
    "   0.001977218777084015,\n",
    "   0.0036586995036725616,\n",
    "   0.006324151121543866,\n",
    "   0.009785455928199816,\n",
    "   0.013112192645441874,\n",
    "   0.015422166919107317,\n",
    "   0.016485239578931988,\n",
    "   0.016662717947703534,\n",
    "   0.016666664795580977,\n",
    "   0.016666666666666247]),\n",
    "'data_augmentation': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.0004395694358616827,\n",
    "   0.0008607058465457645,\n",
    "   0.001651027938816423,\n",
    "   0.003045200797763149,\n",
    "   0.005231079244621852,\n",
    "   0.00801093382400977,\n",
    "   0.010647211929364896,\n",
    "   0.012848270575498736,\n",
    "   0.014933881468222887,\n",
    "   0.016306378321672744])}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    plt.plot(n_preds_range, scores, marker='o', label=key)\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-2')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9875f",
   "metadata": {},
   "source": [
    "### Distribution of prediction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9cf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['training', 'evaluation', 'evaluation-2025']:\n",
    "    predictions = load_all_predictions(f'/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_fix-bug/{key}_*.json')\n",
    "    prediction_length_distribution = {tid: [len(tokens) for tokens in tokenizer(preds['text_predictions'])['input_ids']] \\\n",
    "                                      for tid, preds in tqdm(predictions.items(), desc=\"Computing prediction lengths\", total=len(predictions))}\n",
    "    all_lengths = [length for lengths in prediction_length_distribution.values() for length in lengths]\n",
    "    label = f\"{key} (max output tokens: {max(all_lengths)}, median output tokens: {int(np.median(all_lengths))})\"\n",
    "    bins = np.linspace(0, 2000, 100)\n",
    "    plt.hist(all_lengths, bins=bins, label=label, alpha=0.5, density=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.title('Distribution of prediction lengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e47dc1",
   "metadata": {},
   "source": [
    "### Inspect correct solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id in df[df['pass_rate'] > 0].index.values[:-1]:\n",
    "    print(f'https://arcprize.org/play?task={task_id} pass rate: {df.loc[task_id, \"pass_rate\"]:.2%}')\n",
    "    task = get_task(task_id)\n",
    "    correct_solution_found = False\n",
    "    for idx, output in enumerate(predicted_outputs[task_id]):\n",
    "        if output is None:\n",
    "            continue\n",
    "        if correct_grids_score([np.array(output) for output in task.outputs], output) == 1:\n",
    "            correct_solution_found = True\n",
    "            data_augmentation_params = predictions[task_id]['data_augmentation_params'][idx]\n",
    "            text_pred = predictions[task_id]['text_predictions'][idx]\n",
    "            print(data_augmentation_params)\n",
    "            augmented_task = apply_data_augmentation(task, **data_augmentation_params) if data_augmentation_params is not None else task\n",
    "            plot_task(augmented_task); plt.show()\n",
    "            display(Markdown(text_pred + '\\n\\n---\\n\\n'))\n",
    "            break\n",
    "    if not correct_solution_found:\n",
    "        raise ValueError(\"Could not find correct solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998b948",
   "metadata": {},
   "source": [
    "I'm impressed by the tasks that the model is able to solve. The reasoning is correct. This is a powerful model to experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9afe7",
   "metadata": {},
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74529d0",
   "metadata": {},
   "source": [
    "- [x] Load all ARC data\n",
    "- [x] Modify prompt generation to use all the training data, and a random sample from the test samples.\n",
    "- [x] Update the data augmentation pipeline to use dicts instead of tasks\n",
    "- [x] Modify the code execution to use all the data\n",
    "- [x] Add a new metric to check if the code is correct for the train samples but incorrect for the test samples\n",
    "- [x] Refactor\n",
    "- [ ] Convert to python script so I can make predictions remotely\n",
    "- [ ] Evaluate the datasets\n",
    "- [x] Check if the answers always start with the same prefix, if that is the case I could speedup inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747472b",
   "metadata": {},
   "source": [
    "- [x] Implement new grid encoder\n",
    "- [x] Use the correct prompt\n",
    "- [x] Save predictions to file so I can later reprocess them\n",
    "- [x] Update code execution to match the code generated by BARC model\n",
    "- [x] Check code execution to verify that exceptions are legit and not easily solvable\n",
    "  - [x] Add missing colors to Color object\n",
    "  - [x] Code execution fails when there are auxiliary functions. `Error executing code for task 025d127b, response 5: <class 'NameError'> name 'blend_colors' is not defined`\n",
    "  - [x] Arrays as inputs\n",
    "- [x] Remove dsl usage metric\n",
    "- [x] Add correct task metric\n",
    "- [x] Parallelize code execution\n",
    "- [x] Refactor code\n",
    "- [x] Plots showing the effect of increasing the number of predictions\n",
    "- [x] Validate that I get the same scores of the paper\n",
    "- [x] Evaluate on different datasets\n",
    "- [x] Improve metrics\n",
    "- [x] Data augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
