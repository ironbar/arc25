{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5ef70f",
   "metadata": {},
   "source": [
    "# Search with base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259d",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dabfd",
   "metadata": {},
   "source": [
    "Can we solve ARC tasks using base models with access to a DSL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9df7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import BeamSearchParams\n",
    "\n",
    "from arc25.training_tasks import *\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import pretty_print_prompt, Template\n",
    "from arc25.metrics import pixel_similarity_score, correct_grids_score\n",
    "from arc25.utils import get_timestamp\n",
    "import arc25.BARC_dsl as dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2076585",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ec071",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f12f27",
   "metadata": {},
   "source": [
    "https://github.com/flowersteam/SOAR/blob/main/soar/prompt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f62ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_footprint(module_name: str, show_types: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Load a module by name, then return a newline-separated list of all\n",
    "    top-level functions in it, in the form:\n",
    "\n",
    "      def func_name(arg1, arg2) -> return\n",
    "\n",
    "    If show_types=True, annotations are included; otherwise only names.\n",
    "    \"\"\"\n",
    "    mod = importlib.import_module(module_name)\n",
    "    footprints = []\n",
    "\n",
    "    for name, fn in inspect.getmembers(mod, inspect.isfunction):\n",
    "        # skip imports from elsewhere\n",
    "        if fn.__module__ != module_name or name.startswith(\"_\"):\n",
    "            continue\n",
    "\n",
    "        sig = inspect.signature(fn)\n",
    "        if not show_types:\n",
    "            # strip type info\n",
    "            params = [p.name for p in sig.parameters.values()]\n",
    "            sig_text = f\"({', '.join(params)})\"\n",
    "        else:\n",
    "            sig_text = str(sig)\n",
    "\n",
    "        footprints.append(f\"- dsl.{name}{sig_text}\")\n",
    "\n",
    "    return \"\\n\".join(footprints)\n",
    "\n",
    "print(extract_footprint('arc25.BARC_dsl', show_types=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json', 'r') as f:\n",
    "    training_challenges = json.load(f)\n",
    "\n",
    "with open('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json', 'r') as f:\n",
    "    evaluation_challenges = json.load(f)\n",
    "\n",
    "with open('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2025/arc-agi_evaluation_challenges.json', 'r') as f:\n",
    "    evaluation_challenges_2025 = json.load(f)\n",
    "\n",
    "def get_task(task_name):\n",
    "    if task_name in training_challenges:\n",
    "        task_data = training_challenges[task_name]\n",
    "    elif task_name in evaluation_challenges:\n",
    "        task_data = evaluation_challenges[task_name]\n",
    "    elif task_name in evaluation_challenges_2025:\n",
    "        task_data = evaluation_challenges_2025[task_name]\n",
    "    else:\n",
    "        raise ValueError(f\"Task {task_name} not found in training or evaluation challenges.\")\n",
    "    inputs = [Img(sample['input']) for sample in task_data['train']]\n",
    "    outputs = [Img(sample['output']) for sample in task_data['train']]\n",
    "    return Task(inputs=inputs, outputs=outputs, code='', name=task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f42cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/barc0/Llama-3.1-ARC-Potpourri-Induction-8B\n",
    "system_prompt = \"\"\"You are a world-class puzzle solver with exceptional pattern recognition skills and expertise in Python programming. Your task is to analyze puzzles and provide Python solutions.\"\"\"\n",
    "\n",
    "prompt_template_text = \"\"\"Given input-output grid pairs as reference examples, carefully observe the patterns to predict the output grid for new test input. Each pair follows the same transformation rule. Grids are 2D arrays represented as strings, with cells (colors) separated by spaces and rows by newlines.\n",
    "Here are the input and output grids for the reference examples:\n",
    "{% for sample in train_samples %}Example {{ loop.index }}\n",
    "Input:\n",
    "{{ sample.input }}\n",
    "\n",
    "Output:\n",
    "{{ sample.output }}\n",
    "\n",
    "{% endfor %}\n",
    "Here is the input grid for the test example:\n",
    "{{ test }}\n",
    "\n",
    "Write a Python function `transform` that can convert any given input grid to its corresponding output grid based on the pattern observed in the reference examples.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template = Template(prompt_template_text)\n",
    "\n",
    "\n",
    "def create_prompt_from_task(task, grid_encoder, tokenizer, shuffle_train_samples=False, remove_last_train_sample=False):\n",
    "    train_samples = [{'input': grid_encoder.to_text(grid), 'output': grid_encoder.to_text(output)} for grid, output in zip(task.inputs[:-1], task.outputs[:-1])]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    if remove_last_train_sample and len(train_samples) > 1:\n",
    "        train_samples = train_samples[:-1]\n",
    "    render_kwargs = dict(train_samples=train_samples, test=grid_encoder.to_text(task.inputs[-1]))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae964737",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1, max_model_len=32000):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=0.9,  # Use less GPU memory\n",
    "        # max_model_len=4096,  # Limit context length\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea848918",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae210ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_python_code(text):\n",
    "    # Extract Python code from the text\n",
    "    if '```python' not in text:\n",
    "        return ''\n",
    "    code = text.split('```python')[1]\n",
    "    if not '```' in code:\n",
    "        return ''\n",
    "\n",
    "    code = code.split('```')[0].strip()\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_python_code(code):\n",
    "    remove_line_keywords = ['import dsl', 'from dsl import ', 'print(', 'from common import *']\n",
    "    code = '\\n'.join(line for line in code.split('\\n') if not any(keyword in line for keyword in remove_line_keywords))\n",
    "    # code = 'from arc25.BARC_dsl import *\\n' + code  # Ensure BARC_dsl is imported\n",
    "    return code.strip()\n",
    "\n",
    "def add_additional_imports(code):\n",
    "    additional_imports = [\n",
    "        'from typing import List, Tuple',\n",
    "        'import numpy as np',\n",
    "        'import numpy'\n",
    "    ]\n",
    "    imports = '\\n'.join(additional_imports)\n",
    "    return imports + '\\n' + code if code else imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee204f",
   "metadata": {},
   "source": [
    "### Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_outputs(outputs):\n",
    "    if not outputs:\n",
    "        raise ValueError(\"Outputs list is empty\")\n",
    "    return [_validate_output(output) for output in outputs]\n",
    "\n",
    "def _validate_output(output):\n",
    "    if output is None:\n",
    "        raise ValueError(\"Output is None\")\n",
    "    output = np.array(output) # otherwise I see weird outputs that mix list and numpy arrays\n",
    "    if output.ndim != 2:\n",
    "        raise ValueError(f\"Output is not a 2D array. Output shape: {output.shape}\")\n",
    "    if max(output.shape) > 35:\n",
    "        raise ValueError(f\"Output is too large, the maximum allowed shape is 30x30. Output shape: {output.shape}\")\n",
    "    if min(output.shape) == 0:\n",
    "        raise ValueError(f\"Output has zero dimension, it is empty. Output shape: {output.shape}\")\n",
    "    if np.max(output) > 9 or np.min(output) < 0:\n",
    "        raise ValueError(f\"Output contains invalid values, expected values in range [0, 9]. Output max: {np.max(output)}, min: {np.min(output)}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee616990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def fingerprint(prediction):\n",
    "    \"\"\"\n",
    "    Create a compact hash for a list of matrices.\n",
    "    Includes shape & dtype to distinguish e.g. (2×2) from (4×1).\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    for m in prediction:\n",
    "        # incorporate shape and dtype in a reproducible way\n",
    "        h.update(str(m.shape).encode())\n",
    "        h.update(m.dtype.str.encode())\n",
    "        # raw data bytes\n",
    "        h.update(m.tobytes())\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b4bd9",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_search_metrics(task_ids, predicted_code, predicted_outputs, n_preds):\n",
    "    df = pd.DataFrame(columns=['valid code', 'valid outputs', 'unique outputs', 'dsl usage', 'pixel similarity', 'correct grids', 'solved task'])\n",
    "    for task_id in task_ids:\n",
    "        df.loc[task_id, 'valid code'] = len(predicted_code[task_id])/n_preds\n",
    "        df.loc[task_id, 'valid outputs'] = len(predicted_outputs[task_id])/n_preds\n",
    "        df.loc[task_id, 'unique outputs'] = len(set(fingerprint(output) for output in predicted_outputs[task_id]))/n_preds\n",
    "        df.loc[task_id, 'dsl usage'] = sum(1 for code in predicted_code[task_id] if 'dsl.' in code)/n_preds\n",
    "\n",
    "        task = get_task(task_id)\n",
    "        task_predicted_outputs = predicted_outputs[task_id]\n",
    "        scores = sorted([np.mean([pixel_similarity_score(output, pred) for output, pred in zip(task.outputs, predictions)]) for predictions in task_predicted_outputs])\n",
    "        df.loc[task_id, 'pixel similarity'] = np.mean(scores) if scores else 0.0\n",
    "\n",
    "        task_outputs = [np.array(output) for output in task.outputs]\n",
    "        scores = sorted([correct_grids_score(task_outputs, predictions) for predictions in task_predicted_outputs])\n",
    "        df.loc[task_id, 'correct grids'] = np.mean(scores) if scores else 0.0\n",
    "        df.loc[task_id, 'solved task'] = int(np.max(scores) == 1) if scores else 0\n",
    "\n",
    "    df.loc['MEAN'] = df.mean(axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6171a",
   "metadata": {},
   "source": [
    "## Independent search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf681352",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = list(training_challenges.keys())\n",
    "filepath = '/mnt/hdd0/Kaggle/arc25/predictions/training_8preds_2025_08_18_21_26_04_predictions.json'\n",
    "with open(filepath, 'r') as f:\n",
    "    predictions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aefc255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781c8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_ids = list(training_challenges.keys())\n",
    "task_ids = list(evaluation_challenges.keys())\n",
    "task_ids = list(evaluation_challenges_2025.keys())\n",
    "grid_encoder = create_grid_encoder('ColorNameEncoder()')\n",
    "prompts = [create_prompt_from_task(get_task(task_id), grid_encoder=grid_encoder, tokenizer=tokenizer) for task_id in task_ids]\n",
    "pretty_print_prompt(prompts[0], default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [8]:\n",
    "    sampling_params = SamplingParams(n=n, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "    t0 = time.time()\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in outputs)\n",
    "    inference_time = time.time() - t0\n",
    "    print(f\"Total tokens generated: {total_tokens}\")\n",
    "    print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "    print(f\"Average time per task: {inference_time / len(outputs):.2f} seconds\")\n",
    "    print(f\"Average tokens per task: {total_tokens / len(outputs) / sampling_params.n:.2f} tokens\")\n",
    "    print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "\n",
    "    predictions = {task_id: [output.text for output in output.outputs] for task_id, output in zip(task_ids, outputs)}\n",
    "    # output_filepath = f'/mnt/hdd0/Kaggle/arc25/predictions/training_{sampling_params.n}preds_{get_timestamp()}_predictions.json'\n",
    "    output_filepath = f'/mnt/hdd0/Kaggle/arc25/predictions/evaluation_2025_{sampling_params.n}preds_{get_timestamp()}_predictions.json'\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(f\"Predictions saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e9c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_code = {key: [] for key in task_ids}\n",
    "predicted_outputs = {key: [] for key in task_ids}\n",
    "for task_id, outputs in predictions.items():\n",
    "    task = get_task(task_id)\n",
    "    for i, output in enumerate(outputs):\n",
    "        code = parse_python_code(output)\n",
    "        if code:\n",
    "            code = curate_python_code(code)\n",
    "            predicted_code[task_id].append(code)\n",
    "            try:\n",
    "                input_grids = [np.array(input_grid) for input_grid in task.inputs]\n",
    "                task_predicted_outputs = safe_code_execution(add_additional_imports(code), input_grids, func_name='transform', dsl=dsl)\n",
    "                task_predicted_outputs = validate_outputs(task_predicted_outputs)\n",
    "                predicted_outputs[task_id].append(task_predicted_outputs)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing code for task {task_id}, response {i}: {type(e)} {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions['045e512c'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecd27684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid code</th>\n",
       "      <th>valid outputs</th>\n",
       "      <th>unique outputs</th>\n",
       "      <th>dsl usage</th>\n",
       "      <th>pixel similarity</th>\n",
       "      <th>correct grids</th>\n",
       "      <th>solved task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>007bbfb7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00d62c1b</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.987986</td>\n",
       "      <td>0.475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>017c7c7b</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.680776</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>025d127b</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.803855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>045e512c</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fcc82909</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feca6190</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff28f65a</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff805c23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176743</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.824063</td>\n",
       "      <td>0.656563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.615033</td>\n",
       "      <td>0.148643</td>\n",
       "      <td>0.2975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>401 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         valid code valid outputs unique outputs dsl usage pixel similarity  \\\n",
       "007bbfb7        1.0          0.75           0.75       0.0         0.699177   \n",
       "00d62c1b        1.0           1.0          0.375       0.0         0.987986   \n",
       "017c7c7b        1.0         0.875          0.625       0.0         0.680776   \n",
       "025d127b        1.0         0.875          0.625       0.0         0.803855   \n",
       "045e512c        1.0           0.5            0.5       0.0         0.741119   \n",
       "...             ...           ...            ...       ...              ...   \n",
       "fcc82909        1.0           1.0            1.0       0.0         0.749167   \n",
       "feca6190        1.0         0.875           0.75       0.0         0.131206   \n",
       "ff28f65a        1.0          0.75           0.25       0.0         0.898148   \n",
       "ff805c23        1.0         0.875          0.875       0.0         0.176743   \n",
       "MEAN            1.0      0.824063       0.656563       0.0         0.615033   \n",
       "\n",
       "         correct grids solved task  \n",
       "007bbfb7           0.0           0  \n",
       "00d62c1b         0.475           1  \n",
       "017c7c7b      0.428571           1  \n",
       "025d127b           0.0           0  \n",
       "045e512c           0.0           0  \n",
       "...                ...         ...  \n",
       "fcc82909           0.0           0  \n",
       "feca6190           0.0           0  \n",
       "ff28f65a        0.5625           0  \n",
       "ff805c23           0.0           0  \n",
       "MEAN          0.148643      0.2975  \n",
       "\n",
       "[401 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = compute_search_metrics(task_ids, predicted_code, predicted_outputs, len(list(predictions.values())[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dda835f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid code</th>\n",
       "      <th>valid outputs</th>\n",
       "      <th>unique outputs</th>\n",
       "      <th>dsl usage</th>\n",
       "      <th>pixel similarity</th>\n",
       "      <th>correct grids</th>\n",
       "      <th>solved task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MEAN</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.824063</td>\n",
       "      <td>0.656563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.615033</td>\n",
       "      <td>0.148643</td>\n",
       "      <td>0.2975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     valid code valid outputs unique outputs dsl usage pixel similarity  \\\n",
       "MEAN        1.0      0.824063       0.656563       0.0         0.615033   \n",
       "\n",
       "     correct grids solved task  \n",
       "MEAN      0.148643      0.2975  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b264b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881d64c",
   "metadata": {},
   "source": [
    "## WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_code['007bbfb7'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Given input-output grid pairs as reference examples, carefully observe the patterns to predict the output grid for new test input. Each pair follows the same transformation rule. Grids are 2D arrays represented as strings, with cells (colors) separated by spaces and rows by newlines.\\nHere are the input and output grids for the reference examples:\\nExample 1\\nInput:\\nGray Black Black Gray Black\\nGray Black Black Gray Black\\nGray Black Gray Gray Gray\\nGray Gray Gray Black Black\\nBlack Black Gray Black Black\\nBlack Black Gray Gray Gray\\nBlack Black Black Gray Black\\nGray Gray Gray Gray Black\\nBlack Gray Black Black Black\\nBlack Gray Black Black Black\\nBlack Gray Gray Gray Black\\nBlack Black Black Gray Black\\nBlack Gray Gray Gray Gray\\nGray Gray Black Black Black\\nBlack Gray Black Black Black\\n\\nOutput:\\nGray Black Black Gray Black\\nGray Black Black Gray Black\\nGray Black Gray Gray Gray\\nGray Gray Gray Black Black\\nBlack Black Gray Black Black\\nBlack Black Gray Gray Gray\\nBlack Black Black Gray Purple\\nGray Gray Gray Gray Purple\\nBlack Gray Purple Purple Purple\\nBlack Gray Purple Purple Purple\\nBlack Gray Gray Gray Purple\\nBlack Black Black Gray Purple\\nBlack Gray Gray Gray Gray\\nGray Gray Black Black Black\\nOrange Gray Black Black Black\\n\\n\\nExample 2\\nInput:\\nBlack Black Gray Black Black Gray Black Black Black\\nBlack Black Gray Gray Gray Gray Black Black Black\\nGray Gray Gray Black Black Black Black Black Black\\nBlack Gray Black Black Black Black Black Black Black\\nBlack Gray Black Black Black Gray Gray Gray Gray\\nBlack Gray Gray Gray Gray Gray Black Black Black\\nGray Gray Black Black Black Gray Gray Gray Gray\\nBlack Black Black Black Black Gray Black Black Black\\nGray Gray Gray Gray Gray Gray Black Black Black\\nBlack Black Black Black Black Gray Black Black Black\\n\\nOutput:\\nBlack Black Gray Orange Orange Gray Purple Purple Purple\\nBlack Black Gray Gray Gray Gray Purple Purple Purple\\nGray Gray Gray Purple Purple Purple Purple Purple Purple\\nBlack Gray Purple Purple Purple Purple Purple Purple Purple\\nBlack Gray Purple Purple Purple Gray Gray Gray Gray\\nBlack Gray Gray Gray Gray Gray Black Black Black\\nGray Gray Black Black Black Gray Gray Gray Gray\\nBlack Black Black Black Black Gray Black Black Black\\nGray Gray Gray Gray Gray Gray Black Black Black\\nBlack Black Black Black Black Gray Black Black Black\\n\\n\\nExample 3\\nInput:\\nBlack Gray Black Black Gray Black Black Black Black Gray Black Black\\nBlack Gray Black Black Gray Gray Gray Black Black Gray Black Black\\nBlack Gray Gray Gray Gray Black Gray Black Black Gray Black Black\\nBlack Black Gray Black Black Black Gray Gray Gray Gray Black Black\\nGray Gray Gray Black Black Black Gray Black Black Gray Gray Gray\\nBlack Black Black Black Black Black Gray Black Black Black Black Black\\nBlack Black Black Gray Gray Gray Gray Black Black Black Black Black\\nGray Gray Gray Gray Black Black Gray Black Black Black Black Black\\nBlack Black Black Gray Black Black Gray Gray Gray Black Black Black\\nBlack Black Black Gray Black Black Black Black Gray Black Black Black\\n\\nOutput:\\nBlack Gray Orange Orange Gray Black Black Black Black Gray Black Black\\nBlack Gray Orange Orange Gray Gray Gray Black Black Gray Black Black\\nBlack Gray Gray Gray Gray Black Gray Black Black Gray Black Black\\nBlack Black Gray Black Black Black Gray Gray Gray Gray Black Black\\nGray Gray Gray Black Black Black Gray Purple Purple Gray Gray Gray\\nBlack Black Black Black Black Black Gray Purple Purple Purple Purple Purple\\nBlack Black Black Gray Gray Gray Gray Purple Purple Purple Purple Purple\\nGray Gray Gray Gray Black Black Gray Purple Purple Purple Purple Purple\\nBlack Black Black Gray Black Black Gray Gray Gray Purple Purple Purple\\nBlack Black Black Gray Black Black Black Black Gray Purple Purple Purple\\n\\n\\nHere is the input grid for the test example:\\nInput:\\nBlack Gray Black Black Black Black Black Gray Black Black Gray Black\\nBlack Gray Black Black Black Gray Gray Gray Black Gray Gray Black\\nGray Gray Gray Black Black Gray Black Gray Gray Gray Black Black\\nBlack Black Gray Gray Gray Gray Black Gray Black Gray Gray Black\\nBlack Black Black Gray Black Black Black Gray Black Black Gray Black\\n\\nWrite a Python function `transform` that can convert any given input grid to its corresponding output grid based on the pattern observed in the reference examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.tokenize(grid_encoder.to_text(np.zeros((1, 20), dtype=np.int32))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(grid_encoder.to_text(np.zeros((2, 2), dtype=np.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize('Input:\\nGray Black Black Gray Black\\nGray Black Black Gray Black\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ac4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_encoder.to_text([np.arange(10).tolist()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f44a2",
   "metadata": {},
   "source": [
    "## Learnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fe915",
   "metadata": {},
   "source": [
    "First predictions took 3 seconds per task to make 8 predictions. A quick estimation will make 2048 predictions\n",
    "per task when running on 4 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9cc602",
   "metadata": {},
   "source": [
    "Training tasks\n",
    "\n",
    "```\n",
    "8 preds\n",
    "Average time per task: 2.99 seconds\n",
    "         valid code\tvalid outputs\tunique outputs\tdsl usage\tpixel similarity\tcorrect grids\tsolved task\n",
    "MEAN\t1.0\t0.652188\t0.524687\t0.0\t0.572869\t0.129519\t0.245\n",
    "\n",
    "16 preds\n",
    "Average time per task: 5.60 seconds\n",
    "MEAN\t1.0\t0.657969\t0.473125\t0.0\t0.595426\t0.131211\t0.3075\n",
    "\n",
    "32 preds\n",
    "Average time per task: 11.24 seconds\n",
    "MEAN\t0.25\t0.161973\t0.104766\t0.0\t0.610798\t0.13038\t0.35\n",
    "\n",
    "64 preds\n",
    "Average time per task: 22.60 seconds\n",
    "MEAN\t0.49998\t0.326367\t0.187559\t0.0\t0.605516\t0.123638\t0.38\n",
    "\n",
    "128 preds\n",
    "Average time per task: 45.14 seconds\n",
    "MEAN\t0.99998\t0.657676\t0.336602\t0.0\t0.604492\t0.130672\t0.4475\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063577fd",
   "metadata": {},
   "source": [
    "Evaluation tasks:\n",
    "\n",
    "```\n",
    "8 preds\n",
    "Average time per task: 3.64 seconds\n",
    "\tvalid code\tvalid outputs\tunique outputs\tdsl usage\tpixel similarity\tcorrect grids\tsolved task\n",
    "MEAN\t1.0\t0.62125\t0.551562\t0.0\t0.547859\t0.032785\t0.0475\n",
    "\n",
    "16 preds\n",
    "Average time per task: 6.65 seconds\n",
    "1.0\t0.609062\t0.493906\t0.0\t0.575911\t0.032614\t0.0575\n",
    "\n",
    "32 preds\n",
    "Average time per task: 13.19 seconds\n",
    "MEAN\t1.0\t0.615313\t0.464687\t0.0\t0.569353\t0.031603\t0.1025\n",
    "\n",
    "64 preds\n",
    "Average time per task: 26.53 seconds\n",
    "MEAN\t0.999961\t0.614297\t0.421953\t0.0\t0.578199\t0.031114\t0.115\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3f88f",
   "metadata": {},
   "source": [
    "Evaluation 2025:\n",
    "\n",
    "```\n",
    "8 preds\n",
    "Average time per task: 4.58 seconds\n",
    "MEAN\t1.0\t0.560417\t0.515625\t0.0\t0.505795\t0.000333\t0.0\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ce8cc8",
   "metadata": {},
   "source": [
    "### Execution improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8a0d2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "    valid code\tvalid outputs\tunique outputs\tdsl usage\tpixel similarity\tcorrect grids\tsolved task\n",
    "# baseline\n",
    "MEAN\t1.0\t0.652188\t0.524687\t0.0\t0.572869\t0.129519\t0.245\n",
    "# Add purple and brown to Color.\n",
    "MEAN\t1.0\t0.769687\t0.617188\t0.0\t0.612253\t0.144651\t0.2825\n",
    "# change how the outputs are created\n",
    "MEAN\t1.0\t0.797188\t0.637188\t0.0\t0.60988\t0.144386\t0.285\n",
    "# use np.array instead of Img as input\n",
    "MEAN\t1.0\t0.824063\t0.656563\t0.0\t0.615033\t0.148643\t0.2975\n",
    "```\n",
    "\n",
    "Valid outputs increase by 11%, solved tasks improve from 24.5% to 28.25%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9afe7",
   "metadata": {},
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747472b",
   "metadata": {},
   "source": [
    "- [x] Implement new grid encoder\n",
    "- [x] Use the correct prompt\n",
    "- [x] Save predictions to file so I can later reprocess them\n",
    "- [ ] Update code execution to match the code generated by BARC model\n",
    "- [ ] Check code execution to verify that exceptions are legit and not easily solvable\n",
    "  - [x] Add missing colors to Color object\n",
    "  - [x] Code execution fails when there are auxiliary functions. `Error executing code for task 025d127b, response 5: <class 'NameError'> name 'blend_colors' is not defined`\n",
    "- [ ] Validate that I get the same scores of the paper\n",
    "- [ ] Evaluate on different datasets\n",
    "- [ ] Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49268ab8",
   "metadata": {},
   "source": [
    "8: \"Purple\",\n",
    "9: \"Brown\" \n",
    "\n",
    "TEAL = 8\n",
    "MAROON = 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
