{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5ef70f",
   "metadata": {},
   "source": [
    "# Search with BARC induction models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387259d",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4dabfd",
   "metadata": {},
   "source": [
    "Can we solve ARC tasks using base models with access to a DSL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e9df7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1013af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "import glob\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_python_code(code):\n",
    "    display(Markdown(f\"```python\\n{code}\\n```\"))\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import pretty_print_prompt, Template\n",
    "from arc25.metrics import pixel_similarity_score, correct_grids_score\n",
    "from arc25.utils import get_timestamp\n",
    "from arc25.plot import plot_task\n",
    "from arc25.data_augmentation import apply_data_augmentation, revert_data_augmentation, get_random_data_augmentation_params\n",
    "from arc25.code_execution import safe_code_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2076585",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd75ce",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arc_data_with_solutions(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    solutions_filepath = filepath.replace('challenges.json', 'solutions.json')\n",
    "    if filepath != solutions_filepath and os.path.exists(solutions_filepath):\n",
    "        with open(solutions_filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        for sample_id, task in data.items():\n",
    "            for idx, sample in enumerate(task['test']):\n",
    "                sample['output'] = solutions[sample_id][idx]\n",
    "    verify_that_all_samples_have_output(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def verify_that_all_samples_have_output(data):\n",
    "    for task in data.values():\n",
    "        if isinstance(task, dict):\n",
    "            verify_that_task_has_outputs(task)\n",
    "        elif isinstance(task, list):\n",
    "            for subtask in task:\n",
    "                verify_that_task_has_outputs(subtask)\n",
    "\n",
    "\n",
    "def verify_that_task_has_outputs(task):\n",
    "    for partition, samples in task.items():\n",
    "        if partition not in ['train', 'test']:\n",
    "            continue\n",
    "        for sample in samples:\n",
    "            if 'output' not in sample:\n",
    "                raise ValueError('Not all samples have output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_challenges = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json')\n",
    "evaluation_challenges = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json')\n",
    "evaluation_challenges_2025 = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2025/arc-agi_evaluation_challenges.json')\n",
    "all_challenges = {**training_challenges, **evaluation_challenges, **evaluation_challenges_2025}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfba8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(task_id):\n",
    "    if task_id in all_challenges:\n",
    "        task = all_challenges[task_id]\n",
    "        task = {partition: [{key: np.array(value) for key, value in sample.items()} for sample in samples] for partition, samples in task.items()}\n",
    "        return task\n",
    "    else:\n",
    "        raise ValueError(f'Task ID {task_id} not found in challenges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaff0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id in all_challenges:\n",
    "    try:\n",
    "        _ = get_task(task_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading task {task_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ec071",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f12f27",
   "metadata": {},
   "source": [
    "https://github.com/flowersteam/SOAR/blob/main/soar/prompt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f42cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/barc0/Llama-3.1-ARC-Potpourri-Induction-8B\n",
    "system_prompt = \"\"\"You are a world-class puzzle solver with exceptional pattern recognition skills and expertise in Python programming. Your task is to analyze puzzles and provide Python solutions.\"\"\"\n",
    "\n",
    "prompt_template_text = \"\"\"Given input-output grid pairs as reference examples, carefully observe the patterns to predict the output grid for new test input. Each pair follows the same transformation rule. Grids are 2D arrays represented as strings, with cells (colors) separated by spaces and rows by newlines.\n",
    "Here are the input and output grids for the reference examples:\n",
    "{% for sample in train_samples %}Example {{ loop.index }}\n",
    "Input:\n",
    "{{ sample.input }}\n",
    "\n",
    "Output:\n",
    "{{ sample.output }}\n",
    "\n",
    "{% endfor %}\n",
    "Here is the input grid for the test example:\n",
    "{{ test }}\n",
    "\n",
    "Write a Python function `transform` that can convert any given input grid to its corresponding output grid based on the pattern observed in the reference examples.\n",
    "\"\"\"\n",
    "\n",
    "# I have verified that all responses start with this prefix\n",
    "common_prefix = \"Let's solve this puzzle using Python code with the common library functions. We'll first reason about the problem and then write the code to solve it. The `transform` function will take the input grid and return the output grid. Here is the Python code with the comments describing how to solve the problem:\\n\" #```python\\nfrom common import *\\n\"\n",
    "\n",
    "prompt_template = Template(prompt_template_text)\n",
    "\n",
    "def create_prompt_from_task(task, grid_encoder, tokenizer, shuffle_train_samples=True):\n",
    "    train_samples = [{'input': grid_encoder.to_text(sample['input']), 'output': grid_encoder.to_text(sample['output'])} for sample in task['train']]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    test_sample = random.choice(task['test'])\n",
    "    render_kwargs = dict(train_samples=train_samples, test=grid_encoder.to_text(test_sample['input']))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)},\n",
    "                {\"role\": \"assistant\", \"content\": common_prefix}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=False,\n",
    "                                            continue_final_message=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae964737",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1,\n",
    "               max_model_len=32000, enable_lora=False, max_lora_rank=16):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=0.92,  # Use less GPU memory\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "        enable_lora=enable_lora,\n",
    "        max_lora_rank=max_lora_rank,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea848918",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae210ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_python_code(text):\n",
    "    # Extract Python code from the text\n",
    "    if '```python' not in text:\n",
    "        return ''\n",
    "    code = text.split('```python')[1]\n",
    "    if not '```' in code:\n",
    "        return ''\n",
    "\n",
    "    code = code.split('```')[0].strip()\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_python_code(code):\n",
    "    remove_line_keywords = ['import dsl', 'from dsl import ', 'print(', 'from common import *']\n",
    "    code = '\\n'.join(line for line in code.split('\\n') if not any(keyword in line for keyword in remove_line_keywords))\n",
    "    # code = 'from arc25.BARC_dsl import *\\n' + code  # Ensure BARC_dsl is imported\n",
    "    return code.strip()\n",
    "\n",
    "def add_additional_imports(code):\n",
    "    additional_imports = [\n",
    "        'from typing import List, Tuple',\n",
    "        'import numpy as np',\n",
    "        'import numpy',\n",
    "        'from arc25.BARC_dsl import *',\n",
    "    ]\n",
    "    imports = '\\n'.join(additional_imports)\n",
    "    return imports + '\\n' + code if code else imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExecutionResult = namedtuple(\"ExecutionResult\", [\"task_id\", \"index\", \"code\", \"outputs\", \"error_type\", \"error_message\"])\n",
    "\n",
    "def run_code_from_predictions(predictions: dict[str, list[str]], batch_size=32000, n_jobs=-1):\n",
    "    # Precompute inputs per task once\n",
    "    # task_inputs = {tid: [np.array(g) for g in get_task(tid).inputs] for tid in predictions}\n",
    "\n",
    "    task_inputs = dict()\n",
    "    for task_id in predictions:\n",
    "        task = get_task(task_id)\n",
    "        task_inputs[task_id] = [sample['input'] for sample in task['train']] + [sample['input'] for sample in task['test']]\n",
    "\n",
    "    # Flatten all predictions into a work list\n",
    "    work = [\n",
    "        (tid, i, text_pred, task_inputs[tid], prediction_data['data_augmentation_params'][i])\n",
    "        for tid, prediction_data in predictions.items()\n",
    "        for i, text_pred in enumerate(prediction_data['text_predictions'])\n",
    "    ]\n",
    "\n",
    "    # sort the work by prediction index first and the task id second, I believe this will improve resource allocation\n",
    "    # because some tasks are more resource intensive than others\n",
    "    work.sort(key=lambda x: (x[1], x[0]))\n",
    "\n",
    "    # with tqdm_joblib(total=len(work), desc=\"Executing predictions\", unit=\"pred\", smoothing=0):\n",
    "    #     results = Parallel(\n",
    "    #         n_jobs=n_jobs,\n",
    "    #         backend=\"loky\",\n",
    "    #         prefer=\"processes\",\n",
    "    #         batch_size='auto', # previously was 1\n",
    "    #     )(delayed(_run_one)(*args) for args in work)\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(work), batch_size), desc=\"Executing predictions\", unit=\"batch\"):\n",
    "        batch = work[i:i+batch_size]\n",
    "        with tqdm_joblib(total=len(batch), desc=f\"Executing predictions for batch {i//batch_size}\", unit=\"pred\", smoothing=0):\n",
    "            batch_results = Parallel(\n",
    "                n_jobs=n_jobs,\n",
    "                backend=\"loky\",\n",
    "                prefer=\"processes\",\n",
    "                batch_size='auto', # previously was 1\n",
    "            )(delayed(_run_one)(*args) for args in batch)\n",
    "            results.extend(batch_results)\n",
    "\n",
    "    results_by_task = {tid: [] for tid in predictions}\n",
    "    for result in results:\n",
    "        results_by_task[result.task_id].append(result)\n",
    "    return results_by_task\n",
    "\n",
    "\n",
    "def _run_one(task_id, i, text_prediction, input_grids, data_augmentation_params=None):\n",
    "    code = parse_python_code(text_prediction)\n",
    "    if not code:\n",
    "        return ExecutionResult(task_id, i, None, None, \"ParsingCodeFailed\", '')\n",
    "    try:\n",
    "        if data_augmentation_params is not None:\n",
    "            # Apply data augmentation to the input grids\n",
    "            input_grids = apply_data_augmentation(input_grids, **data_augmentation_params)\n",
    "        outs = safe_code_execution(\n",
    "            add_additional_imports(curate_python_code(code)),\n",
    "            input_grids,\n",
    "            func_name=\"transform\",\n",
    "            execution_method='exec',\n",
    "        )\n",
    "        outs = validate_outputs(outs)\n",
    "        # print(outs)\n",
    "        if data_augmentation_params is not None:\n",
    "            outs = revert_data_augmentation(outs, **data_augmentation_params)\n",
    "        # print(outs)\n",
    "        return ExecutionResult(task_id, i, code, outs, None, None)\n",
    "    except BaseException as e:\n",
    "        return ExecutionResult(task_id, i, code, None, type(e).__name__, e)\n",
    "\n",
    "# tiny_predictions = {'00576224': predictions['00576224']}\n",
    "# predicted_code, predicted_outputs = run_code_from_predictions(tiny_predictions, log_errors=True)\n",
    "# df = compute_search_metrics(list(tiny_predictions.keys()), predicted_code, predicted_outputs, n_preds)\n",
    "# df.round(3)\n",
    "# predicted_code, predicted_outputs = run_code_from_predictions(predictions, log_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee204f",
   "metadata": {},
   "source": [
    "### Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_outputs(outputs):\n",
    "    if not outputs:\n",
    "        raise ValueError(\"Outputs list is empty\")\n",
    "    return [_validate_output(output) for output in outputs]\n",
    "\n",
    "def _validate_output(output):\n",
    "    if output is None:\n",
    "        raise ValueError(\"Output is None\")\n",
    "    output = np.array(output, dtype=int) # otherwise I see weird outputs that mix list and numpy arrays\n",
    "    if output.ndim != 2:\n",
    "        raise ValueError(f\"Output is not a 2D array. Output shape: {output.shape}\")\n",
    "    if max(output.shape) > 35:\n",
    "        raise ValueError(f\"Output is too large, the maximum allowed shape is 30x30. Output shape: {output.shape}\")\n",
    "    if min(output.shape) == 0:\n",
    "        raise ValueError(f\"Output has zero dimension, it is empty. Output shape: {output.shape}\")\n",
    "    if np.max(output) > 9 or np.min(output) < 0:\n",
    "        raise ValueError(f\"Output contains invalid values, expected values in range [0, 9]. Output max: {np.max(output)}, min: {np.min(output)}\")\n",
    "    # if not np.issubdtype(output.dtype, np.integer):\n",
    "    #     raise ValueError(f\"Output contains non-integer values, expected integer values. Output dtype: {output.dtype}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee616990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def fingerprint(prediction):\n",
    "    \"\"\"\n",
    "    Create a compact hash for a list of matrices.\n",
    "    Includes shape & dtype to distinguish e.g. (2Ã—2) from (4Ã—1).\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    for m in prediction:\n",
    "        # incorporate shape and dtype in a reproducible way\n",
    "        h.update(str(m.shape).encode())\n",
    "        h.update(m.dtype.str.encode())\n",
    "        # raw data bytes\n",
    "        h.update(m.tobytes())\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b4bd9",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db97f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_search_metrics(results):\n",
    "    df = pd.DataFrame(columns=['n_preds', 'valid code', 'valid outputs', 'unique outputs', 'pixel similarity', 'correct grids', \n",
    "                               'train_pass_rate', 'train_pass@n', 'pass_rate', 'pass@n'])\n",
    "    for task_id, task_results in results.items():\n",
    "        n_preds = len(task_results)\n",
    "        df.loc[task_id, 'n_preds'] = n_preds\n",
    "        valid_code = [result.code for result in task_results if result.code is not None]\n",
    "        df.loc[task_id, 'valid code'] = len(valid_code)/n_preds\n",
    "        valid_outputs = [result.outputs for result in task_results if result.outputs is not None]\n",
    "        df.loc[task_id, 'valid outputs'] = len(valid_outputs)/n_preds\n",
    "        df.loc[task_id, 'unique outputs'] = len(set(fingerprint(output) for output in valid_outputs))/n_preds\n",
    "\n",
    "        task = get_task(task_id)\n",
    "        task_outputs = [sample['output'] for sample in task['train']] + [sample['output'] for sample in task['test']]\n",
    "        scores = sorted([np.mean([pixel_similarity_score(output, pred) for output, pred in zip(task_outputs, predictions)]) for predictions in valid_outputs])\n",
    "        df.loc[task_id, 'pixel similarity'] = np.mean(scores) if scores else 0.0\n",
    "        \n",
    "        #TODO: there is a bug in this pass_rate calculation, because it does not use all the outputs, only the valid ones\n",
    "        scores = sorted([correct_grids_score(task_outputs, predictions) for predictions in valid_outputs])\n",
    "        df.loc[task_id, 'correct grids'] = np.mean(scores) if scores else 0.0\n",
    "        df.loc[task_id, 'pass_rate'] = np.mean(np.array(scores) == 1) if scores else 0\n",
    "        df.loc[task_id, 'pass@n'] = int(np.max(scores) == 1) if scores else 0\n",
    "\n",
    "        train_outputs = [sample['output'] for sample in task['train']]\n",
    "        train_scores = sorted([correct_grids_score(train_outputs, predictions[:len(train_outputs)]) for predictions in valid_outputs])\n",
    "        df.loc[task_id, 'train_pass_rate'] = np.mean(np.array(train_scores) == 1) if train_scores else 0\n",
    "        df.loc[task_id, 'train_pass@n'] = int(np.max(train_scores) == 1) if train_scores else 0\n",
    "\n",
    "    df.loc['MEAN'] = df.mean(axis=0)\n",
    "    return df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b90d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_analysis(results):\n",
    "    errors_to_check = ['TimeoutException', 'NonDeterministicCode', 'UnsafeCode', 'ParsingCodeFailed']\n",
    "\n",
    "    df = pd.DataFrame(columns=['n_preds', 'error_rate'] + errors_to_check)\n",
    "    all_errors = []\n",
    "    for task_id, task_results in results.items():\n",
    "        task_errors = [result.error_type for result in task_results if result.error_type is not None]\n",
    "        all_errors.extend(task_errors)\n",
    "        df.loc[task_id, 'n_preds'] = len(task_results)\n",
    "        df.loc[task_id, 'error_rate'] = len(task_errors) / len(task_results) if task_results else 0.0\n",
    "        for error_type in errors_to_check:\n",
    "            df.loc[task_id, error_type] = sum(1 for error in task_errors if error == error_type) / len(task_results) if task_results else 0.0\n",
    "    df.loc['MEAN'] = df.mean(axis=0)\n",
    "\n",
    "    error_counts = pd.Series(all_errors).value_counts()\n",
    "    print(\"Most common errors:\")\n",
    "    display(error_counts.head(20))\n",
    "    return df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fed80",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca60f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_predictions(path_pattern):\n",
    "    filepaths = glob.glob(path_pattern)\n",
    "    predictions = dict()\n",
    "    for filepath in tqdm(filepaths, desc=\"Loading predictions\", disable=len(filepaths)<=1):\n",
    "        with open(filepath, 'r') as f:\n",
    "            preds = json.load(f)\n",
    "        for task_id, outputs in preds.items():\n",
    "            if task_id not in predictions:\n",
    "                predictions[task_id] = dict(text_predictions=[], data_augmentation_params=[])\n",
    "            if isinstance(outputs, dict):\n",
    "                predictions[task_id]['text_predictions'].extend(outputs['text_predictions'])\n",
    "                data_augmentation_params = outputs.get('data_augmentation_params', None)\n",
    "                if data_augmentation_params is not None and data_augmentation_params['color_map'] is not None:\n",
    "                    data_augmentation_params['color_map'] = {int(k): int(v) for k, v in data_augmentation_params['color_map'].items()}\n",
    "                predictions[task_id]['data_augmentation_params'].extend([data_augmentation_params]*len(outputs['text_predictions']))\n",
    "            else:\n",
    "                predictions[task_id]['text_predictions'].extend(outputs)\n",
    "                predictions[task_id]['data_augmentation_params'].extend([None] * len(outputs))  # Assuming no params for old format\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ba732",
   "metadata": {},
   "source": [
    "## Independent search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a678a",
   "metadata": {},
   "source": [
    "Does using data augmentation increases the diversity of the predictions and improves the pass@n metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c0da6",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_task_ids = {\n",
    "    'training': list(training_challenges.keys()),\n",
    "    'evaluation': list(evaluation_challenges.keys()),\n",
    "    'evaluation-2025': list(evaluation_challenges_2025.keys())\n",
    "}\n",
    "\n",
    "experiment_name = '2025-08-22_add-common-prefix'\n",
    "dataset = 'training'\n",
    "task_ids = dataset_to_task_ids[dataset]\n",
    "grid_encoder = create_grid_encoder('ColorNameEncoder()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd9fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = create_prompt_from_task(get_task(task_ids[0]), grid_encoder, tokenizer)\n",
    "pretty_print_prompt(text, default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [8]:\n",
    "    sampling_params = SamplingParams(n=n, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "\n",
    "    prompts, data_augmentation_params = [], []\n",
    "    for task_id in task_ids:\n",
    "        params = get_random_data_augmentation_params()\n",
    "        data_augmentation_params.append(params)\n",
    "        task = get_task(task_id)\n",
    "        task = apply_data_augmentation(task, **params)\n",
    "        prompt = create_prompt_from_task(\n",
    "            task, grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    t0 = time.time()\n",
    "    text_predictions = llm.generate(prompts, sampling_params)\n",
    "    total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "    inference_time = time.time() - t0\n",
    "    print(f\"Total tokens generated: {total_tokens}\")\n",
    "    print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "    print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "    print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "    print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "\n",
    "    predictions = dict()\n",
    "    for task_id, output, params in zip(task_ids, text_predictions, data_augmentation_params):\n",
    "        predictions[task_id] = {\n",
    "            'text_predictions': [output.text for output in output.outputs],\n",
    "            'data_augmentation_params': params,\n",
    "        }\n",
    "\n",
    "    output_filepath = f'/mnt/hdd0/Kaggle/arc25/predictions/{experiment_name}/{dataset}_{sampling_params.n}preds_{get_timestamp()}_predictions.json'\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(f\"Predictions saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a2167",
   "metadata": {},
   "source": [
    "```\n",
    "training\n",
    "8 preds, Average time per task: 3.25 seconds\n",
    "Average time per task: 2.88 seconds, when adding the common prefix\n",
    "\n",
    "evaluation\n",
    "8 preds, Average time per task: 4.17 seconds\n",
    "(previously it was 3.64 when using n-1 training samples)\n",
    "\n",
    "evaluation-2025\n",
    "8 preds, Average time per task: 5.57 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91602ec",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdccaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/evaluation-2025_*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/evaluation_*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-23_no-data-augmentation/evaluation_*.json')\n",
    "\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/*.json')\n",
    "# predictions = {key: predictions[key] for key in list(predictions)[:35]}\n",
    "\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_8preds_2025_08_23_11_57_11_predictions.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "results = run_code_from_predictions(predictions)\n",
    "df = compute_search_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528a486",
   "metadata": {},
   "source": [
    "Make inference more robust:\n",
    "\n",
    "- baseline, 10.4s\n",
    "- batch size auto, clearly improves speed 6.6s\n",
    "- BaseException for TimeoutException, 6.4s\n",
    "- Try subprocess: 45.2s\n",
    "- Restrict globals: 6.5s\n",
    "- Redirect prints: 6.5s\n",
    "\n",
    "```\n",
    "baseline, 10.4s\n",
    "\tn_preds\tvalid code\tvalid outputs\tunique outputs\tpixel similarity\tcorrect grids\ttrain_pass_rate\ttrain_pass@n\tpass_rate\tpass@n\n",
    "MEAN\t8.0\t1.0\t0.765625\t0.615938\t0.611174\t0.148103\t0.12081\t0.27\t0.11822\t0.265 # baseline\n",
    "MEAN\t8.0\t1.0\t0.765\t0.615313\t0.611206\t0.148114\t0.12081\t0.27\t0.11822\t0.265 # batch size auto\n",
    "MEAN\t8.0\t1.0\t0.765312\t0.615625\t0.611184\t0.148103\t0.12081\t0.27\t0.11822\t0.265 # BaseException\n",
    "MEAN\t8.0\t1.0\t0.7625\t0.612812\t0.610983\t0.147886\t0.120512\t0.27\t0.117863\t0.2625 # subprocess\n",
    "MEAN\t8.0\t1.0\t0.765\t0.615313\t0.611078\t0.148103\t0.12081\t0.27\t0.11822\t0.265 # restrict globals\n",
    "MEAN\t8.0\t1.0\t0.765312\t0.615625\t0.611184\t0.148103\t0.12081\t0.27\t0.11822\t0.265 # redirect prints\n",
    "\n",
    "NonDeterministicCode    233\n",
    "ValueError              160\n",
    "IndexError              152\n",
    "AssertionError          112\n",
    "TypeError                26\n",
    "TimeoutException         14\n",
    "AttributeError           14\n",
    "UnboundLocalError        10\n",
    "UnsafeCode                7\n",
    "StopIteration             5\n",
    "NameError                 5\n",
    "ZeroDivisionError         5\n",
    "AxisError                 3\n",
    "KeyError                  3\n",
    "SyntaxError               1\n",
    "Name: count, dtype: int64\n",
    "\n",
    "```\n",
    "\n",
    "- Full evaluation, 7m54\n",
    "- Split in batches, 8m5s\n",
    "\n",
    "```\n",
    "n_preds\tvalid code\tvalid outputs\tunique outputs\tpixel similarity\tcorrect grids\ttrain_pass_rate\ttrain_pass@n\tpass_rate\tpass@n\n",
    "MEAN\t480.0\t1.0\t0.708432\t0.438161\t0.564959\t0.027705\t0.01887\t0.225\t0.018578\t0.2175\n",
    "MEAN\t480.0\t1.0\t0.708312\t0.438109\t0.564917\t0.027686\t0.01889\t0.23\t0.018598\t0.2225\n",
    "\n",
    "\n",
    "NonDeterministicCode    12993\n",
    "ValueError              12521\n",
    "AssertionError          11387\n",
    "IndexError              11175\n",
    "TimeoutException         3022\n",
    "TypeError                1538\n",
    "AttributeError           1155\n",
    "UnboundLocalError         618\n",
    "NameError                 398\n",
    "StopIteration             288\n",
    "KeyError                  282\n",
    "ZeroDivisionError         223\n",
    "UnsafeCode                167\n",
    "SyntaxError               131\n",
    "RecursionError             31\n",
    "AxisError                  14\n",
    "OverflowError              12\n",
    "IndentationError           11\n",
    "RuntimeError                7\n",
    "UFuncTypeError              7\n",
    "Name: count, dtype: int64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abafff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/arc25/code_execution/evaluation_480.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c3c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9f17b",
   "metadata": {},
   "source": [
    "Problems with output type\n",
    "```\n",
    "n_preds\tvalid code\tvalid outputs\tunique outputs\tpixel similarity\tcorrect grids\ttrain_pass_rate\ttrain_pass@n\tpass_rate\tpass@n\n",
    "MEAN\t464.0\t1.0\t0.747236\t0.425787\t0.570709\t0.027715\t0.018743\t0.2\t0.018591\t0.195 # baseline\n",
    "MEAN\t464.0\t1.0\t0.745275\t0.425339\t0.570379\t0.027173\t0.018295\t0.1975\t0.018143\t0.1925 # force type to int\n",
    "MEAN\t464.0\t1.0\t0.726083\t0.417619\t0.565073\t0.027337\t0.018675\t0.1975\t0.018524\t0.19 # raise if type is not int\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "n_preds_range = 2**np.arange(0, int(np.log2(n_preds)) + 2)\n",
    "fail_prob = 1 - df['pass_rate'].values[:-1]\n",
    "for n in n_preds_range:\n",
    "    scores.append(float(np.mean(1 - fail_prob**n)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(n_preds_range, scores, marker='o')\n",
    "plt.xscale('log', base=2)\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('pass@n vs Number of Predictions')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "dict(evaluation_data_augmentation=(n_preds_range.tolist(), scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa43cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'evaluation_no_data_augmentation': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.018653395632067863,\n",
    "   0.028123807513897266,\n",
    "   0.040470801097098115,\n",
    "   0.05604907442616278,\n",
    "   0.07517185395071181,\n",
    "   0.09714182632584087,\n",
    "   0.12045095653955698,\n",
    "   0.14292755780979274,\n",
    "   0.16171510346135057,\n",
    "   0.1753448200072779]),\n",
    "   'evaluation_data_augmentation': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.018665553329123646,\n",
    "   0.02879110915200162,\n",
    "   0.04224158293230386,\n",
    "   0.059708013904966675,\n",
    "   0.08223659237317406,\n",
    "   0.10940176527483192,\n",
    "   0.1384685993616617,\n",
    "   0.1664683605627952,\n",
    "   0.1908837471080634,\n",
    "   0.2094882044080468]),\n",
    "   'evaluation_2025': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.0006127450980392154,\n",
    "   0.0011804354094579,\n",
    "   0.0021936594882075557,\n",
    "   0.0038098619423907474,\n",
    "   0.005877918122372206,\n",
    "   0.007609845670347021,\n",
    "   0.008270521205514126,\n",
    "   0.008332859889725193,\n",
    "   0.00833333330643547,\n",
    "   0.008333333333333333]),\n",
    "   'training': ([1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "  [0.11879157321399551,\n",
    "   0.1820998968374452,\n",
    "   0.2523552039747298,\n",
    "   0.320614792900346,\n",
    "   0.3840515764870714,\n",
    "   0.44540837829332164,\n",
    "   0.5039219325812776,\n",
    "   0.5537227577351216,\n",
    "   0.5891170927046698]),\n",
    "   'evaluation_6064': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024,\n",
    "   2048,\n",
    "   4096,\n",
    "   8192],\n",
    "  [0.020675799545352484,\n",
    "   0.031800090738608865,\n",
    "   0.04635819085811816,\n",
    "   0.06506021688568361,\n",
    "   0.08888375747790557,\n",
    "   0.11740666375181305,\n",
    "   0.14840423707784808,\n",
    "   0.1798639618765866,\n",
    "   0.21174751560392843,\n",
    "   0.24540332456877859,\n",
    "   0.27906954197611594,\n",
    "   0.30882907544705135,\n",
    "   0.3307049086550017,\n",
    "   0.3426571213621735])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cda96f6",
   "metadata": {},
   "source": [
    "### Compare with and without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51249750",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'training-arc-agi-1': ([1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "  [0.1208341472917777,\n",
    "   0.1852714696641388,\n",
    "   0.256145417062672,\n",
    "   0.323924981207185,\n",
    "   0.3859031489395669,\n",
    "   0.4455752909442623,\n",
    "   0.5032849187983576,\n",
    "   0.553359922920049,\n",
    "   0.5897329957581053]),\n",
    "   'evaluation-arc-agi-1': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.019526342790732235,\n",
    "   0.02979252155031361,\n",
    "   0.04328670515467564,\n",
    "   0.060383761781122286,\n",
    "   0.08205688086630819,\n",
    "   0.10825312270378742,\n",
    "   0.13655160734160618,\n",
    "   0.1640966274942665,\n",
    "   0.18866504543675547,\n",
    "   0.20813729316511306]),\n",
    "  'evaluation-arc-agi-2': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.0006410256410256406,\n",
    "   0.001232741617357001,\n",
    "   0.002283125007294328,\n",
    "   0.003940730838716728,\n",
    "   0.00601793853224789,\n",
    "   0.007690006963146114,\n",
    "   0.008283669075103941,\n",
    "   0.008333037348707877,\n",
    "   0.008333333322820506,\n",
    "   0.008333333333333333])}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(20, 5))\n",
    "for plot_ids, key in enumerate(metrics):\n",
    "    plt.subplot(1, len(keys), plot_ids + 1)\n",
    "    n_preds_range, scores = metrics[key]\n",
    "    plt.plot(n_preds_range, scores, marker='o', label=key)\n",
    "    plt.xscale('log', base=2)\n",
    "    # plt.grid(which='both', axis='both')\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('pass@n')\n",
    "    plt.title(key)\n",
    "\n",
    "plt.suptitle('pass@n vs Number of Predictions')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "2**16 # might solve training-arc-agi-1, this shows that there is room for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318ee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'baseline': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.01833019880853686,\n",
    "   0.027800302745602773,\n",
    "   0.039987770244753335,\n",
    "   0.05523558985150286,\n",
    "   0.07429521938067872,\n",
    "   0.09699108936447659,\n",
    "   0.12167009514547647,\n",
    "   0.14566997505731888,\n",
    "   0.16623501973442983,\n",
    "   0.18227063267871776]),\n",
    "'data augmentation': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.019526342790732235,\n",
    "   0.02979252155031361,\n",
    "   0.04328670515467564,\n",
    "   0.060383761781122286,\n",
    "   0.08205688086630819,\n",
    "   0.10825312270378742,\n",
    "   0.13655160734160618,\n",
    "   0.1640966274942665,\n",
    "   0.18866504543675547,\n",
    "   0.20813729316511306]),\n",
    "}\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    plt.plot(n_preds_range, scores, marker='o', label=key)\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-1')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630dca40",
   "metadata": {},
   "source": [
    "#### evaluation ARC-AGI-1 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "'baseline': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.01996212663472896,\n",
    "   0.02831747138692302,\n",
    "   0.03977086910606171,\n",
    "   0.05542900033336087,\n",
    "   0.07674247281753263,\n",
    "   0.10320479610157163,\n",
    "   0.13093643779484748,\n",
    "   0.15594708997780551,\n",
    "   0.1770796829272273,\n",
    "   0.19471691227422966,\n",
    "   0.20712048546783934]),\n",
    "'+ data augmentation': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.019813790637075775,\n",
    "   0.029064191088443207,\n",
    "   0.04100375897586597,\n",
    "   0.05671308781660802,\n",
    "   0.07779377587518771,\n",
    "   0.10484135687838109,\n",
    "   0.13591735274813102,\n",
    "   0.1675520359797603,\n",
    "   0.19696420491620842,\n",
    "   0.22203487375136557,\n",
    "   0.23821892651950458])\n",
    "}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    plt.plot(n_preds_range, scores, marker='o', label=key)\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-1')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf42afb",
   "metadata": {},
   "source": [
    "#### Bias of the number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4baf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "'64 preds': ([1, 2, 4, 8, 16, 32, 64, 128],\n",
    "  [0.019106336951575514,\n",
    "   0.028082339121390357,\n",
    "   0.039026953359591325,\n",
    "   0.05221714653327929,\n",
    "   0.06785124181952008,\n",
    "   0.08534713350917737,\n",
    "   0.10178389655716932,\n",
    "   0.11195929946811921]),\n",
    "'112 preds': ([1, 2, 4, 8, 16, 32, 64, 128],\n",
    "  [0.019387411477336133,\n",
    "   0.02887665434462053,\n",
    "   0.04103250559571824,\n",
    "   0.05662528637189867,\n",
    "   0.07657867399726664,\n",
    "   0.1006969445110981,\n",
    "   0.12613859060350577,\n",
    "   0.14808143720453426]),\n",
    "'184 preds': ([1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "  [0.019316797975629206,\n",
    "   0.02854922966041234,\n",
    "   0.0405175566715264,\n",
    "   0.056039023485551356,\n",
    "   0.07638928013313853,\n",
    "   0.10206766052179019,\n",
    "   0.13087569581761588,\n",
    "   0.15844583931075643,\n",
    "   0.1786899038965614]),\n",
    "'584 preds': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.019813790637075775,\n",
    "   0.029064191088443207,\n",
    "   0.04100375897586597,\n",
    "   0.05671308781660802,\n",
    "   0.07779377587518771,\n",
    "   0.10484135687838109,\n",
    "   0.13591735274813102,\n",
    "   0.1675520359797603,\n",
    "   0.19696420491620842,\n",
    "   0.22203487375136557,\n",
    "   0.23821892651950458])\n",
    "}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    plt.plot(n_preds_range, scores, marker='o', label=key)\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-1')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62a70b",
   "metadata": {},
   "source": [
    "#### evaluation arc-agi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a4dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {'baseline': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024,\n",
    "   2048],\n",
    "  [0.000525120200263568,\n",
    "   0.0010290649730527559,\n",
    "   0.001977218777084015,\n",
    "   0.0036586995036725616,\n",
    "   0.006324151121543866,\n",
    "   0.009785455928199816,\n",
    "   0.013112192645441874,\n",
    "   0.015422166919107317,\n",
    "   0.016485239578931988,\n",
    "   0.016662717947703534,\n",
    "   0.016666664795580977,\n",
    "   0.016666666666666247]),\n",
    "'data_augmentation': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.0004395694358616827,\n",
    "   0.0008607058465457645,\n",
    "   0.001651027938816423,\n",
    "   0.003045200797763149,\n",
    "   0.005231079244621852,\n",
    "   0.00801093382400977,\n",
    "   0.010647211929364896,\n",
    "   0.012848270575498736,\n",
    "   0.014933881468222887,\n",
    "   0.016306378321672744])}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    plt.plot(n_preds_range, scores, marker='o', label=key)\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-2')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9875f",
   "metadata": {},
   "source": [
    "### Distribution of prediction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9cf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['training', 'evaluation', 'evaluation-2025']:\n",
    "    predictions = load_all_predictions(f'/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_fix-bug/{key}_*.json')\n",
    "    prediction_length_distribution = {tid: [len(tokens) for tokens in tokenizer(preds['text_predictions'])['input_ids']] \\\n",
    "                                      for tid, preds in tqdm(predictions.items(), desc=\"Computing prediction lengths\", total=len(predictions))}\n",
    "    all_lengths = [length for lengths in prediction_length_distribution.values() for length in lengths]\n",
    "    label = f\"{key} (max output tokens: {max(all_lengths)}, median output tokens: {int(np.median(all_lengths))})\"\n",
    "    bins = np.linspace(0, 2000, 100)\n",
    "    plt.hist(all_lengths, bins=bins, label=label, alpha=0.5, density=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.title('Distribution of prediction lengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e47dc1",
   "metadata": {},
   "source": [
    "### Inspect correct solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa07214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id in df[df['pass_rate'] > 0].index.values[:-1]:\n",
    "    print(f'https://arcprize.org/play?task={task_id} pass rate: {df.loc[task_id, \"pass_rate\"]:.2%}')\n",
    "    task = get_task(task_id)\n",
    "    task_outputs = [sample['output'] for sample in task['train']] + [sample['output'] for sample in task['test']]\n",
    "    correct_solution_found = False\n",
    "    for idx, output in enumerate(predicted_outputs[task_id]):\n",
    "        if output is None:\n",
    "            continue\n",
    "        if correct_grids_score([np.array(output) for output in task_outputs], output) == 1:\n",
    "            correct_solution_found = True\n",
    "            data_augmentation_params = predictions[task_id]['data_augmentation_params'][idx]\n",
    "            text_pred = predictions[task_id]['text_predictions'][idx]\n",
    "            print(data_augmentation_params)\n",
    "            augmented_task = apply_data_augmentation(task, **data_augmentation_params) if data_augmentation_params is not None else task\n",
    "            plot_task(augmented_task); plt.show()\n",
    "            display(Markdown(text_pred + '\\n\\n---\\n\\n'))\n",
    "            break\n",
    "    if not correct_solution_found:\n",
    "        raise ValueError(\"Could not find correct solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998b948",
   "metadata": {},
   "source": [
    "I'm impressed by the tasks that the model is able to solve. The reasoning is correct. This is a powerful model to experiment with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6428ab4",
   "metadata": {},
   "source": [
    "## Create dataset for training with hindsight relabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f98cd2",
   "metadata": {},
   "source": [
    "Using the predictions I'm going to create a dataset to fine-tune the BARC model on the evaluation tasks.\n",
    "\n",
    "What do I need to generate the training data?\n",
    "\n",
    "1. The task after applying data augmentation (if used)\n",
    "2. The code predicted by the model\n",
    "3. The output after applying the code\n",
    "4. The output after reverting the data augmentation (so I can cluster identical solutions)\n",
    "\n",
    "So it seems that I need to modify slightly the function that runs the code to return all that additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f691f",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hindsight_relabeled_tasks(predictions: dict[str, list[str]], log_errors: bool = True):\n",
    "    # Flatten all predictions into a work list\n",
    "    work = [\n",
    "        (tid, i, text_pred, get_task(tid), prediction_data['data_augmentation_params'][i])\n",
    "        for tid, prediction_data in predictions.items()\n",
    "        for i, text_pred in enumerate(prediction_data['text_predictions'])\n",
    "    ]\n",
    "    work.sort(key=lambda x: (x[1], x[0]))\n",
    "    n_jobs = -1  # all cores; set to an int to cap\n",
    "    with tqdm_joblib(total=len(work), desc=\"Executing predictions\", unit=\"pred\", smoothing=0):\n",
    "        results = Parallel(\n",
    "            n_jobs=n_jobs,\n",
    "            backend=\"loky\",\n",
    "            prefer=\"processes\",\n",
    "            batch_size=\"auto\",\n",
    "        )(delayed(_create_hindsight_relabeled_task)(*args) for args in work)\n",
    "\n",
    "    # Rebuild per-task outputs, preserving your original behavior (code appended even on exec error)\n",
    "    hindsight_relabeled_tasks = {tid: dict() for tid in predictions}\n",
    "    for task_id, i, hr_task, err in results:\n",
    "        if hr_task is not None:\n",
    "            key = hr_task['fingerprint']\n",
    "            if key in hindsight_relabeled_tasks[task_id]:\n",
    "                if len(hr_task['code']) < len(hindsight_relabeled_tasks[task_id][key]['code']):\n",
    "                    # Keep the shorter code\n",
    "                    hindsight_relabeled_tasks[task_id][key] = hr_task\n",
    "            else:\n",
    "                hindsight_relabeled_tasks[task_id][key] = hr_task\n",
    "        if err and log_errors:\n",
    "            logging.error(f\"Error executing code for task {task_id}, response {i}: {err}\")\n",
    "\n",
    "    return hindsight_relabeled_tasks\n",
    "\n",
    "\n",
    "def _create_hindsight_relabeled_task(task_id, i, text_prediction, task, data_augmentation_params=None):\n",
    "    code = parse_python_code(text_prediction)\n",
    "    if not code:\n",
    "        return (task_id, i, None, \"parse_failed\")\n",
    "    try:\n",
    "        input_grids = [sample['input'] for sample in task['train']] + [sample['input'] for sample in task['test']]\n",
    "        if data_augmentation_params is not None:\n",
    "            # Apply data augmentation to the input grids\n",
    "            input_grids = apply_data_augmentation(input_grids, **data_augmentation_params)\n",
    "        outs = safe_code_execution(\n",
    "            add_additional_imports(curate_python_code(code)),\n",
    "            input_grids,\n",
    "            func_name=\"transform\",\n",
    "        )\n",
    "        outs = validate_outputs(outs)\n",
    "        if data_augmentation_params is not None:\n",
    "            original_outs = revert_data_augmentation(outs, **data_augmentation_params)\n",
    "        else:\n",
    "            original_outs = outs\n",
    "        pixel_scores = [pixel_similarity_score(output, pred) for output, pred in zip(\n",
    "            [sample['output'] for sample in task['train']] + [sample['output'] for sample in task['test']],\n",
    "            original_outs)]\n",
    "        hr_task = {\n",
    "            'text_prediction': text_prediction,\n",
    "            'code': code,\n",
    "            'train': [{'input': inp, 'output': out} for inp, out in zip(input_grids, outs[:len(task['train'])])],\n",
    "            'test': [{'input': inp, 'output': out} for inp, out in zip(input_grids[len(task['train']):], outs[len(task['train']):])],\n",
    "            'fingerprint': fingerprint(original_outs),\n",
    "            'is_correct_solution': correct_grids_score(\n",
    "                [sample['output'] for sample in task['train']] + [sample['output'] for sample in task['test']],\n",
    "                original_outs) == 1,\n",
    "            'mean_pixel_score': float(np.mean(pixel_scores)),\n",
    "            'correct_grids_ratio': float(np.mean(np.array(pixel_scores) == 1)),\n",
    "        }\n",
    "        return (task_id, i, hr_task, None)\n",
    "    except Exception as e:\n",
    "        return (task_id, i, None, f\"{type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d239fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_task_and_hindsight_variations(task_id, hindsight_relabeled_tasks):\n",
    "    task = get_task(task_id)\n",
    "    print(f\"Original task {task_id}:\")\n",
    "    plot_task(task)\n",
    "    plt.show()\n",
    "    display(Markdown('---'))\n",
    "    if task_id in hindsight_relabeled_tasks:\n",
    "        for i, (key, hr_task) in enumerate(hindsight_relabeled_tasks[task_id].items()):\n",
    "            print(f\"Hindsight relabeled task variation {i+1} (fingerprint: {key}):\")\n",
    "            if hr_task['is_correct_solution']:\n",
    "                print(\"This variation is a correct solution.\")\n",
    "            plot_task(hr_task)\n",
    "            plt.show()\n",
    "            display_python_code(hr_task['code'])\n",
    "            display(Markdown('---'))\n",
    "    else:\n",
    "        print(\"No hindsight relabeled tasks found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8469e5",
   "metadata": {},
   "source": [
    "### Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18612126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-23_no-data-augmentation/evaluation_*.json')\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/evaluation_*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-23_no-data-augmentation/evaluation_*.json')\n",
    "\n",
    "\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindsight_relabeled_tasks = create_hindsight_relabeled_tasks(predictions, log_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_original_task_and_hindsight_variations('195ba7dc', hindsight_relabeled_tasks) #00576224, 009d5c81, 00dbd492, '195ba7dc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\")\n",
    "grid_encoder = create_grid_encoder('ColorNameEncoder()')\n",
    "\n",
    "training_texts = dict()\n",
    "for task_id in tqdm(hindsight_relabeled_tasks):\n",
    "    for hr_task in hindsight_relabeled_tasks[task_id].values():\n",
    "        # if not hr_task['is_correct_solution']:\n",
    "        #     continue\n",
    "        text = create_prompt_from_task(hr_task, grid_encoder, tokenizer)\n",
    "        text += hr_task['text_prediction'] + tokenizer.eos_token\n",
    "        if task_id not in training_texts:\n",
    "            training_texts[task_id] = []\n",
    "        training_texts[task_id].append(text)\n",
    "\n",
    "pretty_print_prompt(text, default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dec62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_per_task = [len(texts) for texts in training_texts.values()]\n",
    "plt.hist(prompts_per_task, bins=30)\n",
    "plt.xlabel('Number of prompts per task')\n",
    "plt.ylabel('Number of tasks')\n",
    "plt.title('Distribution of prompts per task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc97320",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = [text for texts in training_texts.values() for text in texts]\n",
    "prompt_lengths = [len(tokens) for tokens in tokenizer(all_prompts)['input_ids']]\n",
    "plt.hist(prompt_lengths, bins=50, cumulative=True, density=True)\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.title(f'Cumulative Distribution of prompt lengths for hindsight relabeled tasks (n={len(all_prompts)})')\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1077a2",
   "metadata": {},
   "source": [
    "We can train of around 90% of the data with a length of 4000 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed79d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f'/mnt/hdd0/Kaggle/arc25/data/hindsight_relabeled/2025-08-25_evaluation-{len(all_prompts)}.json'\n",
    "with open(filepath, 'w') as f:\n",
    "    json.dump(training_texts, f, indent=2)\n",
    "print(f'Dataset size: {os.path.getsize(filepath) / (1024 * 1024):.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4464be4e",
   "metadata": {},
   "source": [
    "I could create a dataset with correct solutions and another without it. That would allow me to see the impact of training on correct solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1513a",
   "metadata": {},
   "source": [
    "### Create filtered dataset (with smaller number of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/evaluation_*.json')\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-23_no-data-augmentation/evaluation_*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "hindsight_relabeled_tasks = create_hindsight_relabeled_tasks(predictions, log_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c70ed1",
   "metadata": {},
   "source": [
    "Let's study the metrics distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39010ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'mean_pixel_score'\n",
    "bins = np.linspace(0, 1, 50)\n",
    "solved_tasks_distribution, unsolved_tasks_distribution = [], []\n",
    "for task_id in tqdm(hindsight_relabeled_tasks):\n",
    "    tasks = list(hindsight_relabeled_tasks[task_id].values())\n",
    "    is_solved = any(hr_task['is_correct_solution'] for hr_task in tasks)\n",
    "    scores = [hr_task[metric] for hr_task in tasks]\n",
    "    if is_solved:\n",
    "        solved_tasks_distribution.extend(scores)\n",
    "    else:\n",
    "        unsolved_tasks_distribution.extend(scores)\n",
    "plt.hist(solved_tasks_distribution, bins=bins, alpha=0.5, density=True, color='blue', label='solved tasks')\n",
    "plt.hist(unsolved_tasks_distribution, bins=bins, alpha=0.5, density=True, color='red', label='unsolved tasks')\n",
    "plt.legend()\n",
    "plt.title(f'Distribution of {metric} for solved and unsolved tasks')\n",
    "plt.xlabel(metric);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'correct_grids_ratio'\n",
    "bins = np.linspace(0, 1, 10)\n",
    "solved_tasks_distribution, unsolved_tasks_distribution = [], []\n",
    "for task_id in tqdm(hindsight_relabeled_tasks):\n",
    "    tasks = list(hindsight_relabeled_tasks[task_id].values())\n",
    "    is_solved = any(hr_task['is_correct_solution'] for hr_task in tasks)\n",
    "    scores = [hr_task[metric] for hr_task in tasks]\n",
    "    if is_solved:\n",
    "        solved_tasks_distribution.extend(scores)\n",
    "    else:\n",
    "        unsolved_tasks_distribution.extend(scores)\n",
    "plt.hist(solved_tasks_distribution, bins=bins, alpha=0.5, density=True, color='blue', label='solved tasks', log=True)\n",
    "plt.hist(unsolved_tasks_distribution, bins=bins, alpha=0.5, density=True, color='red', label='unsolved tasks', log=True)\n",
    "plt.legend()\n",
    "plt.title(f'Distribution of {metric} for solved and unsolved tasks')\n",
    "plt.xlabel(metric);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c651c8",
   "metadata": {},
   "source": [
    "correct_grids_ratio seems to be a much better way to select samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15465d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\")\n",
    "grid_encoder = create_grid_encoder('ColorNameEncoder()')\n",
    "\n",
    "n_samples_per_task = 8\n",
    "\n",
    "training_texts = dict()\n",
    "mean_scores = []\n",
    "for task_id in tqdm(hindsight_relabeled_tasks):\n",
    "    tasks = list(hindsight_relabeled_tasks[task_id].values())\n",
    "    tasks.sort(key=lambda x: (-x['correct_grids_ratio'], -x['mean_pixel_score'], len(x['code'])))\n",
    "    tasks = tasks[:n_samples_per_task]\n",
    "    mean_scores.append(np.mean([task['correct_grids_ratio'] for task in tasks]))\n",
    "    for task in tasks:\n",
    "        text = create_prompt_from_task(task, grid_encoder, tokenizer)\n",
    "        text += task['text_prediction'] + tokenizer.eos_token\n",
    "        if task_id not in training_texts:\n",
    "            training_texts[task_id] = []\n",
    "        training_texts[task_id].append(text)\n",
    "\n",
    "plt.hist(mean_scores, bins=30, log=True)\n",
    "plt.grid(which='both', axis='y')\n",
    "plt.title(f'Mean correct grids ratio of top {n_samples_per_task} variations')\n",
    "plt.xlabel('Mean correct grids ratio')\n",
    "plt.ylabel('Number of tasks');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfdb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_per_task = [len(texts) for texts in training_texts.values()]\n",
    "plt.hist(prompts_per_task, bins=30)\n",
    "plt.xlabel('Number of prompts per task')\n",
    "plt.ylabel('Number of tasks')\n",
    "plt.title('Distribution of prompts per task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03957a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prompts = [text for texts in training_texts.values() for text in texts]\n",
    "prompt_lengths = [len(tokens) for tokens in tokenizer(all_prompts)['input_ids']]\n",
    "plt.hist(prompt_lengths, bins=50, cumulative=True, density=True)\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.title(f'Cumulative Distribution of prompt lengths for hindsight relabeled tasks (n={len(all_prompts)})')\n",
    "plt.ylim(0, 1)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737fd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f'/mnt/hdd0/Kaggle/arc25/data/hindsight_relabeled/2025-08-25_evaluation-selected{n_samples_per_task}_no-data-augmentation.json'\n",
    "with open(filepath, 'w') as f:\n",
    "    json.dump(training_texts, f, indent=2)\n",
    "print(f'Dataset size: {os.path.getsize(filepath) / (1024 * 1024):.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa04f90",
   "metadata": {},
   "source": [
    "## Evaluate finetuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28d0aa",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1,\n",
    "                            enable_lora=True, max_model_len=16000, max_lora_rank=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4bec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = '/mnt/hdd0/MEGA/TEMP/2025-08-26-lora-rank/2xA6000--1000steps-8192msl-1e-4lr-lora32/checkpoint-1000'\n",
    "lora_request = LoRARequest('LoRA', 1, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_task_ids = {\n",
    "    'training': list(training_challenges.keys()),\n",
    "    'evaluation': list(evaluation_challenges.keys()),\n",
    "    'evaluation-2025': list(evaluation_challenges_2025.keys())\n",
    "}\n",
    "\n",
    "experiment_name = '2025-08-27_first-finetuning-steps'\n",
    "dataset = 'evaluation'\n",
    "task_ids = dataset_to_task_ids[dataset]\n",
    "grid_encoder = create_grid_encoder('ColorNameEncoder()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = create_prompt_from_task(get_task(task_ids[0]), grid_encoder, tokenizer)\n",
    "pretty_print_prompt(text, default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808872da",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_data_augmentation = True\n",
    "for n in [8]:\n",
    "    sampling_params = SamplingParams(n=n, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "\n",
    "    prompts, data_augmentation_params = [], []\n",
    "    for task_id in task_ids:\n",
    "        task = get_task(task_id)\n",
    "        if use_data_augmentation:\n",
    "            params = get_random_data_augmentation_params()\n",
    "            task = apply_data_augmentation(task, **params)\n",
    "        else:\n",
    "            params = None\n",
    "        data_augmentation_params.append(params)\n",
    "        prompt = create_prompt_from_task(\n",
    "            task, grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    t0 = time.time()\n",
    "    text_predictions = llm.generate(prompts, sampling_params, lora_request=lora_request)\n",
    "    total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "    inference_time = time.time() - t0\n",
    "    print(f\"Total tokens generated: {total_tokens}\")\n",
    "    print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "    print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "    print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "    print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "\n",
    "    predictions = dict()\n",
    "    for task_id, output, params in zip(task_ids, text_predictions, data_augmentation_params):\n",
    "        predictions[task_id] = {\n",
    "            'text_predictions': [output.text for output in output.outputs],\n",
    "            'data_augmentation_params': params,\n",
    "        }\n",
    "\n",
    "    output_filepath = f'/mnt/hdd0/Kaggle/arc25/predictions/{experiment_name}/{dataset}_{sampling_params.n}preds_{get_timestamp()}_predictions.json'\n",
    "    os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        json.dump(predictions, f, indent=2)\n",
    "    print(f\"Predictions saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a29825c",
   "metadata": {},
   "source": [
    "Compare inference speed with and without LoRA\n",
    "\n",
    "```\n",
    "evaluation without LoRA\n",
    "8 preds, Average time per task: 4.17 seconds\n",
    "\n",
    "evaluation with LoRA rank 32\n",
    "Average time per task: 4.87 seconds, 5.08, 4.59, 5.06\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b23a87",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27130aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-27_first-finetuning-steps/*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/MEGA/TEMP/predictions/2025-08-27-training-steps/2xH100-8000steps-8192msl-1e-5lr-full-finetuning-continue/evaluation/*.json')\n",
    "predictions = load_all_predictions('/mnt/hdd0/MEGA/TEMP/predictions/2025-08-29-smaller-datasets/2xA6000-1000steps-8192msl-1e-4lr-lora32*/evaluation/*.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/MEGA/TEMP/predictions/2025-08-29-smaller-datasets-no-data-augmentation/2xA6000-1000steps-8192msl-1e-4lr-lora32/evaluation/*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65818cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_code_from_predictions(predictions)\n",
    "df = compute_search_metrics(results)\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc37da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/arc25/code_execution/evaluation_2025-08-29-smaller-datasets-1000steps_512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "n_preds_range = 2**np.arange(0, int(np.log2(n_preds)) + 2)\n",
    "fail_prob = 1 - df['pass_rate'].values[:-1]\n",
    "for n in n_preds_range:\n",
    "    scores.append(float(np.mean(1 - fail_prob**n)))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(n_preds_range, scores, marker='o')\n",
    "plt.xscale('log', base=2)\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('pass@n vs Number of Predictions')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "dict(evaluation_data_augmentation=(n_preds_range.tolist(), scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d330ca",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae94c90",
   "metadata": {},
   "source": [
    "#### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32556f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "   'baseline': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.018665553329123646,\n",
    "   0.02879110915200162,\n",
    "   0.04224158293230386,\n",
    "   0.059708013904966675,\n",
    "   0.08223659237317406,\n",
    "   0.10940176527483192,\n",
    "   0.1384685993616617,\n",
    "   0.1664683605627952,\n",
    "   0.1908837471080634,\n",
    "   0.2094882044080468]),\n",
    "#    'baseline_6064': ([1,\n",
    "#    2,\n",
    "#    4,\n",
    "#    8,\n",
    "#    16,\n",
    "#    32,\n",
    "#    64,\n",
    "#    128,\n",
    "#    256,\n",
    "#    512,\n",
    "#    1024,\n",
    "#    2048,\n",
    "#    4096,\n",
    "#    8192],\n",
    "#   [0.020675799545352484,\n",
    "#    0.031800090738608865,\n",
    "#    0.04635819085811816,\n",
    "#    0.06506021688568361,\n",
    "#    0.08888375747790557,\n",
    "#    0.11740666375181305,\n",
    "#    0.14840423707784808,\n",
    "#    0.1798639618765866,\n",
    "#    0.21174751560392843,\n",
    "#    0.24540332456877859,\n",
    "#    0.27906954197611594,\n",
    "#    0.30882907544705135,\n",
    "#    0.3307049086550017,\n",
    "#    0.3426571213621735]),\n",
    "   '2025-08-29-smaller-datasets/2xA6000-20steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.023606329073264366,\n",
    "   0.0352476921085585,\n",
    "   0.050501333159261946,\n",
    "   0.07003085421915217,\n",
    "   0.09419090612849824,\n",
    "   0.12113183968221045,\n",
    "   0.14803950592285675,\n",
    "   0.1739829401977137,\n",
    "   0.19875668196923418,\n",
    "   0.21980857854481178,\n",
    "   0.2328619884758857]),\n",
    "   '2025-08-29-smaller-datasets/2xA6000-50steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.022661425810380838,\n",
    "   0.03481883641545316,\n",
    "   0.05094183584628108,\n",
    "   0.0716586140699154,\n",
    "   0.09738483594465794,\n",
    "   0.12678086459475907,\n",
    "   0.15656512697994462,\n",
    "   0.18444358302270544,\n",
    "   0.2101792723053778,\n",
    "   0.232086348106364,\n",
    "   0.24541442209449776]),\n",
    "   '2025-08-29-smaller-datasets/2xA6000-100steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.02378096490088292,\n",
    "   0.03754834234524835,\n",
    "   0.055772371764274686,\n",
    "   0.07811478359718953,\n",
    "   0.10437513965709139,\n",
    "   0.13375836535645524,\n",
    "   0.16418840621709735,\n",
    "   0.19274152853230261,\n",
    "   0.21843439192234648,\n",
    "   0.23989066332449474,\n",
    "   0.25305110403574377]),\n",
    "   '2025-08-29-smaller-datasets/2xA6000-200steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.026407894401354506,\n",
    "   0.04183597142086427,\n",
    "   0.06136782516904767,\n",
    "   0.08414126336106854,\n",
    "   0.10926185586165676,\n",
    "   0.13571473592796823,\n",
    "   0.1629771841582275,\n",
    "   0.18954746817746748,\n",
    "   0.21358035273903242,\n",
    "   0.23388701626531558,\n",
    "   0.24738480183743544]),\n",
    "   '2025-08-29-smaller-datasets/2xA6000-400steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.03117564177009315,\n",
    "   0.04998497655109415,\n",
    "   0.07385574233762617,\n",
    "   0.10109329793913328,\n",
    "   0.13032582762064335,\n",
    "   0.16063589178430518,\n",
    "   0.1903189163633815,\n",
    "   0.21801216832181225,\n",
    "   0.24286374668830452,\n",
    "   0.26271536089726744,\n",
    "   0.2752622875910397]),\n",
    "   '2025-08-29-smaller-datasets/2xA6000-1000steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.03393188286719861,\n",
    "   0.05618490491219927,\n",
    "   0.08531506508418366,\n",
    "   0.11875323375283692,\n",
    "   0.1530693753851221,\n",
    "   0.1857451548706929,\n",
    "   0.2151749652079954,\n",
    "   0.24097422198127702,\n",
    "   0.26278415057116694,\n",
    "   0.27943911006045935,\n",
    "   0.2891937503094295]),\n",
    "   '2025-08-29-smaller-datasets/2xA6000-2000steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.03598298972600957,\n",
    "   0.05938001927686663,\n",
    "   0.0899627911892049,\n",
    "   0.12510289665136845,\n",
    "   0.16063207974081734,\n",
    "   0.19122662010398997,\n",
    "   0.21507110907071642,\n",
    "   0.23376178443142487,\n",
    "   0.24870419044859723,\n",
    "   0.2603286642701881,\n",
    "   0.2674460627031699]),\n",
    "#    'full-finetuning': ([1,\n",
    "#    2,\n",
    "#    4,\n",
    "#    8,\n",
    "#    16,\n",
    "#    32,\n",
    "#    64,\n",
    "#    128,\n",
    "#    256,\n",
    "#    512,\n",
    "#    1024],\n",
    "#   [0.020943503218614056,\n",
    "#    0.03539542169542603,\n",
    "#    0.05516301589810301,\n",
    "#    0.07978942217012977,\n",
    "#    0.10925191308893745,\n",
    "#    0.14241075863007166,\n",
    "#    0.17641764413891386,\n",
    "#    0.20827495116804257,\n",
    "#    0.23495958143629544,\n",
    "#    0.2549056629232699,\n",
    "#    0.26631500511324085])\n",
    "}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "# set a viridis colormap with as many colors as keys\n",
    "cmap = plt.get_cmap('viridis', len(keys))\n",
    "colors = [cmap(i) for i in range(len(keys))]\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    label = key if 'steps' not in key else key.split('steps-')[0].split('-')[-1] + ' finetuning steps'\n",
    "    plt.plot(n_preds_range[:9], scores[:9], marker='o', label=label, color=colors[keys.index(key)])\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-1')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f8057c",
   "metadata": {},
   "source": [
    "#### No data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "   'baseline': ([1, 2, 4, 8, 16, 32, 64, 128, 256, 512],\n",
    "  [0.018653395632067863,\n",
    "   0.028123807513897266,\n",
    "   0.040470801097098115,\n",
    "   0.05604907442616278,\n",
    "   0.07517185395071181,\n",
    "   0.09714182632584087,\n",
    "   0.12045095653955698,\n",
    "   0.14292755780979274,\n",
    "   0.16171510346135057,\n",
    "   0.1753448200072779],),\n",
    "   '2025-08-29-smaller-datasets-no-data-augmentation/2xA6000-100steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.024755481392362774,\n",
    "   0.0367296855115465,\n",
    "   0.051931907728330916,\n",
    "   0.071667129777274,\n",
    "   0.09634285881126135,\n",
    "   0.12451250069057292,\n",
    "   0.15435278313874515,\n",
    "   0.18383258470644478,\n",
    "   0.20928428503505228,\n",
    "   0.22709159441068621,\n",
    "   0.23668065815082812]),\n",
    "   '2025-08-29-smaller-datasets-no-data-augmentation/2xA6000-200steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.025170263188187957,\n",
    "   0.03939721859599572,\n",
    "   0.05702557841718987,\n",
    "   0.07870673666868633,\n",
    "   0.10524887049008552,\n",
    "   0.13491733441516116,\n",
    "   0.16497592064853683,\n",
    "   0.19372497684150716,\n",
    "   0.2195907575602184,\n",
    "   0.2408218665804371,\n",
    "   0.2546336295672011]),\n",
    "   '2025-08-29-smaller-datasets-no-data-augmentation/2xA6000-400steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.028915353428317324,\n",
    "   0.04635151032425139,\n",
    "   0.06823145086396849,\n",
    "   0.09350457920187943,\n",
    "   0.12129978397972246,\n",
    "   0.1502646416692249,\n",
    "   0.17806324360439388,\n",
    "   0.20372412789366146,\n",
    "   0.22650050489856888,\n",
    "   0.24383315592615584,\n",
    "   0.25380920871243623]),\n",
    "   '2025-08-29-smaller-datasets-no-data-augmentation/2xA6000-1000steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.06464566983287443,\n",
    "   0.0960075917248096,\n",
    "   0.1273494403720554,\n",
    "   0.15302477552865526,\n",
    "   0.17240088138783308,\n",
    "   0.1877761751774143,\n",
    "   0.2015373151125997,\n",
    "   0.21536516950023035,\n",
    "   0.22903181236179837,\n",
    "   0.23992157059581673,\n",
    "   0.24569130991579102]),\n",
    "   '2025-08-29-smaller-datasets-no-data-augmentation/2xA6000-2000steps-8192msl-1e-4lr-lora32': ([1,\n",
    "   2,\n",
    "   4,\n",
    "   8,\n",
    "   16,\n",
    "   32,\n",
    "   64,\n",
    "   128,\n",
    "   256,\n",
    "   512,\n",
    "   1024],\n",
    "  [0.08198481692553986,\n",
    "   0.11571581682183918,\n",
    "   0.14643463821422242,\n",
    "   0.16763511016356147,\n",
    "   0.17986060840918847,\n",
    "   0.18851721505402538,\n",
    "   0.196651421697631,\n",
    "   0.2049564001993034,\n",
    "   0.21316720212693696,\n",
    "   0.22049393241597864,\n",
    "   0.22546817719225815])\n",
    "}\n",
    "\n",
    "keys = list(metrics.keys())\n",
    "plt.figure(figsize=(10, 5))\n",
    "# set a viridis colormap with as many colors as keys\n",
    "cmap = plt.get_cmap('viridis', len(keys))\n",
    "colors = [cmap(i) for i in range(len(keys))]\n",
    "for key, (n_preds_range, scores) in metrics.items():\n",
    "    label = key if 'steps' not in key else key.split('steps-')[0].split('-')[-1] + ' finetuning steps'\n",
    "    plt.plot(n_preds_range[:9], scores[:9], marker='o', label=label, color=colors[keys.index(key)])\n",
    "plt.xscale('log', base=2)\n",
    "# plt.grid(which='both', axis='both')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of predictions')\n",
    "plt.ylabel('pass@n')\n",
    "plt.title('Evaluation ARC-AGI-1')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1edde",
   "metadata": {},
   "source": [
    "### Check how significative are the improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ec9a4",
   "metadata": {},
   "source": [
    "I want to understand if the improvements get with test-time training are significative. The criticism is that I have made a total of 1024 predictions with those models instead of 1024.\n",
    "Another way to skip the criticism would be to train without the totally correct tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_df = pd.read_csv('/mnt/hdd0/Kaggle/arc25/code_execution/evaluation_2025-08-29-smaller-datasets-1000steps_512.csv'\n",
    "finetuned_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb9a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_df = pd.read_csv('/mnt/hdd0/Kaggle/arc25/code_execution/evaluation_2025-08-29-smaller-datasets-1000steps_512.csv', index_col=0)\n",
    "seed_df = pd.read_csv('/mnt/hdd0/Kaggle/arc25/code_execution/evaluation_480.csv', index_col=0)\n",
    "baseline_df = pd.read_csv('/mnt/hdd0/Kaggle/arc25/code_execution/evaluation_6064.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49227f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "solved_task_ids = finetuned_df[finetuned_df['pass_rate'] > 0].index.values[:-1]\n",
    "print(f\"Number of solved tasks: {len(solved_task_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfdf40",
   "metadata": {},
   "source": [
    "Let's check how many of those were already solved in the seed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a310ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(seed_df.loc[solved_task_ids]['pass_rate'] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4097425",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "88/400, 88/117"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fe9ca",
   "metadata": {},
   "source": [
    "So 75% of the tasks were already solved. 22% score and the baseline solved 22.5% so almost all of the tasks solved in the seed data were solved by the model. Let's focus on the other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e75a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_solved_task_ids = [task_id for task_id in solved_task_ids if seed_df.loc[task_id]['pass_rate'] == 0]\n",
    "already_solved_task_ids = [task_id for task_id in solved_task_ids if seed_df.loc[task_id]['pass_rate'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f62218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bug in the calculation of the pass rate\n",
    "baseline_df['fixed_pass_rate'] = baseline_df['pass_rate'] * baseline_df['valid outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = sorted(1/baseline_df.loc[newly_solved_task_ids]['fixed_pass_rate'].values)\n",
    "values = np.clip(values, None, 6064*2)\n",
    "plt.hist(values, bins=np.logspace(np.log10(2**7), np.log10(6064*2), 30))\n",
    "plt.xscale('log', base=10)\n",
    "# draw a vertical line at 1024\n",
    "plt.axvline(1024, color='red', linestyle='--', label='1024')\n",
    "plt.axvline(6064, color='green', linestyle='--', label='6064')\n",
    "plt.legend(loc=0)\n",
    "plt.grid(axis='x', which='both')\n",
    "plt.xlabel('Estimated number of predictions to solve task with baseline model')\n",
    "plt.ylabel('Number of tasks')\n",
    "plt.title('Distribution of estimated predictions to solve newly solved tasks');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff31f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(values > 7000).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf692f96",
   "metadata": {},
   "source": [
    "The fine-tuned model using TTT was able to solve tasks that on average require more than 1024 predictions to be solved (72% of the newly solved tasks). In fact 17% of the tasks were not solved after doing more than 6000 predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d61828",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70465017",
   "metadata": {},
   "source": [
    "### LoRA saving uses a lot of space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35042701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "sys.path.append('../scripts')\n",
    "from finetuning import get_model, get_tokenizer, get_lora_model, Accelerator\n",
    "\n",
    "\n",
    "model_path = '/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B'\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = get_model(model_path, torch_dtype=\"bfloat16\",\n",
    "                      use_4bit_quantization=False, device_map='None',\n",
    "                      use_gradient_checkpointing=False)\n",
    "tokenizer = get_tokenizer(model_path, model, 'ColorNameEncoder()')\n",
    "model = get_lora_model(model, None, 32, False, False, 'default')\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "shutil.rmtree('/mnt/hdd0/Kaggle/arc25/models/debug_PEFT_saving')\n",
    "model.save_pretrained('/mnt/hdd0/Kaggle/arc25/models/debug_PEFT_saving', safe_serialization=True) #4.3GB\n",
    "print(os.path.getsize('/mnt/hdd0/Kaggle/arc25/models/debug_PEFT_saving/adapter_model.safetensors') / (1024*1024), 'MB') #4112 MB for rank 32, 4034MB for rank 8, 2108 when not using 4 bit quantization and rank 32\n",
    "shutil.rmtree('/mnt/hdd0/Kaggle/arc25/models/debug_PEFT_saving')\n",
    "model.save_pretrained('/mnt/hdd0/Kaggle/arc25/models/debug_PEFT_saving', safe_serialization=True, save_embedding_layers=False)\n",
    "print(os.path.getsize('/mnt/hdd0/Kaggle/arc25/models/debug_PEFT_saving/adapter_model.safetensors') / (1024*1024), 'MB') #104 MB for rank 32, 26MB for rank 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210ab47",
   "metadata": {},
   "source": [
    "### Collator with eos_token=pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import  DataCollatorForCompletionOnlyLM\n",
    "\n",
    "model_path = '/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(tokenizer.special_tokens_map)\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "            tokenizer=tokenizer,\n",
    "            # instruction_template='<|start_header_id|>user<|end_header_id|>',\n",
    "            response_template='<|start_header_id|>assistant<|end_header_id|>',\n",
    ")\n",
    "\n",
    "text = '<|begin_of_text|><|start_header_id|>assistant<|end_header_id|> Hello, world! <|eot_id|><|eot_id|>'\n",
    "\n",
    "# print(tokenizer(text))\n",
    "print(data_collator([tokenizer(text)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999072a",
   "metadata": {},
   "source": [
    "Even if I add multiple eos_tokens, they are all masked. So that is not a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.special_tokens_map)\n",
    "for word in tokenizer.get_vocab():\n",
    "    if 'â–<|' in word or '|>' in word and 'reserved_special' not in word:\n",
    "        print(word, tokenizer.convert_tokens_to_ids(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb099c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<|begin_of_text|> <|start_header_id|>assistant<|end_header_id|> Hello, world! <|eot_id|><|eot_id|>'\n",
    "\n",
    "# print(tokenizer(text))\n",
    "print(data_collator([tokenizer(text)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cd0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = '<|finetune_right_pad_id|>'\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "            tokenizer=tokenizer,\n",
    "            # instruction_template='<|start_header_id|>user<|end_header_id|>',\n",
    "            response_template='<|start_header_id|>assistant<|end_header_id|>',\n",
    ")\n",
    "\n",
    "text = '<|begin_of_text|><|start_header_id|>assistant<|end_header_id|> Hello, world! <|eot_id|><|eot_id|>'\n",
    "\n",
    "# print(tokenizer(text))\n",
    "print(data_collator([tokenizer(text)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea47052",
   "metadata": {},
   "source": [
    "Is it valid for Llama models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/gbarbadillo/models/Llama-3.1-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(tokenizer.special_tokens_map)\n",
    "for word in tokenizer.get_vocab():\n",
    "    if 'â–<|' in word or '|>' in word and 'reserved_special' not in word:\n",
    "        print(word, tokenizer.convert_tokens_to_ids(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/gbarbadillo/models/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "print(tokenizer.special_tokens_map)\n",
    "for word in tokenizer.get_vocab():\n",
    "    if 'â–<|' in word or '|>' in word and 'reserved_special' not in word:\n",
    "        print(word, tokenizer.convert_tokens_to_ids(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46f5c9",
   "metadata": {},
   "source": [
    "Both models have the `<|finetune_right_pad_id|>` token, so I can make a general rule for llama 3.1 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f48e4",
   "metadata": {},
   "source": [
    "### VLLM LoRA compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087cc41",
   "metadata": {},
   "source": [
    "https://docs.vllm.ai/en/v0.9.1/features/lora.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d32631f",
   "metadata": {},
   "source": [
    "#### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c124c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(\n",
    "    model_path, use_4bit_quantization=False,\n",
    "    tensor_parallel_size=1, enable_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc50e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": 'You are a helpful assistant that always ends your responses with \"Have a great day!\"'},\n",
    "    {\"role\": \"user\", \"content\": 'Hi, how are you today?'},]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,  tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=1024)\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "for idx, output in enumerate(outputs[0].outputs):\n",
    "    print(f'{idx}. {output.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a17efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=1024)\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "for idx, output in enumerate(outputs[0].outputs):\n",
    "    print(f'{idx}. {output.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f940add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-lora-compatibility/qLoRA_8/checkpoint-1'\n",
    "lora_request = LoRARequest(\"plain-lora\", 1, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate([prompt], sampling_params, lora_request=lora_request)\n",
    "for idx, output in enumerate(outputs[0].outputs):\n",
    "    print(f'{idx}. {output.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4666ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-lora-compatibility/qLoRA_8_dora_rslora/checkpoint-1'\n",
    "lora_request = LoRARequest(\"dora\", 2, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8613a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: vLLM does not yet support DoRA.\n",
    "outputs = llm.generate([prompt], sampling_params, lora_request=lora_request)\n",
    "for idx, output in enumerate(outputs[0].outputs):\n",
    "    print(f'{idx}. {output.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-lora-compatibility/qLoRA_8_rslora/checkpoint-1'\n",
    "lora_request = LoRARequest(\"rslora\", 3, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa51b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate([prompt], sampling_params, lora_request=lora_request)\n",
    "for idx, output in enumerate(outputs[0].outputs):\n",
    "    print(f'{idx}. {output.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc80f85",
   "metadata": {},
   "source": [
    "VLLM supports LoRA and RSLoRA, it does not support DoRA. Moreover I can give models on the fly, it seems that the first time is slower but otherwise speed looks to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d0db5",
   "metadata": {},
   "source": [
    "#### Speed tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(\n",
    "    model_path, use_4bit_quantization=False,\n",
    "    tensor_parallel_size=1, enable_lora=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "adapter_paths = {\n",
    "    'LoRA': '/mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-lora-compatibility/qLoRA_8/checkpoint-1',\n",
    "    'RSLoRA': '/mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-lora-compatibility/qLoRA_8_rslora/checkpoint-1',\n",
    "}\n",
    "loras = {name: LoRARequest(name, idx, path) for idx, (name, path) in enumerate(adapter_paths.items(), 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bcb225",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": 'You are a helpful assistant that always ends your responses with \"Have a great day!\"'},\n",
    "    {\"role\": \"user\", \"content\": 'Hi, how are you today?'},]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,  tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=800, temperature=1.0, top_p=0.95, max_tokens=10)\n",
    "t0 = time.time()\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "generation_time = time.time() - t0\n",
    "output_tokens = sum(len(output.token_ids) for output in outputs[0].outputs)\n",
    "print(f\"Base model: {output_tokens} tokens generated in {generation_time:.2f} seconds ({output_tokens / generation_time:.2f} tokens/second)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in loras.keys():\n",
    "    t0 = time.time()\n",
    "    outputs = llm.generate([prompt], sampling_params, lora_request=loras[key])\n",
    "    generation_time = time.time() - t0\n",
    "    output_tokens = sum(len(output.token_ids) for output in outputs[0].outputs)\n",
    "    print(f\"{key} model: {output_tokens} tokens generated in {generation_time:.2f} seconds ({output_tokens / generation_time:.2f} tokens/second)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in loras.keys():\n",
    "    t0 = time.time()\n",
    "    outputs = llm.generate([prompt], sampling_params, lora_request=loras[key])\n",
    "    generation_time = time.time() - t0\n",
    "    output_tokens = sum(len(output.token_ids) for output in outputs[0].outputs)\n",
    "    print(f\"{key} model: {output_tokens} tokens generated in {generation_time:.2f} seconds ({output_tokens / generation_time:.2f} tokens/second)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d76953",
   "metadata": {},
   "source": [
    "```\n",
    "sampling_params = SamplingParams(n=800, temperature=1.0, top_p=0.95, max_tokens=1024)\n",
    "Base model: 32055 tokens generated in 18.17 seconds (1764.12 tokens/second)\n",
    "LoRA model: 33062 tokens generated in 28.28 seconds (1169.06 tokens/second)\n",
    "RSLoRA model: 31708 tokens generated in 19.74 seconds (1606.59 tokens/second)\n",
    "LoRA model: 32293 tokens generated in 19.14 seconds (1687.62 tokens/second)\n",
    "RSLoRA model: 32403 tokens generated in 19.16 seconds (1691.01 tokens/second)\n",
    "\n",
    "sampling_params = SamplingParams(n=800, temperature=1.0, top_p=0.95, max_tokens=10)\n",
    "Base model: 8000 tokens generated in 4.94 seconds (1618.46 tokens/second)\n",
    "LoRA model: 8000 tokens generated in 5.30 seconds (1508.31 tokens/second)\n",
    "RSLoRA model: 8000 tokens generated in 5.21 seconds (1534.20 tokens/second)\n",
    "LoRA model: 8000 tokens generated in 5.23 seconds (1530.30 tokens/second)\n",
    "RSLoRA model: 8000 tokens generated in 5.65 seconds (1415.46 tokens/second)\n",
    "\n",
    "# restart\n",
    "sampling_params = SamplingParams(n=800, temperature=1.0, top_p=0.95, max_tokens=10)\n",
    "Base model: 8000 tokens generated in 4.95 seconds (1614.81 tokens/second)\n",
    "LoRA model: 8000 tokens generated in 5.78 seconds (1384.20 tokens/second)\n",
    "RSLoRA model: 8000 tokens generated in 6.01 seconds (1330.54 tokens/second)\n",
    "LoRA model: 8000 tokens generated in 5.26 seconds (1522.17 tokens/second)\n",
    "RSLoRA model: 8000 tokens generated in 5.29 seconds (1512.97 tokens/second)\n",
    "```\n",
    "\n",
    "It seems that the first time a model is called it is slightly slower. And the LoRA model by itself is slightly slower than the base model. But manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a5f2e",
   "metadata": {},
   "source": [
    "### Increase observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a84b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-27_first-finetuning-steps/evaluation_8preds_2025_08_27_12_30_09_predictions.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "results = run_code_from_predictions(predictions)\n",
    "display(compute_search_metrics(results).iloc[-1:])\n",
    "display(error_analysis(results).iloc[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e47189",
   "metadata": {},
   "source": [
    "### Variability in evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cb16e",
   "metadata": {},
   "source": [
    "I have observed that running the evaluation multiple times changes the results. This does not have sense, so let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a838ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/mnt/hdd0/Kaggle/arc25/predictions/2025-08-27_first-finetuning-steps/evaluation_8preds_2025_08_27_12_30_09_predictions.json'\n",
    "dfs = []\n",
    "for _ in tqdm(range(2)):\n",
    "    predictions = load_all_predictions(filepath)\n",
    "    n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "    predicted_code, predicted_outputs = run_code_from_predictions(predictions, log_errors=False)\n",
    "    dfs.append(compute_search_metrics(list(predictions.keys()), predicted_code, predicted_outputs, n_preds))\n",
    "    display(dfs[-1].iloc[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [idx for idx, changed in enumerate(np.sum((dfs[0] - dfs[1]).values, axis=1) != 0) if changed]\n",
    "print(len(indices), (dfs[0] - dfs[1]).iloc[indices].index)\n",
    "(dfs[0] - dfs[1]).iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/mnt/hdd0/Kaggle/arc25/predictions/2025-08-27_first-finetuning-steps/evaluation_8preds_2025_08_27_12_30_09_predictions.json'\n",
    "predictions = load_all_predictions(filepath)\n",
    "\n",
    "keys = ['20818e16', '37d3e8b2', '4acc7107', '69889d6e', '8b28cd80', '93b4f4b3',\n",
    "       'af22c60d', 'cd3c21df', 'd4b1c2b1', 'de493100', 'e78887d1', 'e9c9d9a1']\n",
    "predictions = {key: predictions[key] for key in keys if key in predictions}\n",
    "\n",
    "\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "dfs = []\n",
    "rets = []\n",
    "for _ in tqdm(range(2)):\n",
    "    predicted_code, predicted_outputs = run_code_from_predictions(predictions, log_errors=False)\n",
    "    dfs.append(compute_search_metrics(list(predictions.keys()), predicted_code, predicted_outputs, n_preds))\n",
    "    rets.append((predicted_code, predicted_outputs))\n",
    "dfs[0] - dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e77e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted code seems to be the same\n",
    "rets[0][0] == rets[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81939653",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = '20818e16'\n",
    "pred_idx = 1\n",
    "task = get_task(task_id)\n",
    "task = apply_data_augmentation(task, **predictions[task_id]['data_augmentation_params'][pred_idx]) if predictions[task_id]['data_augmentation_params'][pred_idx] is not None else task\n",
    "outputs = [sample['output'] for sample in task['train']] + [sample['output'] for sample in task['test']]\n",
    "print(rets[0][0][task_id][pred_idx])\n",
    "for idx, output in enumerate(outputs):\n",
    "    print(f\"Output {idx}:\")\n",
    "    print(pixel_similarity_score(output, rets[0][1][task_id][pred_idx][idx]), pixel_similarity_score(output, rets[1][1][task_id][pred_idx][idx]))\n",
    "    print(rets[0][1][task_id][pred_idx][idx].shape, rets[1][1][task_id][pred_idx][idx].shape)\n",
    "    print(output)\n",
    "    # print(rets[1][1][task_id][pred_idx][idx])\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6211374",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(rets[0][1]['20818e16'][0], rets[0][1]['20818e16'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ac451",
   "metadata": {},
   "outputs": [],
   "source": [
    "rets[0][1]['20818e16'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in rets[0][1].keys():\n",
    "    for idx in range(len(rets[0][1][key])):\n",
    "        out1 = rets[0][1][key][idx]\n",
    "        out2 = rets[1][1][key][idx]\n",
    "        if not np.array_equal(out1, out2):\n",
    "            print(f\"Difference found in task {key} at prediction {idx}\")\n",
    "            # print(rets[0][0][key][idx])\n",
    "            print(len(out1), len(out2))\n",
    "            for sample_output1, sample_output2 in zip(out1, out2):\n",
    "                if not np.array_equal(sample_output1, sample_output2):\n",
    "                    print(\"Sample outputs differ:\")\n",
    "                    print(\"Sample Output 1:\", sample_output1.shape)\n",
    "                    print(sample_output1)\n",
    "                    print(\"Sample Output 2:\", sample_output2.shape)\n",
    "                    print(sample_output2)\n",
    "            print()\n",
    "            # raise ValueError(\"Outputs differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bdeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rets[0][1] == rets[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [idx for idx, changed in enumerate(np.sum((dfs[0] - dfs[1]).values, axis=1) != 0) if changed]\n",
    "(dfs[0] - dfs[1]).iloc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c12233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could allow to detect differences in valid outputs\n",
    "for key in rets[0][1].keys():\n",
    "    for idx in range(len(rets[0][1][key])):\n",
    "        out1 = rets[0][1][key][idx]\n",
    "        out2 = rets[1][1][key][idx]\n",
    "        if out1 is None and out2 is not None:\n",
    "            print(f\"Different outputs for task {key}, prediction {idx}\")\n",
    "        elif out1 is not None and out2 is None:\n",
    "            print(f\"Different outputs for task {key}, prediction {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812741e",
   "metadata": {},
   "source": [
    "### Import dsl on each code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-27_first-finetuning-steps/evaluation_8preds_2025_08_27_12_30_09_predictions.json')\n",
    "#predictions = {key: predictions[key] for key in list(predictions.keys())[:1]}\n",
    "results = run_code_from_predictions(predictions)\n",
    "display(compute_search_metrics(results).iloc[-1:])\n",
    "display(error_analysis(results).iloc[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766c35c",
   "metadata": {},
   "source": [
    "### Learn to use subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "code = \"\"\"import numpy as np\n",
    "print(np.__version__)\"\"\"\n",
    "\n",
    "proc = subprocess.Popen(\n",
    "        [sys.executable, \"-I\", \"-c\", code],\n",
    "        start_new_session=True, env=os.environ.copy(),\n",
    ")\n",
    "proc.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085b303",
   "metadata": {},
   "source": [
    "### New safe execution method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f588d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.code_execution import _safe_code_execution_subprocess as safe_code_execution\n",
    "predictions = load_all_predictions('/mnt/hdd0/MEGA/TEMP/predictions/2025-08-29-smaller-datasets/2xA6000-20steps-8192msl-1e-4lr-lora32/evaluation/8preds_2025_09_01_09_54_56_predictions.json')\n",
    "results = run_code_from_predictions(predictions)\n",
    "df = compute_search_metrics(results)\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362c780",
   "metadata": {},
   "source": [
    "```\n",
    "# exec\n",
    "12.2s\n",
    "\tn_preds\tvalid code\tvalid outputs\tunique outputs\tpixel similarity\tcorrect grids\ttrain_pass_rate\ttrain_pass@n\tpass_rate\tpass@n\n",
    "MEAN\t8.0\t1.0\t0.7575\t0.667813\t0.57695\t0.038117\t0.028202\t0.0675\t0.027429\t0.0625\n",
    "MEAN\t8.0\t1.0\t0.757188\t0.6675\t0.575372\t0.036742\t0.026952\t0.065\t0.026179\t0.06\n",
    "MEAN\t8.0\t1.0\t0.7575\t0.667813\t0.576488\t0.037784\t0.026952\t0.065\t0.026179\t0.06\n",
    "ValueError              207\n",
    "IndexError              169\n",
    "AssertionError          146\n",
    "NonDeterministicCode    141\n",
    "TimeoutException         44\n",
    "TypeError                23\n",
    "AttributeError           15\n",
    "UnboundLocalError        10\n",
    "StopIteration            10\n",
    "NameError                 4\n",
    "ZeroDivisionError         4\n",
    "UnsafeCode                2\n",
    "KeyError                  1\n",
    "Name: count, dtype: int64\n",
    "\n",
    "\n",
    "# subprocess\n",
    "53.1s\n",
    "n_preds\tvalid code\tvalid outputs\tunique outputs\tpixel similarity\tcorrect grids\ttrain_pass_rate\ttrain_pass@n\tpass_rate\tpass@n\n",
    "MEAN\t8.0\t1.0\t0.759687\t0.669375\t0.577302\t0.038509\t0.027577\t0.0675\t0.026804\t0.0625\n",
    "MEAN\t8.0\t1.0\t0.759687\t0.669375\t0.577302\t0.038509\t0.027577\t0.0675\t0.026804\t0.0625\n",
    "\n",
    "RuntimeError            526\n",
    "NonDeterministicCode    141\n",
    "ValueError               63\n",
    "TimeoutException         37\n",
    "UnsafeCode                2\n",
    "Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b47bbc",
   "metadata": {},
   "source": [
    "### Run code from predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f260f",
   "metadata": {},
   "source": [
    "#### Effect of using a big number of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65871e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "#predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/*.json')\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_8preds_2025_08_23_11_57_11_predictions.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params, n_jobs=-1)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params, n_jobs=20)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params, n_jobs=200)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45229bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params, n_jobs=2000)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e6b9",
   "metadata": {},
   "source": [
    "#### Large scale run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d9cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                                    n_jobs=-1, timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc929a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/arc25/code_execution/evaluation_6064_bis.csv', index_label='task_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3025c30",
   "metadata": {},
   "source": [
    "Still gets hang randomly, without any hint.\n",
    "\n",
    "```\n",
    "Loadingâ€‡predictions:â€‡100%\n",
    "â€‡758/758â€‡[01:06<00:00,â€‡10.89it/s]\n",
    "Loaded 400 tasks with 6064 predictions each.\n",
    "Executingâ€‡predictions:â€‡â€‡â€‡4%\n",
    "â€‡3/76â€‡[02:52<1:10:04,â€‡57.60s/batch]\n",
    "Executingâ€‡predictionsâ€‡forâ€‡batchâ€‡0:â€‡100%\n",
    "â€‡32000/32000â€‡[00:37<00:00,â€‡849.46pred/s]\n",
    "Executingâ€‡predictionsâ€‡forâ€‡batchâ€‡1:â€‡100%\n",
    "â€‡32000/32000â€‡[01:29<00:00,â€‡356.84pred/s]\n",
    "/mnt/hdd0/MEGA/AI/22_Kaggle/arc25/arc25/validation.py:19: RuntimeWarning: invalid value encountered in cast\n",
    "  output = np.array(output, dtype=int) # otherwise I see weird outputs that mix list and numpy arrays\n",
    "Executingâ€‡predictionsâ€‡forâ€‡batchâ€‡2:â€‡100%\n",
    "â€‡32000/32000â€‡[00:44<00:00,â€‡712.58pred/s]\n",
    "Executingâ€‡predictionsâ€‡forâ€‡batchâ€‡3:â€‡100%\n",
    "â€‡31968/32000â€‡[00:35<00:00,â€‡907.65pred/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8a44b",
   "metadata": {},
   "source": [
    "What if I evaluate each file independently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "\n",
    "for filepath in tqdm(sorted(glob.glob('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/*.json')), desc=\"Processing files\"):\n",
    "    print(filepath)\n",
    "    predictions = load_all_predictions(filepath)\n",
    "    tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "    for task_id, task_preds in predictions.items():\n",
    "        tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "        task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "        text_predictions.extend(task_preds['text_predictions'])\n",
    "        data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "    results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                                        n_jobs=-1, timeout_duration=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ca045",
   "metadata": {},
   "source": [
    "This files produce consistent hangs:\n",
    "\n",
    "- /mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_08_31_09_47_48_predictions.json, 252, problem on task 2661\n",
    "- /mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_01_13_46_42_predictions.json, 670, problem on task \n",
    "\n",
    "This files throw exceptions and are run with subprocess, but it is not consistent:\n",
    "\n",
    "- /mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_08_28_22_05_23_predictions.json\n",
    "- /mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_08_28_20_29_40_predictions.json\n",
    "\n",
    "Apart from that the code is able to evaluate all the other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_08_28_22_05_23_predictions.json')\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                                    n_jobs=-1, timeout_duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_08_28_22_05_23_predictions.json')\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "for i in tqdm(range(len(tasks))):\n",
    "    results = run_code_from_predictions(tasks[i:i+1], task_ids[i:i+1], text_predictions[i:i+1], data_augmentation_params[i:i+1],\n",
    "                                        n_jobs=-1, timeout_duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_predictions[441])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66465fea",
   "metadata": {},
   "source": [
    "```python\n",
    "from common import *\n",
    "\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "# concepts:\n",
    "# pixel patterns, expansion, color swapping\n",
    "\n",
    "# description:\n",
    "# In the input you will see a grid containing a colored star shape with a single pixel in the center of a different color.\n",
    "# To make the output, you should expand the star shape outward in all directions, swapping the colors of the star and the center pixel each time the star expands.\n",
    "\n",
    "def transform(input_grid):\n",
    "    # Identify the center pixel and its color\n",
    "    center_color = None\n",
    "    center_x = center_y = None\n",
    "    \n",
    "    for x in range(input_grid.shape[0]):\n",
    "        for y in range(input_grid.shape[1]):\n",
    "            if input_grid[x, y] != Color.BLACK:\n",
    "                if center_color is None:\n",
    "                    center_color = input_grid[x, y]\n",
    "                    center_x, center_y = x, y\n",
    "                else:\n",
    "                    # If we find another colored pixel, it must be part of the star\n",
    "                    star_color = input_grid[x, y]\n",
    "                    break\n",
    "\n",
    "    # Prepare output grid\n",
    "    output_grid = input_grid.copy()\n",
    "    width, height = input_grid.shape\n",
    "\n",
    "    # Function to expand the star in a specified direction\n",
    "    def expand_star(x, y, color_a, color_b, distance):\n",
    "        for d in range(distance + 1):\n",
    "            # Expand in all four directions\n",
    "            if 0 <= x + d < width:\n",
    "                output_grid[x + d, y] = color_b  # Down\n",
    "            if 0 <= x - d < width:\n",
    "                output_grid[x - d, y] = color_b  # Up\n",
    "            if 0 <= y + d < height:\n",
    "                output_grid[x, y + d] = color_b  # Right\n",
    "            if 0 <= y - d < height:\n",
    "                output_grid[x, y - d] = color_b  # Left\n",
    "            if d == distance:  # Only at the last distance, swap the colors\n",
    "                output_grid[x, y] = color_a  # Center pixel gets the original color\n",
    "\n",
    "    # Expand the star in increasing distances until we hit the edge of the grid\n",
    "    distance = 0\n",
    "    while True:\n",
    "        try:\n",
    "            expand_star(center_x, center_y, star_color, center_color, distance)\n",
    "            distance += 1\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    return output_grid\n",
    "```\n",
    "\n",
    "Other problem on task 166\n",
    "\n",
    "```python\n",
    "from common import *\n",
    "\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "# concepts:\n",
    "# concentric patterns, color propagation\n",
    "\n",
    "# description:\n",
    "# In the input, you will see a grid with one colored pixel located in the center and other colored pixels surrounding it. \n",
    "# To create the output, expand the color of the central pixel outward in concentric layers, coloring adjacent pixels with the same color \n",
    "# until a boundary of another color is reached. Each layer should alternate colors based on the surrounding pixels' colors.\n",
    "\n",
    "def transform(input_grid):\n",
    "    # Plan:\n",
    "    # 1. Identify the center pixel and its color.\n",
    "    # 2. Expand the color of the center pixel outward until a boundary of another color is reached.\n",
    "    # 3. Alternate colors for each layer based on the surrounding pixels' colors.\n",
    "\n",
    "    # Step 1: Find the center pixel\n",
    "    center_x, center_y = input_grid.shape[0] // 2, input_grid.shape[1] // 2\n",
    "    center_color = input_grid[center_x, center_y]\n",
    "\n",
    "    # Step 2: Prepare the output grid and define the layer expansion\n",
    "    output_grid = np.copy(input_grid)\n",
    "    colors = [center_color]  # List to track the colors we are using\n",
    "\n",
    "    # To keep track of already colored pixels\n",
    "    visited = set((center_x, center_y))\n",
    "\n",
    "    def color_layer(layer):\n",
    "        # Iterate over the layer to color adjacent pixels\n",
    "        for dx in range(-layer, layer + 1):\n",
    "            for dy in range(-layer, layer + 1):\n",
    "                if abs(dx) == layer or abs(dy) == layer:  # Only the outer layer\n",
    "                    x, y = center_x + dx, center_y + dy\n",
    "                    if (x, y) not in visited and (0 <= x < output_grid.shape[0] and 0 <= y < output_grid.shape[1]):\n",
    "                        if output_grid[x, y] != Color.BLACK and output_grid[x, y] != center_color:  # Hit a boundary\n",
    "                            break\n",
    "                        output_grid[x, y] = colors[layer % len(colors)]\n",
    "                        visited.add((x, y))\n",
    "\n",
    "    # Step 3: Expand layers until boundaries are reached\n",
    "    layer = 1\n",
    "    while True:\n",
    "        try:\n",
    "            color_layer(layer)\n",
    "        except:\n",
    "            break  # If we hit a boundary, stop\n",
    "        layer += 1\n",
    "\n",
    "    return output_grid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9d03d",
   "metadata": {},
   "source": [
    "Task that raise exception:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c440fc",
   "metadata": {},
   "source": [
    "```python\n",
    "from common import *\n",
    "\n",
    "import numpy as np\n",
    "from typing import *\n",
    "\n",
    "# concepts:\n",
    "# scaling, layering, color matching\n",
    "\n",
    "# description:\n",
    "# In the input, you will see a grid containing colored circles of different sizes, each surrounded by a gray border.\n",
    "# To make the output, you need to scale the largest circle down to the size of the smallest circle and then place it on top of each circle in the same position.\n",
    "# The output grid should show the original circles with the scaled version on top, while ensuring that the colors of the circles are preserved.\n",
    "\n",
    "def transform(input_grid):\n",
    "    # Plan:\n",
    "    # 1. Extract the circles and determine their sizes and positions.\n",
    "    # 2. Identify the largest and smallest circles.\n",
    "    # 3. Scale the largest circle down to the size of the smallest circle.\n",
    "    # 4. Overlay the scaled circle onto each original circle in the output grid.\n",
    "\n",
    "    # 1. Extract the circles\n",
    "    objects = find_connected_components(input_grid, background=Color.BLACK, connectivity=8, monochromatic=False)\n",
    "\n",
    "    # Determine the sizes of the circles\n",
    "    sizes = [np.sum(obj != Color.BLACK) for obj in objects]\n",
    "    positions = [object_position(obj, background=Color.BLACK, anchor='center') for obj in objects]\n",
    "\n",
    "    # Identify the smallest and largest circles\n",
    "    smallest_circle = objects[np.argmin(sizes)]\n",
    "    largest_circle = objects[np.argmax(sizes)]\n",
    "\n",
    "    # Get the size of the smallest circle\n",
    "    smallest_circle_size = np.sum(smallest_circle != Color.BLACK)\n",
    "\n",
    "    # 2. Scale the largest circle down to the size of the smallest circle\n",
    "    largest_circle_sprite = crop(largest_circle, background=Color.BLACK)\n",
    "    scaled_largest_circle = scale_sprite(largest_circle_sprite, smallest_circle_size // np.sum(largest_circle_sprite != Color.BLACK))\n",
    "\n",
    "    # 3. Create the output grid\n",
    "    output_grid = np.full(input_grid.shape, Color.BLACK)\n",
    "\n",
    "    # 4. Overlay the scaled circle onto each original circle\n",
    "    for obj in objects:\n",
    "        circle_position = object_position(obj, background=Color.BLACK, anchor='center')\n",
    "        blit_sprite(output_grid, obj, x=circle_position[0] - obj.shape[0] // 2, y=circle_position[1] - obj.shape[1] // 2)\n",
    "        # Overlay the scaled largest circle\n",
    "        blit_sprite(output_grid, scaled_largest_circle, x=circle_position[0] - scaled_largest_circle.shape[0] // 2, y=circle_position[1] - scaled_largest_circle.shape[1] // 2)\n",
    "\n",
    "    return crop(output_grid, background=Color.BLACK)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf53a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_08_28_22_05_23_predictions.json')\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_*.json')\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                                    n_jobs=-1, timeout_duration=1)\n",
    "\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ddce0",
   "metadata": {},
   "source": [
    "#### Timeout effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094f0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_8preds_2025_08_23_11_57_11_predictions.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                                    n_jobs=-1, timeout_duration=1)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2153e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                                    n_jobs=-1, timeout_duration=5)\n",
    "df = aggregate_metrics(results)\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32380e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b50ffd40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32672244",
   "metadata": {},
   "source": [
    "### Save training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                                    n_jobs=-1, timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab0393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/arc25/code_execution/training_240.csv', index_label='task_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fa983",
   "metadata": {},
   "source": [
    "### Try different parallel implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import run_code_from_predictions\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_8preds_2025_08_23_11_57_11_predictions.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "results = run_code_from_predictions(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                                    n_jobs=-1, timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f1b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['007bbfb7']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729a4d4",
   "metadata": {},
   "source": [
    "```python\n",
    "parallel_kwargs = dict(n_jobs=n_jobs, backend=\"loky\", prefer=\"processes\", batch_size='auto')\n",
    "parallel_kwargs = dict(n_jobs=n_jobs, backend=\"threading\", batch_size='auto')\n",
    "```\n",
    "\n",
    "`signal only works in main thread of the main interpreter`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1930b",
   "metadata": {},
   "source": [
    "### Try the new code runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                          timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_*.json')\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/evaluation_*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                          timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973afa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions2025-09-15-debug-grpo/lr1e-5_small-dataset_80epochs_16gens_continue/training/*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                          timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174861c",
   "metadata": {},
   "source": [
    "Notice that it was trained just on the 67 shortests tasks.\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>n_preds</th>\n",
    "      <th>valid code</th>\n",
    "      <th>valid outputs</th>\n",
    "      <th>unique outputs</th>\n",
    "      <th>train_pixel_score</th>\n",
    "      <th>train_correct_grids</th>\n",
    "      <th>train_pass_rate</th>\n",
    "      <th>train_is_correct</th>\n",
    "      <th>test_pixel_score</th>\n",
    "      <th>test_correct_grids</th>\n",
    "      <th>test_pass_rate</th>\n",
    "      <th>test_is_correct</th>\n",
    "      <th>is_correct</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>MEAN</th>\n",
    "      <td>240.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.881615</td>\n",
    "      <td>0.291</td>\n",
    "      <td>0.632037</td>\n",
    "      <td>0.239512</td>\n",
    "      <td>0.208427</td>\n",
    "      <td>0.58</td>\n",
    "      <td>0.621141</td>\n",
    "      <td>0.23495</td>\n",
    "      <td>0.232323</td>\n",
    "      <td>0.6475</td>\n",
    "      <td>0.575</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c7395",
   "metadata": {},
   "source": [
    "Baseline:\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>n_preds</th>\n",
    "      <th>valid code</th>\n",
    "      <th>valid outputs</th>\n",
    "      <th>unique outputs</th>\n",
    "      <th>train_pixel_score</th>\n",
    "      <th>train_correct_grids</th>\n",
    "      <th>train_pass_rate</th>\n",
    "      <th>train_is_correct</th>\n",
    "      <th>test_pixel_score</th>\n",
    "      <th>test_correct_grids</th>\n",
    "      <th>test_pass_rate</th>\n",
    "      <th>test_is_correct</th>\n",
    "      <th>is_correct</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>MEAN</th>\n",
    "      <td>240.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.763875</td>\n",
    "      <td>0.410521</td>\n",
    "      <td>0.481401</td>\n",
    "      <td>0.117876</td>\n",
    "      <td>0.100604</td>\n",
    "      <td>0.6175</td>\n",
    "      <td>0.471229</td>\n",
    "      <td>0.111345</td>\n",
    "      <td>0.109667</td>\n",
    "      <td>0.6675</td>\n",
    "      <td>0.615</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions2025-09-15-debug-grpo/lr1e-5_small-dataset_80epochs_16gens_continue/evaluation/*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                          timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd593e",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>n_preds</th>\n",
    "      <th>valid code</th>\n",
    "      <th>valid outputs</th>\n",
    "      <th>unique outputs</th>\n",
    "      <th>train_pixel_score</th>\n",
    "      <th>train_correct_grids</th>\n",
    "      <th>train_pass_rate</th>\n",
    "      <th>train_is_correct</th>\n",
    "      <th>test_pixel_score</th>\n",
    "      <th>test_correct_grids</th>\n",
    "      <th>test_pass_rate</th>\n",
    "      <th>test_is_correct</th>\n",
    "      <th>is_correct</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>MEAN</th>\n",
    "      <td>256.0</td>\n",
    "      <td>1.0</td>\n",
    "      <td>0.830996</td>\n",
    "      <td>0.385771</td>\n",
    "      <td>0.522011</td>\n",
    "      <td>0.051177</td>\n",
    "      <td>0.036602</td>\n",
    "      <td>0.1775</td>\n",
    "      <td>0.507435</td>\n",
    "      <td>0.04501</td>\n",
    "      <td>0.043789</td>\n",
    "      <td>0.245</td>\n",
    "      <td>0.175</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-09-19-rl-first-steps/lr1e-6_epochs100_16gen_1prompts-per-step_32lora/training/*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                          timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eade860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/mnt/hdd0/Kaggle/arc25/code_execution/training_2025-09-19-rl-first-steps_lr1e-6_epochs100_16gen_1prompts-per-step_32lora_checkpoint8400.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e0a31",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>n_preds</th>\n",
    "      <th>valid code</th>\n",
    "      <th>valid outputs</th>\n",
    "      <th>unique outputs</th>\n",
    "      <th>train_pixel_score</th>\n",
    "      <th>train_correct_grids</th>\n",
    "      <th>train_pass_rate</th>\n",
    "      <th>train_is_correct</th>\n",
    "      <th>test_pixel_score</th>\n",
    "      <th>test_correct_grids</th>\n",
    "      <th>test_pass_rate</th>\n",
    "      <th>test_is_correct</th>\n",
    "      <th>is_correct</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>MEAN</th>\n",
    "      <td>48.0</td>\n",
    "      <td>0.963958</td>\n",
    "      <td>0.822083</td>\n",
    "      <td>0.492188</td>\n",
    "      <td>0.565531</td>\n",
    "      <td>0.071803</td>\n",
    "      <td>0.048854</td>\n",
    "      <td>0.1875</td>\n",
    "      <td>0.553833</td>\n",
    "      <td>0.06224</td>\n",
    "      <td>0.061458</td>\n",
    "      <td>0.24</td>\n",
    "      <td>0.18</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc94dd2",
   "metadata": {},
   "source": [
    "## Test memory limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import aggregate_metrics, error_analysis\n",
    "# predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/training_*.json')\n",
    "predictions = load_all_predictions('/mnt/hdd0/Kaggle/arc25/predictions/2025-08-22_add-common-prefix/evaluation_*.json')\n",
    "n_preds = len(list(predictions.values())[0]['text_predictions'])\n",
    "print(f\"Loaded {len(predictions)} tasks with {n_preds} predictions each.\")\n",
    "\n",
    "tasks, task_ids, text_predictions, data_augmentation_params = [], [], [], []\n",
    "for task_id, task_preds in predictions.items():\n",
    "    tasks.extend([get_task(task_id)] * len(task_preds['text_predictions']))\n",
    "    task_ids.extend([task_id] * len(task_preds['text_predictions']))\n",
    "    text_predictions.extend(task_preds['text_predictions'])\n",
    "    data_augmentation_params.extend(task_preds['data_augmentation_params'])\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, text_predictions, data_augmentation_params,\n",
    "                          timeout_duration=1, batch_size=5000)\n",
    "df = aggregate_metrics(results)\n",
    "\n",
    "error_analysis(results);\n",
    "df.iloc[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a07759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import error_analysis\n",
    "task_id = '00576224'\n",
    "\n",
    "code = \"\"\"```python\n",
    "import numpy as np\n",
    "\n",
    "def transform(input_grid):\n",
    "    a = np.arange(N_MB * 1024 * 1024 // 8)\n",
    "    return input_grid.copy()\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "mb_range = np.linspace(10, 1000, 20, dtype=int).tolist()\n",
    "codes = [code.replace('N_MB', str(mb)) for mb in mb_range]\n",
    "task_ids = [task_id] * len(codes)\n",
    "tasks = [get_task(task_id)] * len(codes)\n",
    "data_augmentation_params = [None] * len(codes)\n",
    "\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, codes, data_augmentation_params,\n",
    "                          timeout_duration=10, batch_size=5000)\n",
    "\n",
    "successful_runs = [mb for result, mb in zip(results[task_id], mb_range) if 'error_message' not in result]\n",
    "unsuccessful_runs = [mb for result, mb in zip(results[task_id], mb_range) if 'error_message' in result]\n",
    "print(\"Successful runs for MB values:\", successful_runs)\n",
    "print(\"Unsuccessful runs for MB values:\", unsuccessful_runs)\n",
    "if unsuccessful_runs:\n",
    "    error_analysis(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ed002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import error_analysis\n",
    "task_id = '00576224'\n",
    "\n",
    "code = \"\"\"```python\n",
    "import numpy as np\n",
    "\n",
    "def transform(input_grid):\n",
    "    a = np.arange(N_MB * 1024 * 1024 // 8)\n",
    "    return input_grid.copy()\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "mb_range = np.linspace(10, 4000, 20, dtype=int).tolist()\n",
    "codes = [code.replace('N_MB', str(mb)) for mb in mb_range]\n",
    "task_ids = [task_id] * len(codes)\n",
    "tasks = [get_task(task_id)] * len(codes)\n",
    "data_augmentation_params = [None] * len(codes)\n",
    "\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, codes, data_augmentation_params,\n",
    "                          timeout_duration=10, batch_size=5000)\n",
    "\n",
    "successful_runs = [mb for result, mb in zip(results[task_id], mb_range) if 'error_message' not in result]\n",
    "unsuccessful_runs = [mb for result, mb in zip(results[task_id], mb_range) if 'error_message' in result]\n",
    "print(\"Successful runs for MB values:\", successful_runs)\n",
    "print(\"Unsuccessful runs for MB values:\", unsuccessful_runs)\n",
    "if unsuccessful_runs:\n",
    "    error_analysis(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import error_analysis\n",
    "task_id = '00576224'\n",
    "\n",
    "code = \"\"\"```python\n",
    "import numpy as np\n",
    "\n",
    "def transform(input_grid):\n",
    "    a = np.arange(N_MB * 1024 * 1024 // 8)\n",
    "    return input_grid.copy()\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "mb_range = np.linspace(10, 4000, 20, dtype=int).tolist()\n",
    "mb_range = mb_range + mb_range\n",
    "codes = [code.replace('N_MB', str(mb)) for mb in mb_range]\n",
    "task_ids = [task_id] * len(codes)\n",
    "tasks = [get_task(task_id)] * len(codes)\n",
    "data_augmentation_params = [None] * len(codes)\n",
    "\n",
    "\n",
    "code_runner = CodeRunner(n_jobs=-1)\n",
    "\n",
    "results = code_runner.run(tasks, task_ids, codes, data_augmentation_params,\n",
    "                          timeout_duration=10, batch_size=5000)\n",
    "\n",
    "successful_runs = [mb for result, mb in zip(results[task_id], mb_range) if 'error_message' not in result]\n",
    "unsuccessful_runs = [mb for result, mb in zip(results[task_id], mb_range) if 'error_message' in result]\n",
    "print(\"Successful runs for MB values:\", successful_runs)\n",
    "print(\"Unsuccessful runs for MB values:\", unsuccessful_runs)\n",
    "if unsuccessful_runs:\n",
    "    error_analysis(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c9afe7",
   "metadata": {},
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74529d0",
   "metadata": {},
   "source": [
    "- [x] Load all ARC data\n",
    "- [x] Modify prompt generation to use all the training data, and a random sample from the test samples.\n",
    "- [x] Update the data augmentation pipeline to use dicts instead of tasks\n",
    "- [x] Modify the code execution to use all the data\n",
    "- [x] Add a new metric to check if the code is correct for the train samples but incorrect for the test samples\n",
    "- [x] Refactor\n",
    "- [x] Convert to python script so I can make predictions remotely\n",
    "- [x] Evaluate the datasets\n",
    "- [x] Check if the answers always start with the same prefix, if that is the case I could speedup inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747472b",
   "metadata": {},
   "source": [
    "- [x] Implement new grid encoder\n",
    "- [x] Use the correct prompt\n",
    "- [x] Save predictions to file so I can later reprocess them\n",
    "- [x] Update code execution to match the code generated by BARC model\n",
    "- [x] Check code execution to verify that exceptions are legit and not easily solvable\n",
    "  - [x] Add missing colors to Color object\n",
    "  - [x] Code execution fails when there are auxiliary functions. `Error executing code for task 025d127b, response 5: <class 'NameError'> name 'blend_colors' is not defined`\n",
    "  - [x] Arrays as inputs\n",
    "- [x] Remove dsl usage metric\n",
    "- [x] Add correct task metric\n",
    "- [x] Parallelize code execution\n",
    "- [x] Refactor code\n",
    "- [x] Plots showing the effect of increasing the number of predictions\n",
    "- [x] Validate that I get the same scores of the paper\n",
    "- [x] Evaluate on different datasets\n",
    "- [x] Improve metrics\n",
    "- [x] Data augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
