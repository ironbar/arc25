{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03b2969",
   "metadata": {},
   "source": [
    "# Search and learn with unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990134f",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5425e",
   "metadata": {},
   "source": [
    "1. Learn to use unsloth\n",
    "2. See how viable is to use it for search and learn\n",
    "3. Compare speed with other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959ea64",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d88e4",
   "metadata": {},
   "source": [
    "- https://docs.unsloth.ai/\n",
    "- https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\n",
    "- https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide\n",
    "- Inference with LoRA:\n",
    "  - https://github.com/unslothai/unsloth/issues/2009\n",
    "  - https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5733e34",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from arc25.utils import set_cuda_visible_devices_to_least_used_gpu_if_undefined\n",
    "from arc25.logging import configure_logging, logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "set_cuda_visible_devices_to_least_used_gpu_if_undefined()\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n",
    "\n",
    "import time\n",
    "from unsloth import FastLanguageModel\n",
    "import gc\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d95dc",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06574e",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58306edf",
   "metadata": {},
   "source": [
    "### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd13247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03926447",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_bos_token=True, return_tensors=\"pt\"\n",
    ").to(llm.device)\n",
    "outputs = llm.generate(inputs, max_new_tokens = 64, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d41b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "responses = llm.fast_generate(inputs)\n",
    "print(responses[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9de65",
   "metadata": {},
   "source": [
    "This seems to be much faster, 0.3s vs 1.9s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9474",
   "metadata": {},
   "source": [
    "Let's see if we can make more predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "responses = llm.fast_generate(inputs, sampling_params=sampling_params)\n",
    "print(len(responses), len(responses[0].outputs))\n",
    "print(responses[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d62ae",
   "metadata": {},
   "source": [
    "Seems very similar to VLLM, I should do a direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3e6fb",
   "metadata": {},
   "source": [
    "### Compare inference speed of VLLM vs unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cce927",
   "metadata": {},
   "source": [
    "Ideally I would see the same speed with both methods, because unsloth uses VLLM under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360f4e8",
   "metadata": {},
   "source": [
    "#### VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1,\n",
    "               max_model_len=32000, enable_lora=False, max_lora_rank=16):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=0.92,  # Use less GPU memory\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "        enable_lora=enable_lora,\n",
    "        max_lora_rank=max_lora_rank,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=True, tensor_parallel_size=1, max_model_len=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=512, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = llm.generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d8b43",
   "metadata": {},
   "source": [
    "#### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e359b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=512, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = llm.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615f176",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdfdeb",
   "metadata": {},
   "source": [
    "unsloth has a faster startup of 54s vs 1m51s for VLLM.\n",
    "\n",
    "The table below shows the inference speed in tokens/s when generating 100 tokens per prompt.\n",
    "\n",
    "| method \\ n predictions | 8   | 32  | 128  | 512  |\n",
    "|------------------------|-----|-----|------|------|\n",
    "| VLLM                   | 140 | 512 | 1476 | 1992 |\n",
    "| unsloth                | 138 | 510 | 1454 | 1464 |\n",
    "\n",
    "They are very similar except from the last column, where I believe VLLM is using more VRAM memory than\n",
    "unsloth. This is promising because it opens the door to use unsloth both for training and inference\n",
    "in the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4c2d6",
   "metadata": {},
   "source": [
    "### Proof of concept of inference, training and inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9233f8",
   "metadata": {},
   "source": [
    "Let's verify that I can do fast inference, train and fast inference again with unsloth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868177a",
   "metadata": {},
   "source": [
    "#### Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ba81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = llm.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0b045",
   "metadata": {},
   "source": [
    "#### Finetune on a single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bab6e",
   "metadata": {},
   "source": [
    "Let's use a single training sample that simply rejects to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "print(prompt)\n",
    "print(len(tokenizer.tokenize(prompt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc4efe",
   "metadata": {},
   "source": [
    "Now let's add a lora adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e521ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf6241",
   "metadata": {},
   "source": [
    "#### Repeat inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/lora\"\n",
    "model.save_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f356109",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_request = LoRARequest('LoRA', 1, lora_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4551c1",
   "metadata": {},
   "source": [
    "We need to provide the lora request, otherwise it simply uses the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474899d1",
   "metadata": {},
   "source": [
    "#### Create a new fresh lora adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"AI don't know that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()\n",
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/lora-2\"\n",
    "model.save_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ee5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_request = LoRARequest('LoRA-2', 2, lora_save_path)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed4af5",
   "metadata": {},
   "source": [
    "Awesome, seems that we can create new LoRAs without problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b8966",
   "metadata": {},
   "source": [
    "#### Load LoRA and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/lora\"\n",
    "model.load_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2874cf",
   "metadata": {},
   "source": [
    "This returns a lora request, kind of weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed636fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.load_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.load_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047691d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"LoRA\")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea33c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"LoRA-2\")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e98353",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"default\")\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc637535",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/lora\"\n",
    "model.load_adapter(lora_save_path, adapter_name=\"LoRA\", is_trainable=True)\n",
    "model.set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/lora-2\"\n",
    "model.load_adapter(lora_save_path, adapter_name=\"LoRA-2\", is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde96177",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_adapter(\"LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872648e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.peft_config.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cae3ae",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f986f2e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
