{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03b2969",
   "metadata": {},
   "source": [
    "# Search and learn with unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990134f",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5425e",
   "metadata": {},
   "source": [
    "1. Learn to use unsloth\n",
    "2. See how viable is to use it for search and learn\n",
    "3. Compare speed with other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959ea64",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d88e4",
   "metadata": {},
   "source": [
    "- https://docs.unsloth.ai/\n",
    "- https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\n",
    "- https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide\n",
    "- Inference with LoRA:\n",
    "  - https://github.com/unslothai/unsloth/issues/2009\n",
    "  - https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5733e34",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from arc25.utils import set_cuda_visible_devices_to_least_used_gpu_if_undefined\n",
    "from arc25.logging import configure_logging, logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "set_cuda_visible_devices_to_least_used_gpu_if_undefined()\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from joblib import Parallel, delayed\n",
    "import hashlib\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.utils import load_arc_dataset_with_solutions\n",
    "from arc25.data_augmentation import apply_data_augmentation, revert_data_augmentation, get_random_data_augmentation_params\n",
    "from arc25.code_execution import safe_code_execution\n",
    "from arc25.prompting import pretty_print_prompt, Template\n",
    "from arc25.metrics import pixel_similarity_score, correct_grids_score\n",
    "from arc25.plot import plot_task\n",
    "\n",
    "sys.path.append(os.path.realpath(\"../scripts\"))\n",
    "from finetuning import get_data_collator\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d95dc",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f077f1",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe move this to the prompting module\n",
    "# https://huggingface.co/barc0/Llama-3.1-ARC-Potpourri-Induction-8B\n",
    "system_prompt = \"\"\"You are a world-class puzzle solver with exceptional pattern recognition skills and expertise in Python programming. Your task is to analyze puzzles and provide Python solutions.\"\"\"\n",
    "\n",
    "prompt_template_text = \"\"\"Given input-output grid pairs as reference examples, carefully observe the patterns to predict the output grid for new test input. Each pair follows the same transformation rule. Grids are 2D arrays represented as strings, with cells (colors) separated by spaces and rows by newlines.\n",
    "Here are the input and output grids for the reference examples:\n",
    "{% for sample in train_samples %}Example {{ loop.index }}\n",
    "Input:\n",
    "{{ sample.input }}\n",
    "\n",
    "Output:\n",
    "{{ sample.output }}\n",
    "\n",
    "{% endfor %}\n",
    "Here is the input grid for the test example:\n",
    "{{ test }}\n",
    "\n",
    "Write a Python function `transform` that can convert any given input grid to its corresponding output grid based on the pattern observed in the reference examples.\n",
    "\"\"\"\n",
    "\n",
    "# I have verified that all responses start with this prefix\n",
    "common_prefix = \"Let's solve this puzzle using Python code with the common library functions. We'll first reason about the problem and then write the code to solve it. The `transform` function will take the input grid and return the output grid. Here is the Python code with the comments describing how to solve the problem:\\n\" #```python\\nfrom common import *\\n\"\n",
    "\n",
    "prompt_template = Template(prompt_template_text)\n",
    "\n",
    "def create_prompt_from_task(task, grid_encoder, tokenizer, shuffle_train_samples=True):\n",
    "    train_samples = [{'input': grid_encoder.to_text(sample['input']), 'output': grid_encoder.to_text(sample['output'])} for sample in task['train']]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    test_sample = random.choice(task['test'])\n",
    "    render_kwargs = dict(train_samples=train_samples, test=grid_encoder.to_text(test_sample['input']))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)},\n",
    "                {\"role\": \"assistant\", \"content\": common_prefix}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=False,\n",
    "                                            continue_final_message=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be000e",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d17569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_python_code(text):\n",
    "    # Extract Python code from the text\n",
    "    if '```python' not in text:\n",
    "        return ''\n",
    "    code = text.split('```python')[1]\n",
    "    if not '```' in code:\n",
    "        return ''\n",
    "\n",
    "    code = code.split('```')[0].strip()\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ac933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_python_code(code):\n",
    "    remove_line_keywords = ['import dsl', 'from dsl import ', 'print(', 'from common import *']\n",
    "    code = '\\n'.join(line for line in code.split('\\n') if not any(keyword in line for keyword in remove_line_keywords))\n",
    "    # code = 'from arc25.BARC_dsl import *\\n' + code  # Ensure BARC_dsl is imported\n",
    "    return code.strip()\n",
    "\n",
    "\n",
    "def add_additional_imports(code):\n",
    "    additional_imports = [\n",
    "        'from typing import List, Tuple',\n",
    "        'import numpy as np',\n",
    "        'import numpy',\n",
    "        'from arc25.BARC_dsl import *',\n",
    "    ]\n",
    "    imports = '\\n'.join(additional_imports)\n",
    "    return imports + '\\n' + code if code else imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1331e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_outputs(outputs):\n",
    "    if not outputs:\n",
    "        raise ValueError(\"Outputs list is empty\")\n",
    "    return [_validate_output(output) for output in outputs]\n",
    "\n",
    "\n",
    "def _validate_output(output):\n",
    "    if output is None:\n",
    "        raise ValueError(\"Output is None\")\n",
    "    output = np.array(output, dtype=int) # otherwise I see weird outputs that mix list and numpy arrays\n",
    "    if output.ndim != 2:\n",
    "        raise ValueError(f\"Output is not a 2D array. Output shape: {output.shape}\")\n",
    "    if max(output.shape) > 35:\n",
    "        raise ValueError(f\"Output is too large, the maximum allowed shape is 30x30. Output shape: {output.shape}\")\n",
    "    if min(output.shape) == 0:\n",
    "        raise ValueError(f\"Output has zero dimension, it is empty. Output shape: {output.shape}\")\n",
    "    if np.max(output) > 9 or np.min(output) < 0:\n",
    "        raise ValueError(f\"Output contains invalid values, expected values in range [0, 9]. Output max: {np.max(output)}, min: {np.min(output)}\")\n",
    "    # if not np.issubdtype(output.dtype, np.integer):\n",
    "    #     raise ValueError(f\"Output contains non-integer values, expected integer values. Output dtype: {output.dtype}\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3df395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_from_predictions(task, text_predictions, data_augmentation_params):\n",
    "    # Flatten all predictions into a work list\n",
    "    work = [\n",
    "        (text_pred, task, params)\n",
    "        for text_pred, params in zip(text_predictions, data_augmentation_params)\n",
    "    ]\n",
    "    n_jobs = -1  # all cores; set to an int to cap\n",
    "\n",
    "    # sort the work by prediction index first and the task id second, I believe this will improve resource allocation\n",
    "    # because some tasks are more resource intensive than others\n",
    "    work.sort(key=lambda x: (x[1], x[0]))\n",
    "\n",
    "    # with tqdm_joblib(tqdm(total=len(work), desc=\"Executing predictions\", unit=\"pred\")):\n",
    "    with tqdm_joblib(total=len(work), desc=\"Executing code from predictions\", unit=\"runs\", smoothing=0):\n",
    "        results = Parallel(\n",
    "            n_jobs=n_jobs,\n",
    "            backend=\"loky\",\n",
    "            prefer=\"processes\",\n",
    "            batch_size=1,\n",
    "        )(delayed(_run_one)(*args) for args in work)\n",
    "    return results\n",
    "\n",
    "\n",
    "def _run_one(text_prediction, task, data_augmentation_params=None):\n",
    "    code = parse_python_code(text_prediction)\n",
    "    if not code:\n",
    "        return dict(error_type=\"ParsingCodeFailed\", error_message='', text_prediction=text_prediction)\n",
    "    try:\n",
    "        input_grids = [sample['input'] for sample in task['train']] + [sample['input'] for sample in task['test']]\n",
    "        if data_augmentation_params is not None:\n",
    "            input_grids = apply_data_augmentation(input_grids, **data_augmentation_params)\n",
    "        output_grids = safe_code_execution(\n",
    "            add_additional_imports(curate_python_code(code)),\n",
    "            input_grids,\n",
    "            func_name=\"transform\",\n",
    "        )\n",
    "        output_grids = validate_outputs(output_grids)\n",
    "        if data_augmentation_params is not None:\n",
    "            original_output_grids = revert_data_augmentation(output_grids, **data_augmentation_params)\n",
    "        else:\n",
    "            original_output_grids = output_grids\n",
    "        result = dict(code=code, output_grids=output_grids,\n",
    "                      input_grids=input_grids, text_prediction=text_prediction,\n",
    "                      fingerprint=fingerprint(original_output_grids))\n",
    "        result.update(_compute_metrics(task, original_output_grids))\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return dict(code=code, error_type=type(e).__name__, error_message=str(e))\n",
    "\n",
    "\n",
    "def fingerprint(prediction):\n",
    "    \"\"\"\n",
    "    Create a compact hash for a list of matrices.\n",
    "    Includes shape & dtype to distinguish e.g. (2×2) from (4×1).\n",
    "    \"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    for m in prediction:\n",
    "        # incorporate shape and dtype in a reproducible way\n",
    "        h.update(str(m.shape).encode())\n",
    "        h.update(m.dtype.str.encode())\n",
    "        # raw data bytes\n",
    "        h.update(m.tobytes())\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def _compute_metrics(task, predicted_grids):\n",
    "    metrics = {}\n",
    "    for partition in ['train', 'test']:\n",
    "        if not 'output' in task[partition][0]:\n",
    "            continue # we won't have the output when making submissions\n",
    "        gt_grids = [sample['output'] for sample in task[partition]]\n",
    "        n_samples = len(gt_grids)\n",
    "        partition_predicted_grids = predicted_grids[:n_samples] if partition == 'train' else predicted_grids[-n_samples:]\n",
    "        pixel_scores = np.array([pixel_similarity_score(pred, gt) for pred, gt in zip(partition_predicted_grids, gt_grids)])\n",
    "        metrics[f\"{partition}_pixel_score\"] = float(np.mean(pixel_scores))\n",
    "        metrics[f'{partition}_correct_grids'] = float(np.mean(pixel_scores == 1))\n",
    "        metrics[f'{partition}_is_correct'] = int(all(pixel_scores == 1))\n",
    "    return metrics\n",
    "\n",
    "# tiny_predictions = {'00576224': predictions['00576224']}\n",
    "# predicted_code, predicted_outputs = run_code_from_predictions(tiny_predictions, log_errors=True)\n",
    "# df = compute_search_metrics(list(tiny_predictions.keys()), predicted_code, predicted_outputs, n_preds)\n",
    "# df.round(3)\n",
    "# predicted_code, predicted_outputs = run_code_from_predictions(predictions, log_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349afe1c",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metrics(results, task_id):\n",
    "    df = pd.DataFrame()\n",
    "    n_preds = len(results)\n",
    "    df.loc[task_id, 'n_preds'] = n_preds\n",
    "    df.loc[task_id, 'valid code'] = (len([1 for result in results if 'error_type' not in result]))/n_preds\n",
    "    df.loc[task_id, 'unique outputs'] = len(set(result['fingerprint'] for result in results if 'fingerprint' in result))/n_preds\n",
    "    for partition in ['train', 'test']:\n",
    "        df.loc[task_id, f'{partition}_pixel_score'] = np.mean([result.get(f'{partition}_pixel_score', 0) for result in results])\n",
    "        df.loc[task_id, f'{partition}_correct_grids'] = np.mean([result.get(f'{partition}_correct_grids', 0) for result in results])\n",
    "        df.loc[task_id, f'{partition}_pass_rate'] = sum(result.get(f'{partition}_is_correct', 0) for result in results)/n_preds\n",
    "        df.loc[task_id, f'{partition}_is_correct'] = int(any(result.get(f'{partition}_is_correct', 0) for result in results))\n",
    "    return df.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27694cb6",
   "metadata": {},
   "source": [
    "### Hindsight relabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d13e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hindsight_relabeled_tasks(results, task):\n",
    "    # TODO: strategies to avoid repetitions\n",
    "    # TODO: sort the tasks\n",
    "    relabeled_tasks = []\n",
    "    n_train = len(task['train'])\n",
    "    for result in results:\n",
    "        if 'output_grids' not in result:\n",
    "            continue\n",
    "        new_task = {\n",
    "            'train': [{'input': input, 'output': output} for input, output in zip(result['input_grids'][:n_train], result['output_grids'][:n_train])],\n",
    "            'test': [{'input': input, 'output': output} for input, output in zip(result['input_grids'][n_train:], result['output_grids'][n_train:])],\n",
    "            'text_prediction': result['text_prediction'],\n",
    "        }\n",
    "        relabeled_tasks.append(new_task)\n",
    "    return relabeled_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995bc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompts(relabeled_tasks, grid_encoder, tokenizer):\n",
    "    prompts = []\n",
    "    for task in relabeled_tasks:\n",
    "        prompt = create_prompt_from_task(\n",
    "            task, grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "        prompt += task['text_prediction'] + tokenizer.eos_token\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c64d8",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14075650",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class cfg:\n",
    "    # base model\n",
    "    model_path: str = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "    load_in_4bit: bool = True\n",
    "    max_seq_length: int = 10000\n",
    "    grid_encoder: str = 'ColorNameEncoder()'\n",
    "    gpu_memory_utilization: float = 0.90\n",
    "    # dataset\n",
    "    dataset_path: str = \"/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json\"\n",
    "    max_epochs: int = 1\n",
    "    use_data_augmentation: bool = True\n",
    "    inference_batch_size: int = 1\n",
    "    predictions_per_epoch: int = 8\n",
    "    training_batch_size: int = 1\n",
    "\n",
    "assert cfg.predictions_per_epoch % cfg.inference_batch_size == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    cfg.model_path, load_in_4bit=cfg.load_in_4bit, max_seq_length=cfg.max_seq_length,\n",
    "    fast_inference=True, gpu_memory_utilization=cfg.gpu_memory_utilization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ee6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_arc_dataset_with_solutions(cfg.dataset_path)\n",
    "task_ids = list(dataset.keys())\n",
    "print(f\"Loaded {len(dataset)} tasks from {cfg.dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5bf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_encoder = create_grid_encoder(cfg.grid_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = dict()\n",
    "for task_id in tqdm(task_ids[::4], desc=\"Tasks\", unit=\"task\"):\n",
    "    # model.unload()\n",
    "    # model = FastLanguageModel.get_peft_model(\n",
    "    #     llm,\n",
    "    #     r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    #     target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    #     lora_alpha = 64,\n",
    "    #     lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    #     bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    #     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    #     use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    #     use_rslora = True,  # We support rank stabilized LoRA\n",
    "    #     loftq_config = None, # And LoftQ\n",
    "    # )\n",
    "    lora_request = None\n",
    "    sampling_params = SamplingParams(n=cfg.inference_batch_size, temperature=1.0, top_p=0.95, max_tokens=2048) # TODO: move parameters to cfg\n",
    "    for epoch in range(1, cfg.max_epochs + 1):\n",
    "        # the following code makes predictions, should be moved to a function\n",
    "        prompts, data_augmentation_params = [], []\n",
    "        for _ in range(cfg.predictions_per_epoch // cfg.inference_batch_size):\n",
    "            task = dataset[task_id]\n",
    "            if cfg.use_data_augmentation:\n",
    "                params = get_random_data_augmentation_params()\n",
    "                task = apply_data_augmentation(task, **params)\n",
    "            else:\n",
    "                params = None\n",
    "            data_augmentation_params.extend([params] * cfg.inference_batch_size)\n",
    "            prompt = create_prompt_from_task(\n",
    "                task, grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        t0 = time.time()\n",
    "        generations = llm.fast_generate(prompts, sampling_params, lora_request=lora_request)\n",
    "        total_tokens = sum(sum(len(output.token_ids) for output in generation.outputs) for generation in generations)\n",
    "        inference_time = time.time() - t0\n",
    "        logger.info(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second (Inference time: {inference_time:.1f} seconds)\")\n",
    "\n",
    "        text_predictions = []\n",
    "        for generation in generations:\n",
    "            for output in generation.outputs:\n",
    "                text_predictions.append(output.text)\n",
    "        assert len(text_predictions) == cfg.predictions_per_epoch\n",
    "\n",
    "        # run code and compute metrics\n",
    "        results = run_code_from_predictions(task, text_predictions, data_augmentation_params)\n",
    "        display(aggregate_metrics(results, task_id).round(3))\n",
    "        all_results[task_id] = results\n",
    "        # TODO: stop criteria\n",
    "        if epoch == cfg.max_epochs:\n",
    "            break\n",
    "        # TODO: prepare data for training\n",
    "        relabeled_tasks = create_hindsight_relabeled_tasks(results, task)\n",
    "        training_prompts = create_training_prompts(relabeled_tasks, grid_encoder, tokenizer)\n",
    "        train_dataset = Dataset.from_dict({'text': training_prompts})\n",
    "        # TODO: train model\n",
    "        trainer = SFTTrainer(\n",
    "            model = model,\n",
    "            tokenizer = tokenizer,\n",
    "            train_dataset = train_dataset,\n",
    "            dataset_text_field = \"text\",\n",
    "            max_seq_length = 8192,\n",
    "            packing = False, # Can make training 5x faster for short sequences.\n",
    "            data_collator=get_data_collator(tokenizer),\n",
    "            args = SFTConfig(\n",
    "                per_device_train_batch_size = 1,\n",
    "                gradient_accumulation_steps = 1,\n",
    "                warmup_ratio=0.1,\n",
    "                num_train_epochs=1,\n",
    "                save_strategy='no',\n",
    "                learning_rate = 1e-5,\n",
    "                logging_steps = 1,\n",
    "                optim = \"adamw_torch_fused\",\n",
    "                weight_decay = 0.01,\n",
    "                lr_scheduler_type = 'constant_with_warmup',\n",
    "                # seed = 3407,\n",
    "                output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-04-debug-unsloth\",\n",
    "                report_to = \"none\", # Use this for WandB etc\n",
    "            ),\n",
    "        )\n",
    "        trainer_stats = trainer.train()\n",
    "        model.save_lora(\"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-04-debug-unsloth/lora\")\n",
    "        lora_request = model.load_lora(\"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-04-debug-unsloth/lora\")\n",
    "# TODO: select best predictions and prepare submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for task_id, results in all_results.items():\n",
    "    df = aggregate_metrics(results, task_id)\n",
    "    dfs.append(df)\n",
    "final_df = pd.concat(dfs)\n",
    "final_df['is_correct'] = final_df['test_is_correct']*final_df['train_is_correct']\n",
    "final_df.loc['MEAN'] = final_df.mean()\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90278dbe",
   "metadata": {},
   "source": [
    "```\n",
    "8 preds\n",
    "20 tasks, 9 min, 15% correct\n",
    "100 tasks, 49 min, 13% correct\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_prompt(training_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "relabeled_tasks = create_hindsight_relabeled_tasks(results, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b71c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc25.plot import plot_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[result.get('train_is_correct', None) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff40e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "[result.get('train_pixel_score', None) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_task(relabeled_tasks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06574e",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58306edf",
   "metadata": {},
   "source": [
    "### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd13247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03926447",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_bos_token=True, return_tensors=\"pt\"\n",
    ").to(llm.device)\n",
    "outputs = llm.generate(inputs, max_new_tokens = 64, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d41b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "responses = llm.fast_generate(inputs)\n",
    "print(responses[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9de65",
   "metadata": {},
   "source": [
    "This seems to be much faster, 0.3s vs 1.9s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9474",
   "metadata": {},
   "source": [
    "Let's see if we can make more predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "responses = llm.fast_generate(inputs, sampling_params=sampling_params)\n",
    "print(len(responses), len(responses[0].outputs))\n",
    "print(responses[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d62ae",
   "metadata": {},
   "source": [
    "Seems very similar to VLLM, I should do a direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3e6fb",
   "metadata": {},
   "source": [
    "### Compare inference speed of VLLM vs unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cce927",
   "metadata": {},
   "source": [
    "Ideally I would see the same speed with both methods, because unsloth uses VLLM under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360f4e8",
   "metadata": {},
   "source": [
    "#### VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1,\n",
    "               max_model_len=32000, enable_lora=False, max_lora_rank=16):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=0.92,  # Use less GPU memory\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "        enable_lora=enable_lora,\n",
    "        max_lora_rank=max_lora_rank,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=True, tensor_parallel_size=1, max_model_len=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=512, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = llm.generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d8b43",
   "metadata": {},
   "source": [
    "#### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e359b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=512, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = llm.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615f176",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdfdeb",
   "metadata": {},
   "source": [
    "unsloth has a faster startup of 54s vs 1m51s for VLLM.\n",
    "\n",
    "The table below shows the inference speed in tokens/s when generating 100 tokens per prompt.\n",
    "\n",
    "| method \\ n predictions | 8   | 32  | 128  | 512  |\n",
    "|------------------------|-----|-----|------|------|\n",
    "| VLLM                   | 140 | 512 | 1476 | 1992 |\n",
    "| unsloth                | 138 | 510 | 1454 | 1464 |\n",
    "\n",
    "They are very similar except from the last column, where I believe VLLM is using more VRAM memory than\n",
    "unsloth. This is promising because it opens the door to use unsloth both for training and inference\n",
    "in the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4c2d6",
   "metadata": {},
   "source": [
    "### Proof of concept of inference, training and inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9233f8",
   "metadata": {},
   "source": [
    "Let's verify that I can do fast inference, train and fast inference again with unsloth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868177a",
   "metadata": {},
   "source": [
    "#### Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ba81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = llm.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(generations):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0b045",
   "metadata": {},
   "source": [
    "#### Finetune on a single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bab6e",
   "metadata": {},
   "source": [
    "Let's use a single training sample that simply rejects to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc4efe",
   "metadata": {},
   "source": [
    "Now let's add a lora adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "print(prompt)\n",
    "print(len(tokenizer.tokenize(prompt)))\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e521ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "model.save_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf6241",
   "metadata": {},
   "source": [
    "#### Repeat inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = model.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(generations):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "lora_request = LoRARequest('LoRA', 1, lora_save_path)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(generations):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea0567",
   "metadata": {},
   "source": [
    "We need to provide the lora request, otherwise it simply uses the base model.\n",
    "\n",
    "We can also generate the lora request with the `load_lora` method from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2348943",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_request = model.load_lora(lora_save_path)\n",
    "print(lora_request)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(generations):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474899d1",
   "metadata": {},
   "source": [
    "#### Create a new fresh lora adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unload()\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cb422",
   "metadata": {},
   "source": [
    "Check that the model is untrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, but maybe tomorrow.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed4af5",
   "metadata": {},
   "source": [
    "Awesome, seems that we can create new LoRAs without problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3222c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/tomorrow\"\n",
    "model.save_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fe350",
   "metadata": {},
   "source": [
    "#### Inference with multiple loras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "lora_request = model.load_lora(lora_save_path)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(generations):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/tomorrow\"\n",
    "lora_request = model.load_lora(lora_save_path)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(generations):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b8966",
   "metadata": {},
   "source": [
    "#### Load LoRA and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5486c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95698eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, but maybe tomorrow.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf11170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unload()\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54039e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, but maybe tomorrow.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a31060",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30088fb",
   "metadata": {},
   "source": [
    "Let's verify that calling the lora adapter has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "model.load_lora(lora_save_path)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71a9d9",
   "metadata": {},
   "source": [
    "Now let's try using the adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "model.load_adapter(lora_save_path, adapter_name=\"refusal\", is_trainable=True)\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)\n",
    "model.set_adapter(\"refusal\")\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fededa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519603ff",
   "metadata": {},
   "source": [
    "Awesome, this seems to be working! Let's train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 50,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd72480",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal-v2\"\n",
    "model.save_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_request = model.load_lora(lora_save_path)\n",
    "print(lora_request)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "generations = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in generations)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(generations):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(generations) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(generations):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf2746",
   "metadata": {},
   "source": [
    "This is weird, it seems to have saved the untrained adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83567727",
   "metadata": {},
   "source": [
    "#### Try another approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unload()\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51989884",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal/adapter_model.safetensors\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097b5ba",
   "metadata": {},
   "source": [
    "I could use the checkpoints and avoid doing weird tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dadbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c58817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal/adapter_model.safetensors\", weights_only=False), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51650101",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.from_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah /mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/*/adapter_model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.load_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal-v2\"\n",
    "model.save_lora(lora_save_path, 'refusal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cae3ae",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda038d",
   "metadata": {},
   "source": [
    "- [x] Maybe I should integrate evaluation, and hindsight relabeling all in the same code execution step because it is done in parallel\n",
    "- [x] Check if training prompts are being created correctly or if I'm missing some space or anything. I find weird the high loss I see when I start training (2.3, 5). They are created correctly but I'm not using data collator\n",
    "- [x] Add data collator. Now the training loss is much lower\n",
    "- [ ] Run a test without any learning to compare with my benchmarks\n",
    "- [ ] Compare the code to the training script to see if there are relevant differences\n",
    "- [ ] Tokenization seems to be slow at the beginning of the training\n",
    "- [ ] Make the code robust to code execution errors\n",
    "- [ ] Doing the first inference run for all the tasks could be more efficient, because throughput is increased with the number of predictions. Also it might have sense to do a wider exploration at first and focus on the best results second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f986f2e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
