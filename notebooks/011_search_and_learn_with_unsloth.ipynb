{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03b2969",
   "metadata": {},
   "source": [
    "# Search and learn with unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990134f",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5425e",
   "metadata": {},
   "source": [
    "1. Learn to use unsloth\n",
    "2. See how viable is to use it for search and learn\n",
    "3. Compare speed with other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959ea64",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d88e4",
   "metadata": {},
   "source": [
    "- https://docs.unsloth.ai/\n",
    "- https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\n",
    "- https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5733e34",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3eb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from arc25.utils import set_cuda_visible_devices_to_least_used_gpu_if_undefined\n",
    "from arc25.logging import configure_logging\n",
    "\n",
    "configure_logging()\n",
    "set_cuda_visible_devices_to_least_used_gpu_if_undefined()\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbced08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from vllm import SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d95dc",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58306edf",
   "metadata": {},
   "source": [
    "## First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd13247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03926447",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_bos_token=True, return_tensors=\"pt\"\n",
    ").to(llm.device)\n",
    "outputs = llm.generate(inputs, max_new_tokens = 64, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d41b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "responses = llm.fast_generate(inputs)\n",
    "print(responses[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9de65",
   "metadata": {},
   "source": [
    "This seems to be much faster, 0.3s vs 1.9s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9474",
   "metadata": {},
   "source": [
    "Let's see if we can make more predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "responses = llm.fast_generate(inputs, sampling_params=sampling_params)\n",
    "print(len(responses), len(responses[0].outputs))\n",
    "print(responses[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d62ae",
   "metadata": {},
   "source": [
    "Seems very similar to VLLM, I should do a direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4551c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5cae3ae",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f986f2e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
