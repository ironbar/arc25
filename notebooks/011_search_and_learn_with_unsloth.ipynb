{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03b2969",
   "metadata": {},
   "source": [
    "# Search and learn with unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990134f",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5425e",
   "metadata": {},
   "source": [
    "1. Learn to use unsloth\n",
    "2. See how viable is to use it for search and learn\n",
    "3. Compare speed with other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959ea64",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d88e4",
   "metadata": {},
   "source": [
    "- https://docs.unsloth.ai/\n",
    "- https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\n",
    "- https://docs.unsloth.ai/basics/reinforcement-learning-rl-guide\n",
    "- Inference with LoRA:\n",
    "  - https://github.com/unslothai/unsloth/issues/2009\n",
    "  - https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5733e34",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb3eb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 13:52:45,650 - arc25.utils - INFO - set_cuda_visible_devices_to_least_used_gpu_if_undefined - CUDA_VISIBLE_DEVICES is already set to 0, not changing it.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from arc25.utils import set_cuda_visible_devices_to_least_used_gpu_if_undefined\n",
    "from arc25.logging import configure_logging, logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "set_cuda_visible_devices_to_least_used_gpu_if_undefined()\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import random\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.utils import load_arc_dataset_with_solutions\n",
    "from arc25.data_augmentation import apply_data_augmentation, get_random_data_augmentation_params\n",
    "from arc25.prompting import Template\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d95dc",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e31a8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe move this to the prompting module\n",
    "# https://huggingface.co/barc0/Llama-3.1-ARC-Potpourri-Induction-8B\n",
    "system_prompt = \"\"\"You are a world-class puzzle solver with exceptional pattern recognition skills and expertise in Python programming. Your task is to analyze puzzles and provide Python solutions.\"\"\"\n",
    "\n",
    "prompt_template_text = \"\"\"Given input-output grid pairs as reference examples, carefully observe the patterns to predict the output grid for new test input. Each pair follows the same transformation rule. Grids are 2D arrays represented as strings, with cells (colors) separated by spaces and rows by newlines.\n",
    "Here are the input and output grids for the reference examples:\n",
    "{% for sample in train_samples %}Example {{ loop.index }}\n",
    "Input:\n",
    "{{ sample.input }}\n",
    "\n",
    "Output:\n",
    "{{ sample.output }}\n",
    "\n",
    "{% endfor %}\n",
    "Here is the input grid for the test example:\n",
    "{{ test }}\n",
    "\n",
    "Write a Python function `transform` that can convert any given input grid to its corresponding output grid based on the pattern observed in the reference examples.\n",
    "\"\"\"\n",
    "\n",
    "# I have verified that all responses start with this prefix\n",
    "common_prefix = \"Let's solve this puzzle using Python code with the common library functions. We'll first reason about the problem and then write the code to solve it. The `transform` function will take the input grid and return the output grid. Here is the Python code with the comments describing how to solve the problem:\\n\" #```python\\nfrom common import *\\n\"\n",
    "\n",
    "prompt_template = Template(prompt_template_text)\n",
    "\n",
    "def create_prompt_from_task(task, grid_encoder, tokenizer, shuffle_train_samples=True):\n",
    "    train_samples = [{'input': grid_encoder.to_text(sample['input']), 'output': grid_encoder.to_text(sample['output'])} for sample in task['train']]\n",
    "    if shuffle_train_samples:\n",
    "        random.shuffle(train_samples)\n",
    "    test_sample = random.choice(task['test'])\n",
    "    render_kwargs = dict(train_samples=train_samples, test=grid_encoder.to_text(test_sample['input']))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt_template.render(**render_kwargs)},\n",
    "                {\"role\": \"assistant\", \"content\": common_prefix}]\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=False,\n",
    "                                            continue_final_message=True,\n",
    "                                            # enable_thinking=False,\n",
    "                                            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c64d8",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14075650",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class cfg:\n",
    "    # base model\n",
    "    model_path: str = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "    load_in_4bit: bool = True\n",
    "    max_seq_length: int = 16000\n",
    "    grid_encoder: str = 'ColorNameEncoder()'\n",
    "    # dataset\n",
    "    dataset_path: str = \"/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json\"\n",
    "    max_epochs: int = 2\n",
    "    use_data_augmentation: bool = True\n",
    "    inference_batch_size: int = 8\n",
    "    predictions_per_epoch: int = 16\n",
    "    training_batch_size: int = 1\n",
    "\n",
    "assert cfg.predictions_per_epoch % cfg.inference_batch_size == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    cfg.model_path, load_in_4bit=cfg.load_in_4bit, max_seq_length=cfg.max_seq_length, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01ee6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400 tasks from /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json\n"
     ]
    }
   ],
   "source": [
    "dataset = load_arc_dataset_with_solutions(cfg.dataset_path)\n",
    "task_ids = list(dataset.keys())\n",
    "print(f\"Loaded {len(dataset)} tasks from {cfg.dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c07afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move parameters to cfg\n",
    "sampling_params = SamplingParams(n=cfg.inference_batch_size, temperature=1.0, top_p=0.95, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b5bf709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 13:50:24,674 - arc25.encoders - INFO - create_grid_encoder - Created `ColorNameEncoder()` as grid encoder\n"
     ]
    }
   ],
   "source": [
    "grid_encoder = create_grid_encoder(cfg.grid_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a7b6b54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The decoder prompt (length 795) is longer than the maximum model length of 256. Make sure that `max_model_len` is no smaller than the number of text tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(prompt)\n\u001b[1;32m     17\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 18\u001b[0m text_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(_output\u001b[38;5;241m.\u001b[39mtoken_ids) \u001b[38;5;28;01mfor\u001b[39;00m _output \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39moutputs) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m text_predictions)\n\u001b[1;32m     20\u001b[0m inference_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/miniconda3/envs/arc25-unsloth/lib/python3.10/site-packages/vllm/utils.py:1196\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1189\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1191\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1192\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1193\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m         )\n\u001b[0;32m-> 1196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arc25-unsloth/lib/python3.10/site-packages/vllm/entrypoints/llm.py:465\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Use default sampling params.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_and_add_requests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguided_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguided_options_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_engine(use_tqdm\u001b[38;5;241m=\u001b[39muse_tqdm)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/miniconda3/envs/arc25-unsloth/lib/python3.10/site-packages/vllm/entrypoints/llm.py:1354\u001b[0m, in \u001b[0;36mLLM._validate_and_add_requests\u001b[0;34m(self, prompts, params, lora_request, prompt_adapter_request, guided_options, priority)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;66;03m# Add requests to the engine.\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts):\n\u001b[0;32m-> 1354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpriority\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arc25-unsloth/lib/python3.10/site-packages/vllm/entrypoints/llm.py:1372\u001b[0m, in \u001b[0;36mLLM._add_request\u001b[0;34m(self, prompt, params, lora_request, prompt_adapter_request, priority)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_add_request\u001b[39m(\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1365\u001b[0m     prompt: PromptType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     priority: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   1370\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1371\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter))\n\u001b[0;32m-> 1372\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arc25-unsloth/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:183\u001b[0m, in \u001b[0;36mLLMEngine.add_request\u001b[0;34m(self, request_id, prompt, params, arrival_time, lora_request, trace_headers, prompt_adapter_request, priority)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_request\u001b[39m(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    173\u001b[0m     request_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Process raw inputs into the request.\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     prompt_str, request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_adapter_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     n \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, SamplingParams) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# Make a new RequestState and queue.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arc25-unsloth/lib/python3.10/site-packages/vllm/v1/engine/processor.py:240\u001b[0m, in \u001b[0;36mProcessor.process_inputs\u001b[0;34m(self, request_id, prompt, params, arrival_time, lora_request, trace_headers, prompt_adapter_request, priority)\u001b[0m\n\u001b[1;32m    233\u001b[0m current_platform\u001b[38;5;241m.\u001b[39mvalidate_request(\n\u001b[1;32m    234\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    235\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    236\u001b[0m     processed_inputs\u001b[38;5;241m=\u001b[39mprocessed_inputs,\n\u001b[1;32m    237\u001b[0m )\n\u001b[1;32m    238\u001b[0m eos_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_preprocessor\u001b[38;5;241m.\u001b[39mget_eos_token_id(lora_request)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m encoder_inputs, decoder_inputs \u001b[38;5;241m=\u001b[39m split_enc_dec_inputs(processed_inputs)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# TODO: Impl encoder-decoder\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arc25-unsloth/lib/python3.10/site-packages/vllm/v1/engine/processor.py:331\u001b[0m, in \u001b[0;36mProcessor._validate_model_inputs\u001b[0;34m(self, inputs, lora_request)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_input(encoder_inputs,\n\u001b[1;32m    328\u001b[0m                                lora_request,\n\u001b[1;32m    329\u001b[0m                                prompt_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/arc25-unsloth/lib/python3.10/site-packages/vllm/v1/engine/processor.py:380\u001b[0m, in \u001b[0;36mProcessor._validate_model_input\u001b[0;34m(self, prompt_inputs, lora_request, prompt_type)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     suggestion \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that `max_model_len` is no smaller than the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of text tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prompt (length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlonger than the maximum model length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_prompt_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The decoder prompt (length 795) is longer than the maximum model length of 256. Make sure that `max_model_len` is no smaller than the number of text tokens."
     ]
    }
   ],
   "source": [
    "task_id = task_ids[0]\n",
    "lora_request = None\n",
    "for epoch in range(cfg.max_epochs):\n",
    "    prompts, data_augmentation_params = [], []\n",
    "    for _ in range(cfg.predictions_per_epoch // cfg.inference_batch_size):\n",
    "        task = dataset[task_id]\n",
    "        if cfg.use_data_augmentation:\n",
    "            params = get_random_data_augmentation_params()\n",
    "            task = apply_data_augmentation(task, **params)\n",
    "        else:\n",
    "            params = None\n",
    "        data_augmentation_params.append(params)\n",
    "        prompt = create_prompt_from_task(\n",
    "            task, grid_encoder=grid_encoder, tokenizer=tokenizer, shuffle_train_samples=True)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    t0 = time.time()\n",
    "    text_predictions = llm.fast_generate(prompts, sampling_params, lora_request=lora_request)\n",
    "    total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "    inference_time = time.time() - t0\n",
    "    logger.info(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06574e",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58306edf",
   "metadata": {},
   "source": [
    "### First steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd13247",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03926447",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_bos_token=True, return_tensors=\"pt\"\n",
    ").to(llm.device)\n",
    "outputs = llm.generate(inputs, max_new_tokens = 64, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d41b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "responses = llm.fast_generate(inputs)\n",
    "print(responses[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9de65",
   "metadata": {},
   "source": [
    "This seems to be much faster, 0.3s vs 1.9s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee9474",
   "metadata": {},
   "source": [
    "Let's see if we can make more predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=2048)\n",
    "responses = llm.fast_generate(inputs, sampling_params=sampling_params)\n",
    "print(len(responses), len(responses[0].outputs))\n",
    "print(responses[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d62ae",
   "metadata": {},
   "source": [
    "Seems very similar to VLLM, I should do a direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3e6fb",
   "metadata": {},
   "source": [
    "### Compare inference speed of VLLM vs unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cce927",
   "metadata": {},
   "source": [
    "Ideally I would see the same speed with both methods, because unsloth uses VLLM under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360f4e8",
   "metadata": {},
   "source": [
    "#### VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=False, tensor_parallel_size=1,\n",
    "               max_model_len=32000, enable_lora=False, max_lora_rank=16):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=0.92,  # Use less GPU memory\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "        enable_lora=enable_lora,\n",
    "        max_lora_rank=max_lora_rank,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(model_path, use_4bit_quantization=True, tensor_parallel_size=1, max_model_len=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f7adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=512, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = llm.generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d8b43",
   "metadata": {},
   "source": [
    "#### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e359b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=512, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = llm.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0615f176",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdfdeb",
   "metadata": {},
   "source": [
    "unsloth has a faster startup of 54s vs 1m51s for VLLM.\n",
    "\n",
    "The table below shows the inference speed in tokens/s when generating 100 tokens per prompt.\n",
    "\n",
    "| method \\ n predictions | 8   | 32  | 128  | 512  |\n",
    "|------------------------|-----|-----|------|------|\n",
    "| VLLM                   | 140 | 512 | 1476 | 1992 |\n",
    "| unsloth                | 138 | 510 | 1454 | 1464 |\n",
    "\n",
    "They are very similar except from the last column, where I believe VLLM is using more VRAM memory than\n",
    "unsloth. This is promising because it opens the door to use unsloth both for training and inference\n",
    "in the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd4c2d6",
   "metadata": {},
   "source": [
    "### Proof of concept of inference, training and inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9233f8",
   "metadata": {},
   "source": [
    "Let's verify that I can do fast inference, train and fast inference again with unsloth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868177a",
   "metadata": {},
   "source": [
    "#### Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(model_path, load_in_4bit=True, max_seq_length=12000, fast_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ba81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = llm.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0b045",
   "metadata": {},
   "source": [
    "#### Finetune on a single sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bab6e",
   "metadata": {},
   "source": [
    "Let's use a single training sample that simply rejects to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc4efe",
   "metadata": {},
   "source": [
    "Now let's add a lora adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "print(prompt)\n",
    "print(len(tokenizer.tokenize(prompt)))\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e521ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "model.save_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf6241",
   "metadata": {},
   "source": [
    "#### Repeat inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.lora.request import LoRARequest\n",
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "lora_request = LoRARequest('LoRA', 1, lora_save_path)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea0567",
   "metadata": {},
   "source": [
    "We need to provide the lora request, otherwise it simply uses the base model.\n",
    "\n",
    "We can also generate the lora request with the `load_lora` method from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2348943",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_request = model.load_lora(lora_save_path)\n",
    "print(lora_request)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474899d1",
   "metadata": {},
   "source": [
    "#### Create a new fresh lora adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unload()\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74cb422",
   "metadata": {},
   "source": [
    "Check that the model is untrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, but maybe tomorrow.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed4af5",
   "metadata": {},
   "source": [
    "Awesome, seems that we can create new LoRAs without problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3222c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/tomorrow\"\n",
    "model.save_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fe350",
   "metadata": {},
   "source": [
    "#### Inference with multiple loras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "lora_request = model.load_lora(lora_save_path)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/tomorrow\"\n",
    "lora_request = model.load_lora(lora_save_path)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b8966",
   "metadata": {},
   "source": [
    "#### Load LoRA and continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5486c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95698eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, but maybe tomorrow.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf11170",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unload()\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54039e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, but maybe tomorrow.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a31060",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30088fb",
   "metadata": {},
   "source": [
    "Let's verify that calling the lora adapter has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "model.load_lora(lora_save_path)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca71a9d9",
   "metadata": {},
   "source": [
    "Now let's try using the adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal\"\n",
    "model.load_adapter(lora_save_path, adapter_name=\"refusal\", is_trainable=True)\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)\n",
    "model.set_adapter(\"refusal\")\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fededa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 10,\n",
    "        learning_rate = 1e-7,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519603ff",
   "metadata": {},
   "source": [
    "Awesome, this seems to be working! Let's train for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e8940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sorry, I can't help with that.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_eos_token=True,\n",
    ")\n",
    "train_dataset = Dataset.from_dict({'text': [prompt]})\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 100,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 50,\n",
    "        learning_rate = 1e-5,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_torch_fused\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd72480",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal-v2\"\n",
    "model.save_lora(lora_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_request = model.load_lora(lora_save_path)\n",
    "print(lora_request)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a chapter about the history of AI.\"}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=False,\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(n=8, temperature=1.0, top_p=0.95, max_tokens=100)\n",
    "t0 = time.time()\n",
    "text_predictions = model.fast_generate(prompt, sampling_params, lora_request=lora_request)\n",
    "total_tokens = sum(sum(len(_output.token_ids) for _output in output.outputs) for output in text_predictions)\n",
    "inference_time = time.time() - t0\n",
    "print(f\"Total tokens generated: {total_tokens}\")\n",
    "print(f\"Time taken: {inference_time:.2f} seconds\")\n",
    "print(f\"Average time per task: {inference_time / len(text_predictions):.2f} seconds\")\n",
    "print(f\"Average tokens per task: {total_tokens / len(text_predictions) / sampling_params.n:.2f} tokens\")\n",
    "print(f\"Average tokens per second: {total_tokens / inference_time:.2f} tokens/second\")\n",
    "for i, output in enumerate(text_predictions):\n",
    "    for j, out in enumerate(output.outputs):\n",
    "        print(f\"Output {i}-{j}: {out.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf2746",
   "metadata": {},
   "source": [
    "This is weird, it seems to have saved the untrained adapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83567727",
   "metadata": {},
   "source": [
    "#### Try another approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unload()\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    llm,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = False, # True or \"unsloth\" for very long context\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "print(model.peft_config.keys())\n",
    "print(model.active_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51989884",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal/adapter_model.safetensors\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1097b5ba",
   "metadata": {},
   "source": [
    "I could use the checkpoints and avoid doing weird tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dadbf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c58817",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal/adapter_model.safetensors\", weights_only=False), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51650101",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.from_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah /mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/*/adapter_model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e26e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model.load_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cac20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_save_path = \"/mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-debug-unsloth/refusal-v2\"\n",
    "model.save_lora(lora_save_path, 'refusal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cae3ae",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f986f2e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25-unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
