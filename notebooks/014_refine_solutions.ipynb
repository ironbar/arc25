{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ad9707",
   "metadata": {},
   "source": [
    "# Refine solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2345dd8",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407b660",
   "metadata": {},
   "source": [
    "Can we use the BARC induction model to refine its incorrect solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f5e00",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867aa133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt, create_refine_prompt\n",
    "from arc25.utils import get_timestamp, load_json, load_arc_dataset_with_solutions, write_json\n",
    "from arc25.data_augmentation import apply_data_augmentation, get_random_data_augmentation_params\n",
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import aggregate_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b8cb8",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc884ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=True, tensor_parallel_size=1,\n",
    "               max_model_len=14500, enable_lora=False, max_lora_rank=16,\n",
    "               gpu_memory_utilization=0.92,\n",
    "               max_num_seqs=128, # default is 256\n",
    "            ):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=gpu_memory_utilization,  # Use less GPU memory\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "        enable_lora=enable_lora,\n",
    "        max_lora_rank=max_lora_rank,\n",
    "        max_num_seqs=max_num_seqs, # default is 256\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccf003",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc3f79",
   "metadata": {},
   "source": [
    "## Do we have enough VRAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e049f6",
   "metadata": {},
   "source": [
    "### VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65072ea1",
   "metadata": {},
   "source": [
    "Check how much memory VLLM requires to make predictions with a sequence lenght of 14500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a416ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "model, tokenizer = load_model(\n",
    "    model_path, use_4bit_quantization=False, tensor_parallel_size=1, gpu_memory_utilization=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a280f",
   "metadata": {},
   "source": [
    "Results for 4-bit quantization:\n",
    "\n",
    "```\n",
    "0.5 memory\n",
    "(EngineCore_DP0 pid=12555) INFO 10-02 17:21:14 [gpu_worker.py:298] Available KV cache memory: 4.88 GiB\n",
    "(EngineCore_DP0 pid=12555) INFO 10-02 17:21:14 [kv_cache_utils.py:864] GPU KV cache size: 40,000 tokens\n",
    "(EngineCore_DP0 pid=12555) INFO 10-02 17:21:14 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 2.76x\n",
    "\n",
    "# 0.75 memory\n",
    "(EngineCore_DP0 pid=12029) INFO 10-02 17:19:16 [gpu_worker.py:298] Available KV cache memory: 10.78 GiB\n",
    "(EngineCore_DP0 pid=12029) INFO 10-02 17:19:16 [kv_cache_utils.py:864] GPU KV cache size: 88,272 tokens\n",
    "(EngineCore_DP0 pid=12029) INFO 10-02 17:19:16 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 6.08x\n",
    "\n",
    "# 0.92 memory\n",
    "(EngineCore_DP0 pid=10241) INFO 10-02 17:11:05 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
    "(EngineCore_DP0 pid=10241) INFO 10-02 17:11:06 [kv_cache_utils.py:864] GPU KV cache size: 121,104 tokens\n",
    "(EngineCore_DP0 pid=10241) INFO 10-02 17:11:06 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 8.35x\n",
    "```\n",
    "\n",
    "Results for unquantized model:\n",
    "\n",
    "```\n",
    "# 0.5 memory -> OOM\n",
    "# 0.7 memory -> OOM\n",
    "\n",
    "# 0.8 memory\n",
    "(EngineCore_DP0 pid=9446) INFO 10-08 14:53:33 [gpu_worker.py:298] Available KV cache memory: 2.61 GiB\n",
    "(EngineCore_DP0 pid=9446) INFO 10-08 14:53:33 [kv_cache_utils.py:864] GPU KV cache size: 21,408 tokens\n",
    "(EngineCore_DP0 pid=9446) INFO 10-08 14:53:33 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 1.48x\n",
    "\n",
    "# 0.9 memory\n",
    "(EngineCore_DP0 pid=10147) INFO 10-08 14:55:46 [gpu_worker.py:298] Available KV cache memory: 4.97 GiB\n",
    "(EngineCore_DP0 pid=10147) INFO 10-08 14:55:46 [kv_cache_utils.py:864] GPU KV cache size: 40,720 tokens\n",
    "(EngineCore_DP0 pid=10147) INFO 10-08 14:55:46 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 2.81x\n",
    "\n",
    "# 0.95 memory\n",
    "(EngineCore_DP0 pid=10628) INFO 10-08 14:57:29 [gpu_worker.py:298] Available KV cache memory: 6.15 GiB\n",
    "(EngineCore_DP0 pid=10628) INFO 10-08 14:57:29 [kv_cache_utils.py:864] GPU KV cache size: 50,368 tokens\n",
    "(EngineCore_DP0 pid=10628) INFO 10-08 14:57:29 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 3.47x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29207d",
   "metadata": {},
   "source": [
    "VLLM seems to allow longer sequence lengths than unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883019ab",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e495bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_path, load_in_4bit=True, max_seq_length=14500,\n",
    "    fast_inference=True, gpu_memory_utilization=0.90,\n",
    "    float8_kv_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "help(FastLanguageModel.from_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7f6a6",
   "metadata": {},
   "source": [
    "```\n",
    "# gpu_memory_utilization=0.70\n",
    "Unsloth: Your GPU cannot handle sequence lengths of 14500 due to limited GPU memory.\n",
    "Unsloth: Your GPU can only handle approximately the maximum sequence length of 14500.\n",
    "Unsloth: vLLM loading /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B with actual GPU utilization = 69.08%\n",
    "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.57 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 7680. Num Sequences = 128.\n",
    "Unsloth: vLLM's KV Cache can use up to 1.01 GB. Also swap space = 6 GB.\n",
    "\n",
    "# gpu_memory_utilization=0.75\n",
    "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.57 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 14500. Num Sequences = 160.\n",
    "Unsloth: vLLM's KV Cache can use up to 2.18 GB. Also swap space = 6 GB.\n",
    "\n",
    "# gpu_memory_utilization=0.90\n",
    "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.57 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 14500. Num Sequences = 201.\n",
    "Unsloth: vLLM's KV Cache can use up to 5.66 GB. Also swap space = 6 GB.\n",
    "```\n",
    "\n",
    "```\n",
    "Kaggle gpu_memory_utilization=0.75, max_seq_length=9674\n",
    "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 22.28 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 9674. Num Sequences = 128.\n",
    "Unsloth: vLLM's KV Cache can use up to 1.27 GB. Also swap space = 6 GB.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d41b4",
   "metadata": {},
   "source": [
    "## Refine predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2303feb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "llm, tokenizer = load_model(\n",
    "    model_path, use_4bit_quantization=False, tensor_parallel_size=1, gpu_memory_utilization=0.95, max_num_seqs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_encoder =  create_grid_encoder('ColorNameEncoder()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36561b22",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_arc_dataset_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/hdd0/Kaggle/arc25/trainings/2025-10-08-generate-predictions-to-refine/64i'\n",
    "df = pd.read_csv(os.path.join(folder, 'metrics.csv'))\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28499c0",
   "metadata": {},
   "source": [
    "Pass rate is below 2%, thus it should be easy to pick predictions that did not solved the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_json(os.path.join(folder, 'results.json.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00037dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions['00576224'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8181dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['00576224'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e701a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_predictions(predictions):\n",
    "    curated_predictions = dict()\n",
    "    for task_id, task_predictions in tqdm(predictions.items(), total=len(predictions), desc=\"Curating predictions\"):\n",
    "        # keep only predictions that generate valid outputs but do not solve the train set completely\n",
    "        task_predictions = [prediction for prediction in task_predictions if 'train_correct_grids' in prediction and prediction['train_correct_grids'] < 1.0]\n",
    "        task_predictions = sorted(task_predictions, key=lambda x: (x['train_correct_grids'], x['train_pixel_score'] ), reverse=True)\n",
    "        unique_task_predictions = []\n",
    "        for prediction in task_predictions:\n",
    "            if prediction['fingerprint'] not in {p['fingerprint'] for p in unique_task_predictions}:\n",
    "                unique_task_predictions.append(prediction)\n",
    "        curated_predictions[task_id] = unique_task_predictions\n",
    "    return curated_predictions\n",
    "\n",
    "curated_predictions = curate_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f481b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(curated_predictions['00576224'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc23685",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(v) for v in curated_predictions.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66fb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_predictions['00576224'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee01036",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(create_prompt_from_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5c439",
   "metadata": {},
   "source": [
    "### Create prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e40e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "prompts_per_task = 8\n",
    "prompts, inference_task_ids, data_augmentation_params = [], [], []\n",
    "for task_id, task_predictions in tqdm(curated_predictions.items(), total=len(curated_predictions), desc=\"Creating prompts\"):\n",
    "    if not task_predictions:\n",
    "        print(f\"No valid predictions for task {task_id}, skipping...\")\n",
    "    for prediction_idx in range(prompts_per_task):\n",
    "        data_augmentation_kwargs = task_predictions[prediction_idx % len(task_predictions)]['data_augmentation_params']\n",
    "        data_augmentation_kwargs['color_map'] = {int(k): v for k, v in data_augmentation_kwargs['color_map'].items()}\n",
    "        augmented_task = apply_data_augmentation(dataset[task_id], **data_augmentation_kwargs)\n",
    "\n",
    "        prompt = create_refine_prompt(\n",
    "            augmented_task, grid_encoder, tokenizer,\n",
    "            task_predictions[prediction_idx % len(task_predictions)]['text_prediction'],\n",
    "            task_predictions[prediction_idx % len(task_predictions)]['output_grids'])\n",
    "        prompts.append(prompt)\n",
    "        inference_task_ids.extend([task_id] * batch_size)\n",
    "        data_augmentation_params.extend([data_augmentation_kwargs] * batch_size)\n",
    "len(prompts), len(inference_task_ids), len(data_augmentation_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a6dd0",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02292bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=batch_size, temperature=1.0, top_p=0.95, max_tokens=1024)\n",
    "generations = llm.generate(prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_predictions = []\n",
    "for generation in generations:\n",
    "    for output in generation.outputs:\n",
    "        text_predictions.append(output.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_runner = CodeRunner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab0618",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = code_runner.run([dataset[task_id] for task_id in inference_task_ids], task_ids=inference_task_ids, text_predictions=text_predictions, data_augmentation_params=data_augmentation_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a67c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id, task_results in results.items():\n",
    "        for result in task_results:\n",
    "            for key in ['input_grids', 'output_grids', 'test_output_grids']:\n",
    "                if key in result:\n",
    "                    result[key] = [grid.tolist() for grid in result[key]]\n",
    "write_json(results, '/mnt/hdd0/Kaggle/arc25/predictions/2025-10-11-refinement/results.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e327fa88",
   "metadata": {},
   "source": [
    "### Compare against the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b75746",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/hdd0/Kaggle/arc25/trainings/2025-10-08-generate-predictions-to-refine/128i'\n",
    "df = pd.read_csv(os.path.join(folder, 'metrics.csv'))\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/hdd0/Kaggle/arc25/trainings/2025-10-08-generate-predictions-to-refine/64i'\n",
    "df = pd.read_csv(os.path.join(folder, 'metrics.csv'))\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_64i = load_json(os.path.join(folder, 'results.json.gz'))\n",
    "predictions_refinement = load_json('/mnt/hdd0/Kaggle/arc25/predictions/2025-10-11-refinement/results.json.gz')\n",
    "combined_predictions = dict()\n",
    "for task_id in predictions_64i.keys():\n",
    "    combined_predictions[task_id] = predictions_64i[task_id] + predictions_refinement.get(task_id, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = aggregate_metrics(combined_predictions)\n",
    "metrics.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222ef44",
   "metadata": {},
   "source": [
    "| initial predictions | refinement predictions | valid code | valid outputs | unique outputs | train_pixel_score | train_correct_grids | train_pass_rate | train_is_correct | test_pixel_score | test_correct_grids | test_pass_rate | test_is_correct | is_correct |\n",
    "|---------------------|------------------------|------------|---------------|----------------|-------------------|---------------------|-----------------|------------------|------------------|--------------------|----------------|-----------------|------------|\n",
    "| 128                 | 0                      | **99.9%**  | 71.7%         | **49.8%**      | 42.1%             | **2.4%**            | **1.6%**        | 16.3%            | 40.9%            | **2.0%**           | **2.0%**       | **23.0%**       | **16.3%**  |\n",
    "| 64                  | 64                     | 99.7%      | **74.0%**     | 43.7%          | **45.8%**         | 2.1%                | 1.1%            | **16.5%**        | **44.4%**        | 1.7%               | 1.6%           | 21.5%           | 16.0%      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7ccce",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1a9e2",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7923c7",
   "metadata": {},
   "source": [
    "- [x] How to create a prompt for refinement. Start with the base prompt, then add the response, then the output and requirement to refine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a324d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
