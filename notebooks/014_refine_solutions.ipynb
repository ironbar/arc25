{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ad9707",
   "metadata": {},
   "source": [
    "# Refine solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2345dd8",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407b660",
   "metadata": {},
   "source": [
    "Can we use the BARC induction model to refine its incorrect solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f5e00",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from arc25.utils import get_least_used_gpu_index\n",
    "from arc25.logging import configure_logging, log_execution_time\n",
    "\n",
    "configure_logging()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())\n",
    "\n",
    "# Add VLLM specific environment variables to avoid common issues\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'False'\n",
    "os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867aa133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "import glob\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_python_code(code):\n",
    "    display(Markdown(f\"```python\\n{code}\\n```\"))\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "from arc25.encoders import create_grid_encoder\n",
    "from arc25.prompting import create_prompt_from_task, pretty_print_prompt, create_refine_prompt\n",
    "from arc25.metrics import pixel_similarity_score, correct_grids_score\n",
    "from arc25.utils import get_timestamp, load_json, load_arc_dataset_with_solutions\n",
    "from arc25.plot import plot_task\n",
    "from arc25.data_augmentation import apply_data_augmentation, revert_data_augmentation, get_random_data_augmentation_params\n",
    "from arc25.code_execution import safe_code_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b8cb8",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc884ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution_time\n",
    "def load_model(model_path, use_4bit_quantization=True, tensor_parallel_size=1,\n",
    "               max_model_len=14500, enable_lora=False, max_lora_rank=16,\n",
    "               gpu_memory_utilization=0.92):\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    cleanup_gpu()\n",
    "    llm = LLM(\n",
    "        model=model_path,\n",
    "        gpu_memory_utilization=gpu_memory_utilization,  # Use less GPU memory\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",  # Use float16 to save memory\n",
    "        tensor_parallel_size=tensor_parallel_size,  # Single GPU\n",
    "        quantization=\"bitsandbytes\" if use_4bit_quantization else None,\n",
    "        enable_prefix_caching=True, # Seems that it is true by default, but let's be explicit\n",
    "        max_model_len=max_model_len,\n",
    "        enable_lora=enable_lora,\n",
    "        max_lora_rank=max_lora_rank,\n",
    "    )\n",
    "    if model_path.endswith('.gguf'):\n",
    "        tokenizer_path = os.path.join(os.path.dirname(model_path), 'tokenizer')\n",
    "    else:\n",
    "        tokenizer_path = model_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    return llm, tokenizer\n",
    "\n",
    "\n",
    "def cleanup_gpu():\n",
    "    \"\"\"Clean up GPU memory before loading VLLM\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccf003",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc3f79",
   "metadata": {},
   "source": [
    "## Do we have enough VRAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e049f6",
   "metadata": {},
   "source": [
    "### VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65072ea1",
   "metadata": {},
   "source": [
    "Check how much memory VLLM requires to make predictions with a sequence lenght of 14500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a416ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "model, tokenizer = load_model(\n",
    "    model_path, use_4bit_quantization=False, tensor_parallel_size=1, gpu_memory_utilization=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a280f",
   "metadata": {},
   "source": [
    "Results for 4-bit quantization:\n",
    "\n",
    "```\n",
    "0.5 memory\n",
    "(EngineCore_DP0 pid=12555) INFO 10-02 17:21:14 [gpu_worker.py:298] Available KV cache memory: 4.88 GiB\n",
    "(EngineCore_DP0 pid=12555) INFO 10-02 17:21:14 [kv_cache_utils.py:864] GPU KV cache size: 40,000 tokens\n",
    "(EngineCore_DP0 pid=12555) INFO 10-02 17:21:14 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 2.76x\n",
    "\n",
    "# 0.75 memory\n",
    "(EngineCore_DP0 pid=12029) INFO 10-02 17:19:16 [gpu_worker.py:298] Available KV cache memory: 10.78 GiB\n",
    "(EngineCore_DP0 pid=12029) INFO 10-02 17:19:16 [kv_cache_utils.py:864] GPU KV cache size: 88,272 tokens\n",
    "(EngineCore_DP0 pid=12029) INFO 10-02 17:19:16 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 6.08x\n",
    "\n",
    "# 0.92 memory\n",
    "(EngineCore_DP0 pid=10241) INFO 10-02 17:11:05 [gpu_worker.py:298] Available KV cache memory: 14.78 GiB\n",
    "(EngineCore_DP0 pid=10241) INFO 10-02 17:11:06 [kv_cache_utils.py:864] GPU KV cache size: 121,104 tokens\n",
    "(EngineCore_DP0 pid=10241) INFO 10-02 17:11:06 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 8.35x\n",
    "```\n",
    "\n",
    "Results for unquantized model:\n",
    "\n",
    "```\n",
    "# 0.5 memory -> OOM\n",
    "# 0.7 memory -> OOM\n",
    "\n",
    "# 0.8 memory\n",
    "(EngineCore_DP0 pid=9446) INFO 10-08 14:53:33 [gpu_worker.py:298] Available KV cache memory: 2.61 GiB\n",
    "(EngineCore_DP0 pid=9446) INFO 10-08 14:53:33 [kv_cache_utils.py:864] GPU KV cache size: 21,408 tokens\n",
    "(EngineCore_DP0 pid=9446) INFO 10-08 14:53:33 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 1.48x\n",
    "\n",
    "# 0.9 memory\n",
    "(EngineCore_DP0 pid=10147) INFO 10-08 14:55:46 [gpu_worker.py:298] Available KV cache memory: 4.97 GiB\n",
    "(EngineCore_DP0 pid=10147) INFO 10-08 14:55:46 [kv_cache_utils.py:864] GPU KV cache size: 40,720 tokens\n",
    "(EngineCore_DP0 pid=10147) INFO 10-08 14:55:46 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 2.81x\n",
    "\n",
    "# 0.95 memory\n",
    "(EngineCore_DP0 pid=10628) INFO 10-08 14:57:29 [gpu_worker.py:298] Available KV cache memory: 6.15 GiB\n",
    "(EngineCore_DP0 pid=10628) INFO 10-08 14:57:29 [kv_cache_utils.py:864] GPU KV cache size: 50,368 tokens\n",
    "(EngineCore_DP0 pid=10628) INFO 10-08 14:57:29 [kv_cache_utils.py:868] Maximum concurrency for 14,500 tokens per request: 3.47x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29207d",
   "metadata": {},
   "source": [
    "VLLM seems to allow longer sequence lengths than unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fcb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883019ab",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e495bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "\n",
    "llm, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_path, load_in_4bit=True, max_seq_length=14500,\n",
    "    fast_inference=True, gpu_memory_utilization=0.90,\n",
    "    float8_kv_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "help(FastLanguageModel.from_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7f6a6",
   "metadata": {},
   "source": [
    "```\n",
    "# gpu_memory_utilization=0.70\n",
    "Unsloth: Your GPU cannot handle sequence lengths of 14500 due to limited GPU memory.\n",
    "Unsloth: Your GPU can only handle approximately the maximum sequence length of 14500.\n",
    "Unsloth: vLLM loading /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B with actual GPU utilization = 69.08%\n",
    "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.57 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 7680. Num Sequences = 128.\n",
    "Unsloth: vLLM's KV Cache can use up to 1.01 GB. Also swap space = 6 GB.\n",
    "\n",
    "# gpu_memory_utilization=0.75\n",
    "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.57 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 14500. Num Sequences = 160.\n",
    "Unsloth: vLLM's KV Cache can use up to 2.18 GB. Also swap space = 6 GB.\n",
    "\n",
    "# gpu_memory_utilization=0.90\n",
    "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 23.57 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 14500. Num Sequences = 201.\n",
    "Unsloth: vLLM's KV Cache can use up to 5.66 GB. Also swap space = 6 GB.\n",
    "```\n",
    "\n",
    "```\n",
    "Kaggle gpu_memory_utilization=0.75, max_seq_length=9674\n",
    "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 22.28 GB.\n",
    "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 9674. Num Sequences = 128.\n",
    "Unsloth: vLLM's KV Cache can use up to 1.27 GB. Also swap space = 6 GB.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d41b4",
   "metadata": {},
   "source": [
    "## Refine predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2303feb",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "model, tokenizer = load_model(\n",
    "    model_path, use_4bit_quantization=False, tensor_parallel_size=1, gpu_memory_utilization=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36561b22",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8294e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "grid_encoder =  create_grid_encoder('ColorNameEncoder()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf58a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_arc_dataset_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/hdd0/Kaggle/arc25/trainings/2025-10-08-generate-predictions-to-refine/64i'\n",
    "df = pd.read_csv(os.path.join(folder, 'metrics.csv'))\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28499c0",
   "metadata": {},
   "source": [
    "Pass rate is below 2%, thus it should be easy to pick predictions that did not solved the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = load_json(os.path.join(folder, 'results.json.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00037dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions['00576224'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8181dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['00576224'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee01036",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(create_prompt_from_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5c439",
   "metadata": {},
   "source": [
    "### Create prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a063ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = '00576224'\n",
    "prediction_idx = 1\n",
    "data_augmentation_kwargs = predictions[task_id][prediction_idx]['data_augmentation_params']\n",
    "data_augmentation_kwargs['color_map'] = {int(k): v for k, v in data_augmentation_kwargs['color_map'].items()}\n",
    "augmented_task = apply_data_augmentation(dataset[task_id], **data_augmentation_kwargs)\n",
    "\n",
    "prompt = create_refine_prompt(augmented_task, grid_encoder, tokenizer,\n",
    "                              predictions[task_id][prediction_idx]['text_prediction'],\n",
    "                              predictions[task_id][prediction_idx]['output_grids'])\n",
    "pretty_print_prompt(prompt, default_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1a9e2",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7923c7",
   "metadata": {},
   "source": [
    "- [ ] How to create a prompt for refinement. Start with the base prompt, then add the response, then the output and requirement to refine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a324d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
