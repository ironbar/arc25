{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ad9707",
   "metadata": {},
   "source": [
    "# Refine solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2345dd8",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407b660",
   "metadata": {},
   "source": [
    "Can we use the BARC induction model to refine its incorrect solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f5e00",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867aa133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_python_code(code):\n",
    "    display(Markdown(f\"```python\\n{code}\\n```\"))\n",
    "\n",
    "from arc25.plot import plot_task, plot_grid\n",
    "from arc25.utils import get_timestamp, load_json, load_arc_dataset_with_solutions, write_json\n",
    "from arc25.data_augmentation import apply_data_augmentation, get_random_data_augmentation_params\n",
    "from arc25.parallel_code_execution import CodeRunner\n",
    "from arc25.metrics import aggregate_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d60f5d",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_curate_predictions(folder):\n",
    "    predictions = load_json(os.path.join(folder, 'results.json.gz'))\n",
    "    for task_id, task_predictions in predictions.items():\n",
    "        # fix color maps in the first place\n",
    "        for prediction in task_predictions:\n",
    "            if 'data_augmentation_params' in prediction:\n",
    "                prediction['data_augmentation_params']['color_map'] = {int(k): v for k, v in prediction['data_augmentation_params']['color_map'].items()}\n",
    "            if 'train_is_correct' in prediction:\n",
    "                prediction['is_correct'] = prediction['train_is_correct'] and prediction['test_is_correct']\n",
    "            else:\n",
    "                prediction['is_correct'] = 0\n",
    "        # sort by train score\n",
    "        task_predictions = sorted(task_predictions, key=lambda x: (x.get('train_correct_grids', -1), x.get('train_pixel_score', -1)), reverse=True)\n",
    "        predictions[task_id] = task_predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_task_performance(task_id, index=0):\n",
    "    task = dataset[task_id]\n",
    "    task_predictions = predictions[task_id]\n",
    "    prediction = task_predictions[index]\n",
    "\n",
    "    display(df[df.index == task_id])\n",
    "    plot_task_metrics(task_predictions)\n",
    "    print_prediction_summary(prediction)\n",
    "    plot_task_and_predictions(apply_data_augmentation(task, **prediction.get('data_augmentation_params', None)), prediction['output_grids'])\n",
    "    display_python_code(prediction['code'])\n",
    "\n",
    "\n",
    "def plot_task_metrics(task_predictions):\n",
    "    metric_groups = [\n",
    "        ['train_correct_grids', 'test_correct_grids', 'is_correct'],\n",
    "        ['train_pixel_score', 'test_pixel_score'],\n",
    "    ]\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for plot_idx, metrics in enumerate(metric_groups, 1):\n",
    "        plt.subplot(1, len(metric_groups), plot_idx)\n",
    "        for metric in metrics:\n",
    "            values = []\n",
    "            for pred in task_predictions:\n",
    "                if metric in pred:\n",
    "                    values.append(pred[metric])\n",
    "            plt.hist(values, bins=np.linspace(0, 1, 20), alpha=0.5, label=metric, density=True)\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_prediction_summary(prediction):\n",
    "    metrics = ['train_correct_grids', 'test_correct_grids', 'is_correct'\n",
    "               'train_pixel_score', 'test_pixel_score']\n",
    "    relevant_metrics = {metric: prediction.get(metric, None) for metric in metrics}\n",
    "    display_python_code(f\"Metrics: {relevant_metrics}\")\n",
    "\n",
    "\n",
    "def plot_task_and_predictions(task, output_grids):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plot_task(task, n_rows=3)\n",
    "    for plot_idx, grid in enumerate(output_grids, 1):\n",
    "        plt.subplot(3, len(output_grids), plot_idx + len(output_grids)*2)\n",
    "        plot_grid(grid)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e45f07",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b01ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_arc_dataset_with_solutions('/mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a0f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/hdd0/Kaggle/arc25/trainings/2025-10-08-generate-predictions-to-refine/128i'\n",
    "df = pd.read_csv(os.path.join(folder, 'metrics.csv'), index_col=0)\n",
    "predictions = load_and_curate_predictions(folder)\n",
    "display(df.tail(1))\n",
    "df = df.head(len(df) - 1)\n",
    "df.sort_values(['is_correct', 'train_correct_grids'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652dc430",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_task_performance(df.index[10], index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ab52a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
