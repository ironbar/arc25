{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"arc25","text":""},{"location":"#arc25","title":"arc25","text":"<p>Guillermo Barbadillo's solution for ARC25 challenge</p> <p>\ud83d\udc49 Solution summary</p> <p></p>"},{"location":"01_Business_Understanding/","title":"Business Understanding","text":""},{"location":"01_Business_Understanding/#business-understanding","title":"Business Understanding","text":""},{"location":"01_Business_Understanding/#challenge-description","title":"Challenge description","text":"<p>In this competition, you\u2019ll develop AI systems to efficiently learn new skills and solve open-ended problems, rather than depend exclusively on systems trained with extensive datasets. The top submissions will show improvement toward human level reasoning.</p> <p>Create a system that is able to efficiently learn to do new tasks. It's all about efficiency because we don't have as much compute as OpenAI had when they announced o3.</p>"},{"location":"01_Business_Understanding/#evaluation","title":"Evaluation","text":"<p>This competition evaluates submissions on the percentage of correct predictions. For each task, you should make 2 attempts to predict the exact outputs for every test input grid contained in the task. (Tasks can have more than one test input that needs a predicted output.) Each task test output has one ground truth. For a given task output, any of the 2 predicted outputs matches the ground truth exactly, you score 1 for that task test output, otherwise 0. The final score is the sum averaged of the highest score per task output divided by the total number of task test outputs.</p>"},{"location":"01_Business_Understanding/#assess-situation","title":"Assess situation","text":"<p>The challenge ends on 4-11-2025, so we have 7 months to try new ideas. The grand prize is 700k\\(, whereas the progress prize would be 75k\\) in the best case scenario (25k$ first prize and 50k$ best paper). So the prizes are designed to encourage the development of new and bold approaches. I should mainly work on novel and promising ideas.</p> <p>At Veridas I can devote half of the time to the ARC challenge, and use the compute resources from the cluster. I could also use up to 15k$ in cloud computing if needed.</p> <p>We can make one submission a day that it is worth 50$ in compute, that would be around 10k$ for all the competition. I believe I should use it as much as possible to gain information. Maybe I could use the development set as my test set and the public test set as my validation set. However be aware that making a lot of submissions could prevent joining other people later in the competition.</p>"},{"location":"02_Data_Understanding/","title":"Data Understanding","text":""},{"location":"02_Data_Understanding/#data-understanding","title":"Data Understanding","text":""},{"location":"02_Data_Understanding/#collect-initial-data","title":"Collect initial data","text":""},{"location":"02_Data_Understanding/#external-data","title":"External data","text":""},{"location":"02_Data_Understanding/#describe-data","title":"Describe data","text":""},{"location":"02_Data_Understanding/#arc-agi-2","title":"ARC-AGI-2","text":"<ul> <li>Addressing Flaws: Removed task that were susceptible to brute force search from the evaluation and test set (50% of the test tasks could be solved with an ensemble from 2020), also removed tasks with contamination from training tasks.</li> <li>Compositional Tasks: ARC-v2 features compositional tasks with multiple interacting rules, making it harder for brute-force methods. </li> <li>Solvability: All the tasks are solved at least but 2 persons out of a maximum of 10, and the average solving rate is 60%.</li> <li>Human Calibration Study: A formal human calibration study was conducted to assess how humans perform on the tasks. All the evaluation and tests sets should have a similar difficulty.</li> <li>The new training dataset has 1000 tasks, it has almost all the previous ARC-AGI-1 tasks as shown in this notebook.</li> <li>Not adversarial with ARC24 models. Although we see a huge drop in accuracy compared to ARC-AGI-1, this is caused by the higher complexity of the tasks. They could have made o3 to score 0 but they didn't do it.</li> </ul>"},{"location":"02_Data_Understanding/#explore-data","title":"Explore data","text":""},{"location":"02_Data_Understanding/#verify-data-quality","title":"Verify data quality","text":""},{"location":"02_Data_Understanding/#amount-of-data","title":"Amount of data","text":""},{"location":"03_State_of_the_art/","title":"State of the art","text":""},{"location":"03_State_of_the_art/#state-of-the-art","title":"State of the art","text":"<p>I'm going to recap all the learnings from the previous ARC24 challenge. I would also add new and relevant papers during the ARC25 competition.</p>"},{"location":"03_State_of_the_art/#learnings-from-arc24","title":"Learnings from ARC24","text":"<ul> <li>Reasoning models trained with RL like <code>o3</code> can solve ARC-AGI-1, but they need a lot of compute. That is around x40000 times the compute allowed in the competition ($8 vs $340k). However on ARC-AGI-2 they seem to be scoring below 5%.</li> <li>Test-time training is crucial to improve the accuracy of transduction models. In my case the score improves from 11 to 33.</li> <li>Frontier LLMs can generate code that solves more than half of the semi-private ARC set.</li> <li>Induction and transduction are complementary approaches. It would have sense to first try with induction (which has higher guarantees) and use transduction only if induction fails.</li> <li>LLMs struggle with tasks that have big grids, however the fact that <code>o3</code> can solve ARC might hint that a 2d representation for the grid is not needed.</li> </ul>"},{"location":"03_State_of_the_art/#openai-solved-the-arc-challenge-with-a-tuned-version-of-o3","title":"OpenAI solved the ARC challenge with a tuned version of <code>o3</code>","text":"<p>Details are not public, but it is very likely that <code>o3</code> is trained just with reinforcement learning like DeepSeek's <code>r1</code>. When o1 was announced they said:</p> <p>Our large-scale reinforcement learning algorithm teaches the model how to think productively using its chain of thought in a highly data-efficient training process.</p> <p>This sounds very similar to the training process described in the <code>r1</code> paper. The only different thing is that in the table with the results there is a <code>samples</code> field that is 6 for low compute and 1024 for high compute. If we compare the numbers it seems that it is the number of times each task was tried to be solved. So there must be an aggregation mechanism to combine the responses from all the runs. It could be as simple as a voting mechanism and as complex as the model receiving as input the responses and choosing.</p> <p>On average it uses 55k tokens per run. For 100 tasks that would be 5.5M output tokens if each task is run only once. If we are allowed 12 hours for the submission that would require an output speed of 127 token/second. That might be possible for a model like Qwen2.5 1.5B, probably not for a model like Llama3 8B.</p> <p>So in theory we could take a reasoning model such as deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B and fine-tune it to do ARC tasks using RL. We could reward the model for creating shorter and correct answers. OpenAI also offers a service to do \"Reinforcement Fine-tuning\". </p> <p>Notice that OpenAI has decided not to release <code>o3</code>, so we don't know how necessary the tuning for ARC was needed.</p> <p>Mikel Bobel-Irizar did an awesome analysis of the effect of task length on the accuracy of <code>o3</code>. We could use upscaling as data augmentation so the model learns to work with bigger images. There is also another blogpost with the unsolved evaluation tasks.</p>"},{"location":"03_State_of_the_art/#other-reasoning-models","title":"Other reasoning models","text":"<p>There are other reasoning models such as <code>r1</code> and Sonnet 3.7 but none of them achieve as high results as OpenAI's model. That does not happen in other fields such as mathematics, so probably OpenAI is using some 2d data for its RL training.</p> <p>Interestingly in the results we can see that <code>r1</code> uses 6-11k tokens to solve each task. That is between 5 and 10 times less than <code>o3</code>.</p>"},{"location":"03_State_of_the_art/#test-time-training-ttt","title":"Test-time training (TTT)","text":"<p>Test-time training was arguably the biggest discovery of ARC24 challenge. In retrospective it is clear that if intelligence is all about adaptation to novelty, then we should not keep the models frozen but let them adapt to do new tasks. The MindsAI team found this approach but they decided not to make their solution public.</p> <p>Probably the best implementation and description was done by the Architects. There is also a paper named The Surprising Effectiveness of Test-Time Training for Abstract Reasoning and my own solution also used TTT.</p> <p>Update: The MindsAI team has published a paper describing their approach.</p>"},{"location":"03_State_of_the_art/#transduction-and-induction","title":"Transduction and induction","text":"<p>This paper defined the terms transduction (generating the output grid directly) and induction (writing code to solve the tasks) and showed they were complimentary. Additionally they generated 400k new tasks using LLMs, showing that is possible to augment the data.</p> <p>The code is open-source and I should take a look at it, it could serve as inspiration for creating the DSL.</p> <p>Notice that they are able to generate new tasks using LLMs because they work directly with code, not with the grid images. So they switch the modality from image to text and that way are able to harness the power of LLMs. They used gpt-4o-mini for generation, so today we could use more powerful models for more diverse and complex tasks.</p>"},{"location":"03_State_of_the_art/#code-generation-search","title":"Code generation (Search)","text":"<p>Different attempts have tried using LLMs to generate python code to solve the ARC tasks. This induction approach has the advantage that the functions can be verified, whereas output grids from the transduction approach cannot be verified. This allows to generate thousands of candidate solutions and filter all those that do not generate correct outputs for the training samples. The main differences between this methods is how the model is prompted to generate the responses.</p> <ul> <li>Summary of the progress in the public leaderboard in 2024</li> <li>Jeremy Berman uses an approach similar to FunSearch</li> <li>Ryan Greenblatt was the first to show that this approach could work and how it scaled with the number of predictions.</li> <li>Eric Pang scored 77.1% on ARC-AGI-1 and 26% on ARC-AGI-2 using a code generation approach similar to FunSearch.</li> </ul>"},{"location":"03_State_of_the_art/#inference-time-scaling-and-collective-intelligence-for-frontier-ai","title":"Inference-Time Scaling and Collective Intelligence for Frontier AI","text":"<p>Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to \"go wider\" by expanding new candidate responses or \"go deeper\" by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS consistently outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling.</p> <p></p> <p></p> <p>This paper is interesting because clearly explains the challenges of search: go wider or deeper? Their search strategy learns to decide which LLM to use and wether to go wider or deeper for each ARC problem.</p> <p>One worrying thing is that even when generating code, they pass@250 of 30% goes down to 19% when doing pass@2. I thought that selecting the correct code was more or less trivial, but does not seem to be the case.</p>"},{"location":"03_State_of_the_art/#funsearch-and-alphaevolve","title":"FunSearch and AlphaEvolve","text":"<p>These two papers describe an evolutionary method where LLMs write code to optimize some function. Funsearch used small LLMs that were sampled millions of times, whereas AlphaEvolve uses frontier models that only need in the order of thousand calls to optimize the task.</p> <p>On each new generation the LLM receives the context of the problem, and previous code generation with their scores.</p> <p>This method shows how we can use search with LLMs to discover new solutions to problems. The main difference with ARC is that AlphaEvolve requires a continuous metric (f.e. execution time, memory usage...) and on ARC we have sparse rewards. AlphaEvolve requires the continuous metric to be able to evolve the population (they used an island-based method).</p> <p>Finally they use multiple prompts to increase diversity in the responses.</p>"},{"location":"03_State_of_the_art/#jeremy-berman-high-score-on-arc-agi-2","title":"Jeremy Berman high score on ARC-AGI-2","text":"<p>The code is public.</p> <p>score_instructions_on_challenge records per-example results, calculates a simple cell-wise similarity score</p> <p>I'm not sure if cell-wise similarity is a good guiding metrics. I believe that checking if a grid is correct is a better option, however if none of the grids are correct it might be useful.</p> <p>I used the same Evolutionary Test-Time Compute architecture as my v1 solution but replaced Python functions with plain English instructions. My original solution used language models to generate Python functions to solve tasks. This approach had a key advantage: functions are deterministic and testable. I could generate hundreds of candidate functions, rank them by their performance on training examples, and evolve better solutions from the highest-scoring ones. This strategy hits a wall with ARC v2. The transformations are often too complex to express elegantly in Python\u2014they require nuanced pattern recognition and contextual understanding that would result in unwieldy, brittle code. So I turned to a language much older than Python: English.</p> <p>When we use natural language the model becomes the \"python interpreter\". If we have access to a model like Grok-4 that might be an option, but I don't believe we have a similar small open-source model. I still believe that python code with the right set of primitive functions is the way to go.</p>"},{"location":"03_State_of_the_art/#reasoning-code-and-rl","title":"Reasoning, code and RL","text":""},{"location":"03_State_of_the_art/#self-improving-language-models-for-evolutionary-program-synthesis-a-case-study-on-arc-agi","title":"Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI","text":"<p>Many program synthesis tasks prove too challenging for even state-of-the-art language models to solve in single attempts. Search-based evolutionary methods offer a promising alternative by exploring solution spaces iteratively, but their effectiveness remain limited by the fixed capabilities of the underlying generative model. We propose SOAR, a method that learns program synthesis by integrating language models into a self-improving evolutionary loop. SOAR alternates between an evolutionary search that uses an LLM to sample and refine candidate solutions, and a hindsight learning phase that converts search attempts into valid problem-solution pairs used to fine-tune the LLM's sampling and refinement capabilities\u2014enabling increasingly effective search in subsequent iterations. On the challenging ARC-AGI benchmark, SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52\\% of the public test set.</p> <p></p> <p></p> <ul> <li>It does not use a DSL, writes all the python code directly. I believe that the DSL allows for shorter programs, enabling faster and easier exploration. Maybe we can give the interface of the DSL functions as input to the model.</li> <li>Search is mostly independent, they do 3k samples in the first place and then try to refine those. I believe it is more efficient to have a more global view when searching.</li> <li>One thing that worries me is that the improvement of test-time training is small, just 3-5%. There is no table that makes this explicit</li> <li>It seems that no data augmentation was used, this could improve the results slightly but won't be game-changer</li> <li>It is beneficial to use data from all the models, diversity is crucial</li> <li>Bigger models get better scores</li> <li>Remember that closed frontier models could score higher than 50% just with search</li> <li>They use sglang for inference</li> <li>By looking at the code it seems that they use <code>temperature=1.0</code> for inference</li> </ul> <p>I believe this paper is in the right direction. It uses code, hindsight relabelling and combines search and learning. Why it does not solve ARC?</p> <ul> <li>Maybe a tighter integration between search and learning is needed, a more integral approach.</li> <li>Using a DSL could also have a big effect, but probably not enough to solve ARC</li> </ul>"},{"location":"03_State_of_the_art/#codeit-self-improving-language-models-with-prioritized-hindsight-replay","title":"CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay","text":"<p>This is a very interesting paper that uses code and hindsight experience replay. They use Hodel's DSL as a start point but they apply mutation to augment the tasks.</p> <p>This paper shows that it's possible to learn from the test set using hindsight replay. How can we improve it?</p> <ul> <li>Using a bigger and better model, they use a small 220M LLM, we could be using a 7B parameter model</li> <li>Fine-tune the model individually for each task</li> <li>Do the search first on the training tasks to generate more training data</li> <li>More data augmentation</li> <li>Use a more simple and minimal DSL</li> </ul>"},{"location":"03_State_of_the_art/#rlef-grounding-code-llms-in-execution-feedback-with-reinforcement-learning","title":"RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning","text":"<p>In this paper they train the models to use effectively the feedback from code execution by using reinforcement learning. </p> <ul> <li>They train on 13k problems, an order of magnitude higher than ARC. The model is updated 12k times, each update is done with a batch size of 256. </li> <li>The model is given 3 attempts to solve the tasks. </li> <li>Only the final response is considered to compute the reward</li> <li>Trained took 5700 A100 GPU hours (20*288), that is around $10k. If I can work an order of magnitude below I would be fine. </li> <li>The 70B model roughly doubles the performance of the 8B model. </li> <li>Their implementation of SFT does not match the results from RL (This contradicts the R1 paper)</li> </ul>"},{"location":"03_State_of_the_art/#deepseek-r1-incentivizing-reasoning-capability-in-llms-via-reinforcement-learning","title":"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning","text":"<p>They show that the model can develop capabilities such as self-verification, reflection  just with RL, without the need of SFT.</p> <p>One interesting finding is that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models.</p>"},{"location":"03_State_of_the_art/#improving-multi-turn-tool-use-with-reinforcement-learning","title":"Improving Multi-Turn Tool Use with Reinforcement Learning","text":"<p>Bespoke Labs employed Group Relative Policy Optimization (GRPO) to train Qwen2.5-7B-Instruct on just 100 examples from the BFCL benchmark, enhancing multi-turn tool use performance by 23% without relying on human or teacher demonstrations. This approach enabled the agent to learn complex tool orchestration\u2014such as sequencing API calls for tasks like flight booking\u2014through reinforcement learning guided solely by outcome-based rewards.</p> <p>They trained for around 15 hours on 4xH200s. source</p>"},{"location":"03_State_of_the_art/#retool-reinforcement-learning-for-strategic-tool-use-in-llms","title":"ReTool: Reinforcement Learning for Strategic Tool Use in LLMs","text":"<p>ReTool is a reinforcement learning framework that equips LLMs with the ability to strategically invoke tools through multi-turn code execution during reasoning. By integrating real-time code execution into the learning loop, ReTool not only improves performance on complex tasks like the AIME math benchmark but also achieves significantly more efficient training compared to text-only RL methods. The model autonomously learns when and how to use code, resulting in concise, accurate reasoning and emergent behaviors such as code self-correction.</p>"},{"location":"03_State_of_the_art/#does-reinforcement-learning-really-incentivize-reasoning-capacity-in-llms-beyond-the-base-model","title":"Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?","text":"<p>This paper shows that if we look at <code>pass@k</code> metric the base model outperform RL fine-tuned models when we use big <code>k</code> values. This shows that the base model has higher variance than the fine-tuned one, so if we sample long enough we will eventually get some correct answer.</p> <p>The advantage of RL is that it increases the <code>pass@1</code> metric, which is arguably much more important than the <code>pass@256</code> metric. If we don't have a reliable method to select the correct answer there is no value in generating a correct answer among a lot of incorrect ones.</p> <p>It is likely that in the future better exploration methods during RL might change these results. Otherwise the models will be constrained by the goodness of the base model.</p>"},{"location":"03_State_of_the_art/#absolute-zero-reinforced-self-play-reasoning-with-zero-data","title":"Absolute Zero Reinforced Self-play Reasoning with Zero Data","text":"<p>This paper shows that it is possible to train with reinforcement learning a model to both propose and solve tasks if we have access to an external verifier such a python interpreter. The goal of the proposer is to generate tasks that are neither too easy or too hard, maximizing learnability. One interesting thing is that they have triplets of inputs, outputs and program, and they teach the model to predict any of the elements of the triplet, in a very similar way to my omni-arc approach. So they do induction (program), deduction (output) and abduction (inputs).</p> <p>Although they train for 500 steps, it seems that the biggest improvement is due to the first 100 steps.</p> <p>Bigger models benefit more from this technique. Probably they can generate more diverse training samples. This agrees with the findings in the DeepSeek R1 paper: use RL on a big model and distill to smaller models.</p> <p>They check for program safety and determinism, I should also do that.</p>"},{"location":"03_State_of_the_art/#llms-for-engineering-teaching-models-to-design-high-powered-rockets","title":"LLMs for Engineering: Teaching Models to Design High Powered Rockets","text":"<p>Very cool paper that shows how test-time RL can help an LLM to optimize rocket designs given access to a simulation environment and a continuous reward.</p> <p>Similar to AlphaEvolve in spirit, but using a much smaller LLM that evolves over time.</p> <p>Is there room for a platform that helps users do this kind of things?</p>"},{"location":"03_State_of_the_art/#learning-from-sparse-and-binary-rewards","title":"Learning from sparse and binary rewards","text":"<ol> <li>Reward shaping</li> <li>Curiosity: Intrinsic motivation</li> <li>Hindsight Experience Replay</li> <li>Curriculum learning</li> <li>Imitation learning</li> </ol> <p>This search for methods that allow learning from sparse and binary rewards hasn't thrown anything new. I was already aware of HER. Curriculum learning might naturally arise in ARC because the model will likely solve easier tasks first, train on them and then solve more complex tasks. Imitation learning is the basis of the training of an LLM. </p> <p>Reward shaping might be achieved by defining a metric function that isn't binary. However I'm afraid that simple pixel accuracy is not enough: there might be some program that is on the correct direction but has lower accuracy than a program that is in the wrong direction.</p> <p>Curiosity might be used during training, or during search for node exploration. However again it's difficult to write a metric for curiosity.</p> <ul> <li>ChatGPT Deep Research</li> <li>Gemini Deep Research</li> </ul>"},{"location":"03_State_of_the_art/#other","title":"Other","text":""},{"location":"03_State_of_the_art/#arc-agi-without-pretraining","title":"ARC-AGI without pretraining","text":"<p>This novel approach does not use any training data! Scores 4.17 on ARC-AGI-2.</p> <p>We propose that lossless information compression can serve as an effective framework for solving ARC-AGI puzzles. A more efficient (i.e., lower-bit) compression of a puzzle correlates with a more accurate solution.</p> <p>I don't understand the method well but it seems to be trying to create a compressed representation of the task, that is used to generate the output for the test sample.</p>"},{"location":"03_State_of_the_art/#searching-latent-program-spaces","title":"Searching Latent Program Spaces","text":"<p>They use an autoencoder to learn the space of programs. At inference the encoder gives a good starting point, but the gradient is used to find a better task representation. The idea is interesting but the performance is very weak, will have to wait if they are able to make it work at Ndea.</p>"},{"location":"03_State_of_the_art/#a-2d-ngpt-model-for-arc-prize","title":"A 2D nGPT Model for Arc Prize","text":"<p>Interesting because it uses a 2d transformer, not 1d as most of the other solutions.</p>"},{"location":"04_Initial_Plan/","title":"Initial Plan","text":""},{"location":"04_Initial_Plan/#initial-plan","title":"Initial Plan","text":"<p>Since the end of ARC24 competition I have been thinking of how to better approach the challenge. Now that ARC25 has been launched I'm going to describe my initial ideas to try to solve ARC.</p> <p>I have identified two possible paths that converge on the same approach.</p> <p>It's all about efficiency!</p>"},{"location":"04_Initial_Plan/#path-1-combine-the-best-approaches-induction-and-test-time-training","title":"Path 1. Combine the best approaches: Induction and Test-time Training","text":"<p>Last year's competition showed that test-time training allowed the models to adapt to the novel tasks. At the same time in the semi-private dataset we saw that frontier models could generate code to solve more than half of the tasks.</p> <p>Using code is a more promising approach because:</p> <ol> <li>It is verifiable</li> <li>Enables to iteratively refine the solution by comparing the outputs with the ground truth. I would argue that this is similar to reasoning.</li> </ol> <p>My hypothesis is that we can use hindsight experience replay (HER) at test time to update the beliefs of the model and find the right solution more efficiently. Instead of sampling thousands of programs, sample a few and learn from the mistakes. That is the way to combine induction and test-time training.</p> <p>We can treat the failed code attempts that run as new tasks, and train the model on those tasks. Those tasks will be in the neighborhood of the task that we want to solve.</p> <p>We already know that HER enables faster learning, specially in very sparse reward environments.</p> <p></p> <p>Additionally we could define a continuous metric such as the number of correct pixels and use it with reinforcement learning to modify the model towards solutions that score higher.</p>"},{"location":"04_Initial_Plan/#path-2-human-inspired-approach","title":"Path 2. Human inspired approach","text":"<p>When humans try to solve ARC tasks we draw some hypothesis and test it in our heads, if it is not correct we update our beliefs and refine the hypothesis. What modules are needed to do this process?</p> <ul> <li>Policy. What action do I have to do to achieve the goal? Learned with hindsight</li> <li>World model. What happens if do this action? Learned with past experiences</li> <li>Judgment. Is the solution correct? Learned with human feedback or by comparison</li> <li>Learning. In difficult problems we are able to learn from our errors and modify our initial beliefs about the problem.</li> </ul> <p>Reasoning is an iterative process, as shown in the loop diagram in the image.</p>"},{"location":"04_Initial_Plan/#how-o3-solved-arc-agi-1","title":"How o3 solved ARC-AGI-1?","text":"<p>My intuition is that o3 success in ARC-AGI-1 is likely due to an improved policy and better judgment. Vanilla LLMs are not good at judgment, but reasoning models need to learn to know if some answer is correct or wrong. By training with reinforcement learning the model improves its policy, it learns which strategies are good and which are bad to solve the ARC tasks. o3 very likely describes the task with natural language, generates the output grid and checks if the output looks correct. It might try different approaches, refine the description and when it is certain returns the response.</p> <p>The main problems of o3 are:</p> <ul> <li>It generates 55k tokens per run, we probably cannot afford with the compute budget given in Kaggle.</li> <li>Does not seem to generalize well to more complex tasks with interacting rules such as ARC-AGI-2</li> </ul>"},{"location":"04_Initial_Plan/#how-ai-might-solve-arc","title":"How AI might solve ARC?","text":"<p>We can reuse the diagram of how humans solve ARC and replace the elements.</p> <p></p> <p>Focusing on efficiency the best configuration for ARC might be the following:</p> <ul> <li>Policy: a Large Reasoning Model.</li> <li>World model: python interpreter</li> <li>Judgment: metric function</li> <li>Learning: reinforcement learning and hindsight experience replay</li> </ul> <p>That way we only have to learn the policy and parametrize the learning, all the other modules are guaranteed to work perfectly.</p>"},{"location":"04_Initial_Plan/#rl-arc","title":"RL-ARC","text":"<p>The idea is to frame ARC as a reinforcement learning problem. The system is given a new task and it needs to learn it as efficiently as possible. It is like playing a game, but instead of hitting the buttons it has to write code to play. The code generates an output that is evaluated against the ground truth and returns an score.</p> <p>Finding the right program is equivalent to finding the right trajectory to solve a game. Instead of actions we write code, but the problem is exactly the same. When we want to solve a new task in ARC is the same as wanting to solve a new game. We can frame the problem as a Reinforcement learning game, with a very sparse reward.</p> <p>The challenge is how to create a very efficient system to do this: how to design the DSL, how to pre-train the model, how to do split the compute between inference and test-time training... There is a huge number of possibilities that we need to explore to find the winning system.</p> <p>The DSL is perhaps one of the most critic parts of the system. Without a DSL the system will have to write very long programs to solve the task, making the exploration of the solution space much harder and requiring more compute (generating more tokens requires more compute). We have to design a complete yet minimal DSL. Probably the best way to do it is to use an iterative method, growing the DSL when noticing that certain tasks cannot be solved without new primitives.</p>"},{"location":"04_Initial_Plan/#algorithm","title":"Algorithm","text":"<p>While solution is not found:</p> <ol> <li>Generate <code>n</code> python functions given input-output pairs. <code>n=8</code> might be a good parametrization if we apply rotations and transpose to the original task.</li> <li>Do <code>r</code> steps of iterative function refinement</li> <li>Test-time training. The model has generated <code>m</code> python functions that are not correct, but they run and hopefully they are in the right direction. We can treat those functions as new tasks and do test-time training on them (we know the inputs, outputs and code). We could also apply RL techniques such as GRPO.</li> </ol> <p>The model will be given all the task inputs and the available outputs, and will have to write python code that implements the task. By giving all the inputs we force the model to create python code to generalize to inputs that do not have output.</p>"},{"location":"04_Initial_Plan/#model-fine-tuning","title":"Model fine-tuning","text":"<p>The base model already knows how to code. In the fine-tuning phase we want to teach the model:</p> <ul> <li>How to use the primitive functions from the DSL (and how many primitive functions are there)</li> <li>The prior knowledge needed to solve ARC, for example to understand the 2d nature of the grids</li> <li>The intuition needed to solve ARC tasks using code</li> <li>The ability to reason: given the output to the code refine the code to reach the correct solution</li> </ul> <p>All this knowledge will make learning at test time faster. Maybe we could solve all tasks from zero given enough compute and time, but ARC is all about efficiency and this fine-tuning will give the model the building blocks to solve the new tasks faster.</p>"},{"location":"04_Initial_Plan/#how-to-learn-to-reason-from-multi-turn-conversations","title":"How to learn to reason from multi-turn conversations?","text":"<p>Imagine that we have a 3 turn conversation that ends with a correct answer. How can we use that data to teach the model to reason? We want to achieve two objectives: the model should create the correct function to solve the ARC task and if it doesn't it should be able to refine the function iteratively until it is correct.</p> <p>We can create variations of the conversation of 2 and 1 turns.</p> <p>TODO: I need to investigate this matter</p> <p>One option would be to train and all the conversation variants. That could probably work. Deepseek trains on all the chain of thought if the final response is correct.</p> <p>If we only trained the model in the latest response I'm not sure if it will need the reasoning, because the prefix for all the conversations would be the same and it could ignore the intermediate conversation.</p> <p>On an online RL setup we would reward all CoT that generates the correct answer. We could shape the reward to give higher score to shorter answers, favouring efficiency. On an offline setup maybe curriculum learning could help by training first on the longer conversations and finally on the shorter ones.</p>"},{"location":"04_Initial_Plan/#global-diagram","title":"Global diagram","text":"<p>My responsibilities are:</p> <ul> <li>Define a complete yet minimal domain specific language (DSL)</li> <li>Create training data to learn to use the DSL</li> <li>Define all the training hyperparameters: base model, loss, training data...</li> <li>Define the hyperparameters of test</li> </ul> <p>I should not solve the tasks by writing code, the model should be able to solve the tasks given its training and the test setup.</p> <p>This is an iterative process, hopefully a virtuous cycle where improvements in one side of the diagram cause improvements in the other side and vice versa. If this is true I could expect slow progress at the beginning and fast progress at the end.</p> <p>On a first step I have to define a start DSL, create training data and train a first model to start to spin up the wheel.</p>"},{"location":"04_Initial_Plan/#soft-metric","title":"Soft metric","text":"<p>Current ARC metric is binary, maybe using a continuous and softer metric could help learning on some tasks.</p> <pre><code>def soft_arc_metric(y_true, y_pred):\n    if y_true.shape == y_pred.shape:\n        return np.mean(y_true == y_pred) # \u2208 [0, 1]\n    else:\n        return -np.mean(np.abs(np.array(y_true.shape) - np.array(y_pred.shape))/29) # \u2208 [-1, 0)\n</code></pre>"},{"location":"04_Initial_Plan/#tricks","title":"Tricks","text":"<ul> <li>Teach how to use the DSL. It is important to create examples of how to use each function in the DSL. Also learning to combine the primitive functions might be important for ARC-AGI-2 because there is an emphasis on compositionality.</li> <li>Upsampling as data augmentation</li> <li>I can remove words from the tokenizer of a model to simplify grid representation.</li> <li>I could teach the model to draw. Given some painting generate code to create the painting. That might help the model to learn the 2d structure of the grids.</li> <li>Focus on an end to end approach. On ARC24 I lost the focus and mostly worked on pre-training. I should always evaluate the end to end system, although it requires more compute is the right way to do it.</li> <li>Being able to create embeddings with 2d information could be a boost for the model. That needs to be done when pre-training the model.</li> <li>Deepseek-R1 paper describes that for small LLMs it is better to use distillation from bigger models than to use RL.</li> </ul>"},{"location":"05_Solution_Summary/","title":"Solution summary","text":""},{"location":"05_Solution_Summary/#exploring-the-combination-of-search-and-learn-for-the-arc25-challenge","title":"Exploring the combination of search and learn for the ARC25 challenge","text":"<p>Guillermo Barbadillo, November 3, 2025</p>"},{"location":"05_Solution_Summary/#abstract","title":"Abstract","text":"<p>This is a technical report of the work and research done by Guillermo Barbadillo for the ARC25 challenge. Most of the research was oriented towards a deep-learning-guided program synthesis system that searches program space and adapts at test time with test-time training via hindsight relabeling, in a tight search-and-learn loop. Evidence was obtained that search and learn outperforms pure search approaches for the same number of predictions per task. However, that effort is not yet complete, and pieces and ideas are missing, as it does not yet solve any of the private test tasks from ARC-AGI-2. The best result on the leaderboard was achieved with minor adaptations of last year's transduction with test-time training approach.</p>"},{"location":"05_Solution_Summary/#table-of-contents","title":"Table of contents","text":"<ul> <li>Abstract</li> <li>Table of contents</li> <li>Introduction. What is ARC and why is it relevant?</li> <li>Vision: Search and learn<ul> <li>Path 1. Search and learn</li> <li>Path 2. Combine the best approaches from ARC24: test-time training and program synthesis</li> <li>Path 3. Imitate how humans solve ARC<ul> <li>How humans solve ARC</li> <li>How AI might solve ARC</li> </ul> </li> <li>Path 4. Frame ARC as a game and solve it with RL</li> <li>Why search and learn will beat the other approaches<ul> <li>Transduction and test-time training</li> <li>Natural language program search (o3)</li> <li>Program search with frontier models</li> </ul> </li> </ul> </li> <li>Research Journey<ul> <li>1. How does test-time training compare against o3?</li> <li>2. Does hindsight relabeling work for program synthesis on toy tasks?</li> <li>3. Does hindsight relabeling work for program synthesis on ARC tasks?<ul> <li>3.1 Try to train my own models</li> <li>3.2 Experiment with base models</li> <li>3.3 Experiment with BARC induction model<ul> <li>3.3.1 Replicate results from BARC paper</li> <li>3.3.2 Hindsight relabeling and BARC induction model</li> </ul> </li> </ul> </li> <li>4. Can we get a stronger base model with reinforcement learning?</li> <li>5. Can we improve the search accuracy by doing prediction refinement?<ul> <li>5.1 Can the BARC induction model refine its predictions?</li> <li>5.2 Can the BARC induction model learn to refine its predictions using RL?</li> </ul> </li> </ul> </li> <li>Conclusions and next steps</li> <li>Acknowledgements</li> <li>Links</li> </ul>"},{"location":"05_Solution_Summary/#introduction-what-is-arc-and-why-is-it-relevant","title":"Introduction. What is ARC and why is it relevant?","text":"<p>Fran\u00e7ois Chollet defined intelligence as skill-acquisition intelligence in the paper On the Measure of Intelligence back in 2019.</p> <p>Humans (and that includes many AI researchers) tend to confuse skill with intelligence. However, skill is the product of intelligence. Intelligence is the rate at which a learner turns its experience and priors into new skills. This confusion between intelligence and skill happens because when a person shows a great level of skill, for example at chess, that person is very likely to be intelligent. Skill and intelligence are correlated in humans because humans do not know chess at birth and they have to learn how to play it. Thus if a person is able to achieve a great level of skill at chess, it is because they have been able to acquire that skill more efficiently than other people. However, in the case of machines, that correlation is completely broken. Given some task like playing chess, it is possible to achieve an arbitrary level of skill by using unlimited priors, training data, and compute. But that machine would only be capable of playing chess and nothing more. Its adaptation capacity is very limited and thus its intelligence is very limited as well.</p> <p>The intelligence of a system is a measure of its skill-acquisition efficiency over a scope of tasks, with respect to priors, experience, and generalization difficulty.</p> <p>Based on this definition, Chollet created the Abstraction and Reasoning Corpus (ARC). ARC is a collection of visual intelligence tasks that only require core knowledge priors. Each task has only a few examples to understand the task, and all the evaluation tasks are novel and different from the training tasks. Notice how ARC has been designed to control for priors, experience and generalization difficulty. The image below shows a sample of the images used in the ARC tasks.</p> <p></p> <p>ARC is important because it is currently the only benchmark that measures intelligence. All the other benchmarks just measure skill (math skills, coding skills, general knowledge...). If we want to make progress towards AGI, ARC is the north star metric that we should follow.</p>"},{"location":"05_Solution_Summary/#vision-search-and-learn","title":"Vision: Search and learn","text":"<p>Vision</p> <p>I believe ARC will be solved first by deep-learning-guided program synthesis that searches program space and adapts at test time with test-time training via hindsight relabeling in a tight search-and-learn loop.</p> <p>There are at least four different paths to arrive at that vision:</p> <ol> <li>Search and learn</li> <li>Combine the best approaches from ARC24: test-time training and program synthesis</li> <li>Imitate how humans solve ARC</li> <li>Frame ARC as a game and solve it with RL</li> </ol> <p>In the following sections I describe the different paths in more detail.</p>"},{"location":"05_Solution_Summary/#path-1-search-and-learn","title":"Path 1. Search and learn","text":"<p>There are only two methods to adapt to novelty: search and learn.</p> <p>All the top scoring solutions from ARC24 relied on learning: they used test-time training to adapt the model to the new tasks.</p> <p>On the other hand, the solutions for the semi-private evaluation relied on search. o3 and other reasoning models search the space of natural language programs to find solutions for novel tasks. Other methods pioneered by Greenblatt searched the space of Python programs.</p> <p>Humans use both methods. When we approach a new task, we try different approaches to solve it and we learn from the failures. When trying subsequent approaches we do not repeat the mistakes. We try new approaches that take into account the information obtained from the failing trials. So we search, learn from our mistakes, and start the cycle again until we eventually find the solution. For simple problems (like solving an ARC task), this cycle can take seconds or minutes. For harder problems (like building a system that solves ARC), this cycle can take many years.</p> <p>I believe that a system that will solve ARC will very likely combine search and learn as well. All my work during the ARC25 challenge has moved in that direction.</p> <p></p><p></p>"},{"location":"05_Solution_Summary/#path-2-combine-the-best-approaches-from-arc24-test-time-training-and-program-synthesis","title":"Path 2. Combine the best approaches from ARC24: test-time training and program synthesis","text":"<p>Last year's competition showed that test-time training allowed the models to adapt to the novel tasks. At the same time in the semi-private dataset, we saw that frontier models could generate code to solve more than half of the tasks.</p> <p>Using code is a more promising approach because:</p> <ol> <li>It is verifiable.</li> <li>It enables iterative refinement of the solution by comparing outputs with the ground truth. This is similar to reasoning.</li> </ol> <p>My hypothesis is that we can use hindsight experience replay (HER) at test time to update the beliefs of the model and find the right solution more efficiently. Hindsight Experience Replay is a reinforcement learning technique where the agent learns from failures by relabeling unsuccessful episodes as if they were successful for a different goal. For example we want to go from A to B, but instead we end up in C. We can use that trajectory to teach the model how to go from A to C.</p> <p>Instead of sampling thousands of programs, we can sample a few and learn from the mistakes. That is the way to combine induction and test-time training.</p> <p>We can treat failed code attempts that run as new tasks and train the model on those tasks. Those tasks will be in the neighborhood of the task that we want to solve.</p> <p>We already know that HER enables faster learning, especially in very sparse reward environments.</p> <p></p> <p>In the rest of the report I will use Hindsight Experience Replay or Hindsight Relabeling interchangeably. I believe Hindsight relabeling is more correct because we relabel the tasks and use them for training, we don't replay the tasks many times.</p>"},{"location":"05_Solution_Summary/#path-3-imitate-how-humans-solve-arc","title":"Path 3. Imitate how humans solve ARC","text":""},{"location":"05_Solution_Summary/#how-humans-solve-arc","title":"How humans solve ARC","text":"<p>When humans try to solve ARC tasks, we draw some hypotheses and test them in our heads. If a hypothesis is not correct, we update our beliefs and refine the hypothesis. What modules are needed to do this process?</p> <ul> <li>Policy. What action do I have to take to achieve the goal? Learned with hindsight.</li> <li>World model. What happens if I do this action? Learned with past experiences.</li> <li>Judgment. Is the solution correct? Learned with human feedback or by comparison.</li> <li>Learning. In difficult problems, we learn from errors and modify our initial beliefs about the problem.</li> </ul> <p>Reasoning is an iterative process as shown in the loop diagram in the image.</p>"},{"location":"05_Solution_Summary/#how-ai-might-solve-arc","title":"How AI might solve ARC","text":"<p>Focusing on efficiency, the best configuration for ARC might be the following:</p> <p></p> <ul> <li>Policy: a Large Reasoning Model.</li> <li>World model: Python interpreter.</li> <li>Judgment: metric function.</li> <li>Learning: reinforcement learning and hindsight experience replay.</li> </ul> <p>That way we only have to learn the policy and parametrize the learning process. All the other modules are guaranteed to work correctly.</p>"},{"location":"05_Solution_Summary/#path-4-frame-arc-as-a-game-and-solve-it-with-rl","title":"Path 4. Frame ARC as a game and solve it with RL","text":"<p>The idea is to frame ARC as a reinforcement learning problem. The system is given a new task and it needs to solve it as efficiently as possible. It is like playing a game, but instead of hitting buttons, it has to write code. The code generates an output that is evaluated against the ground truth and returns a score.</p> <p>Finding the right program is equivalent to finding the right trajectory to solve a game. Instead of actions, we write code, but the problem is the same. We can frame the problem as a reinforcement learning game with a very sparse reward.</p> <p>The challenge of ARC tasks is that the reward is very sparse, and standard RL methods do not work well in that setting. When rewards are very sparse we need to add tricks like hindsight experience replay, curiosity to promote exploration or access to human demonstrations.</p>"},{"location":"05_Solution_Summary/#why-search-and-learn-will-beat-the-other-approaches","title":"Why search and learn will beat the other approaches","text":"<p>ARC can be solved (and will be solved) with many different approaches, but in this section, I will argue why search and learn will be the first approach to solve it.</p> <p>In the following subsections I will argue why I believe search and learn has advantages over the other approaches.</p>"},{"location":"05_Solution_Summary/#transduction-and-test-time-training","title":"Transduction and test-time training","text":"<p>Transduction is the process of directly drawing conclusions about new data from previous data, without constructing a model</p> <p>Although it was the dominant approach in the ARC24 prize and very likely one of the dominant approaches in ARC25, I do not believe it is the best bet to solve ARC-AGI-2 because:</p> <ul> <li>Transduction does not seem to be the best way to solve the complex tasks from ARC-AGI-2 that have multiple interacting rules. On the other hand, code allows expressing any combination of rules.</li> <li>Predictions generated with transduction do not have any guarantee of being correct. On the other hand, code can be tested with the training samples of each task, allowing rejection of incorrect programs.</li> <li>The models used for transduction are black boxes. On the other hand, when doing program synthesis, we can interpret the generated code and make a better diagnosis of failures.</li> </ul> <p>The advantage of transduction is that the signal when doing test-time training is much better and more direct than the one that can be obtained when doing test-time training with hindsight relabeling. Transduction can solve ARC, but I do not believe it is the easiest way to do it. Using more data for pretraining, an architecture with better inductive priors (that better represents the logic of the tasks), and improvements in the test-time training setup, it would be possible to solve ARC-AGI-2. But it is easier to do it with induction.</p>"},{"location":"05_Solution_Summary/#natural-language-program-search-o3","title":"Natural language program search (o3)","text":"<p>Although OpenAI did not share any details of how a fine-tuned version of o3 was able to solve ARC-AGI-1, it is believed that it used natural language program search. For each task, o3 described the task using natural language, then transformed the grids conditioned on that description, and analyzed the outputs to find errors and refine the description of the task. However:</p> <ul> <li>Any natural language description of a task can be implemented using Python code and could be expressed   in a short program if a good domain-specific language (DSL) is available. Thus I do not see a clear   advantage of using natural language over Python code.</li> <li>All deep learning models are fallible. Even if the model finds the correct description of the task,   it might fail to transform the grids accordingly. The Python interpreter is a deterministic executor: once a correct program is found, it will always produce the correct output.</li> </ul>"},{"location":"05_Solution_Summary/#program-search-with-frontier-models","title":"Program search with frontier models","text":"<p>Public approaches with frontier LLMs like the ones by Ryan Greenblatt and Jeremy Berman achieve state-of-the-art accuracy on ARC by generating Python code and refining the code using feedback from execution.</p> <p>My guess is that a frozen model, no matter how big, will not be able to generalize when the generalization gap is large (at least for a constrained inference budget). I hypothesize that search and learn will beat a pure search approach.</p>"},{"location":"05_Solution_Summary/#research-journey","title":"Research Journey","text":""},{"location":"05_Solution_Summary/#1-how-does-test-time-training-compare-against-o3","title":"1. How does test-time training compare against o3?","text":"<p>At the start of the ARC25 challenge, I was curious to see how well test-time training compared against o3. A custom version of o3 was presented in December 2024 and reported to have solved 87.5% of the semi-private test set of ARC-AGI-1. However, with the release of ARC-AGI-2, o3 solved less than 5% of the semi-private test set. It was not the exact same version of o3, but the change was dramatic.</p> <p>To my surprise, I was able to score 11.94 on the leaderboard, doubling the score of o3 and being the first team to score above 10% in the challenge using transduction and test-time training. Sadly, this was in April, and I was unable to improve on this baseline during the six long months that remained of the challenge.</p> <p>To achieve this, I simply took the solution for ARC24 from the Architects and made a few small modifications:</p> <ul> <li>Apply test-time training to each task individually instead of training for a group of tasks together.</li> <li>Modify it to work efficiently on 4 GPUs.</li> <li>Hyperparameter tuning.</li> </ul> <p>These results demonstrated the power of test-time training, beating o3 and establishing a strong baseline for the rest of the challenge.</p> <p>Learning</p> <p>Test-time training with the model for ARC24 from the Architects was able to score 11.94% on the leaderboard while o3 scored less than 5%.</p> <p>Please go to iterations 1, 2 and 3 for more information.</p>"},{"location":"05_Solution_Summary/#2-does-hindsight-relabeling-work-for-program-synthesis-on-toy-tasks","title":"2. Does hindsight relabeling work for program synthesis on toy tasks?","text":"<p>Before starting to work with ARC tasks, I wanted to validate that hindsight relabeling was helpful for program synthesis on toy tasks. Instead of training a model to learn to use dozens of primitive functions, I decided to train a model to learn to draw. Thus the model only had access to a minimal DSL (Domain Specific Language) with just a few primitives like <code>draw_pixel</code>, <code>draw_line</code> and <code>draw_rectangle</code>.</p> <p>The training data was generated by doing random drawings with up to 5 function calls on each drawing. Each task started from an initial grid (that could be a random solid color or randomly initialized pixels) and up to 5 new elements were added (points, lines or rectangles). When training, the model was shown the input and output grid and taught to answer with the code that created the drawing. See some training examples below:</p> <p></p> <p></p> <p>As expected, when we tested the model with out-of-distribution tasks (tasks with more than 5 drawings), the performance dropped drastically.</p> <p></p> <p>Then I started experiments with hindsight relabeling. I manually created tasks that were so far from the training distribution that the model was unable to solve them. For example, below you can see a task with 25 squares of different colors. The image below shows the best prediction for each epoch. The final prediction is perfect, and it can be seen how the best prediction improves over the epochs.</p> <p></p> <p>This second image shows how the accuracy distribution evolved during the epochs. Notice how on the first epoch the prediction is very poor, and the accuracy distribution shows that no matter how many predictions are generated with the base model, it will be impossible to solve the task.</p> <p></p> <p>The initial algorithm was the following:</p> <ol> <li>Given the inputs and outputs, the model generates n predictions (for example n=256).</li> <li>The predictions are run to generate output images.</li> <li>Remove duplicates: keep only one prediction per output.</li> <li>Validate the predicted code (remove lines that do not affect the output).</li> <li>Create new tasks using hindsight relabeling. Use the original output, the output generated when running the code, and the predicted code. The model is trained to predict the code that generated the output.</li> <li>Sort the tasks in ascending order using the pixel accuracy of the prediction. The worst predictions come first.</li> <li>Fine-tune the model on these new hindsight relabeled tasks.</li> <li>Repeat until a perfect solution is achieved or the maximum number of epochs is reached.</li> </ol> <p>One interesting thing is that this method still works even if we do not sort the tasks by accuracy. This implies that the reward function is not necessary.</p> <p>After a few tweaks and hyperparameter tuning, I demonstrated that the model was capable of learning to draw anything using test-time training on hindsight relabeling tasks. It was able to solve tasks with 100 squares and complex drawings with multiple elements like the chick image below.</p> <p></p> <p></p> <p>Learning</p> <p>Hindsight relabeling allowed a model trained to draw to generalize outside its training distribution. The model was trained to draw up to 5 elements and by doing test-time training with hindsight relabeling it was able to solve tasks with more than 100 drawn elements.</p> <p>For more information go to iterations 4, 5, 6, 8 and 9.</p>"},{"location":"05_Solution_Summary/#3-does-hindsight-relabeling-work-for-program-synthesis-on-arc-tasks","title":"3. Does hindsight relabeling work for program synthesis on ARC tasks?","text":"<p>After validating that test-time training on hindsight relabeled tasks allowed solving toy tasks, it was time to see if we could validate the approach on ARC tasks that were much more complex.</p>"},{"location":"05_Solution_Summary/#31-try-to-train-my-own-models","title":"3.1 Try to train my own models","text":"<p>As a first step, I tried to continue the approach taken for the toy drawing tasks. I defined a small set of primitive functions (~40), and I implemented task generators that created random tasks to teach how to use them.</p> <p>However, the models trained on those synthetic tasks were unable to solve any of the real ARC tasks. Despite being able to generate an infinite number of synthetic tasks, the diversity was limited. I implemented 32 task generators, but they likely had biases, and the model was unable to learn something that generalized from that data distribution. Furthermore, the diversity of model predictions was very small, so the search space of solutions was not fully explored.</p> <p>Learning</p> <p>Infinite synthetic data is not enough if the diversity of the data is low.</p> <p>For more information go to iterations 10, 12, 13, 14 and 15.</p>"},{"location":"05_Solution_Summary/#32-experiment-with-base-models","title":"3.2 Experiment with base models","text":"<p>After learning that creating synthetic tasks to teach a model to learn a DSL was very hard, I decided to try open-weight models. The idea was to prompt the models with a list of the available DSL functions and their signatures so the model could use them to generate a solution. I decided to use the BARC DSL in these experiments.</p> <p>I tried the Qwen2.5-Coder family of models because there were many different model sizes. The plot below shows that bigger models generate valid outputs more frequently and use the DSL more frequently as well. The results are for the ARC-AGI-1 training set.</p> <p></p> <p>The plot below shows how the solved task rate changes with the number of predictions for the 7B Qwen2.5-Coder model. However, it also shows that the number of unique outputs decreases very fast showing a lack of diversity in the predictions.</p> <p></p> <p>A surprising finding was that trying different prompting techniques to increase output diversity produced worse results than simply asking the model to solve the task. For example, I gave already generated solutions by the model in the prompt and requested something new and different, but the effect was the opposite. In many cases, instead of doing something new, the model simply copied the code given in the prompt. It seems that small LLMs lack capabilities that frontier models have.</p> <p>Learning</p> <p>Small open-weight models with access to a DSL can solve some ARC tasks by writing Python code.</p> <p>For more information go to iterations 16 and 17.</p>"},{"location":"05_Solution_Summary/#33-experiment-with-barc-induction-model","title":"3.3 Experiment with BARC induction model","text":"<p>After seeing that open-weight models with access to the BARC DSL were able to solve ARC tasks, I decided to use the BARC induction model directly. This model already knew how to use the DSL so I did not have to provide the signature of the DSL functions in the prompt. One brilliant aspect of the BARC paper is that they implemented generators and solvers for 162 ARC-AGI-1 training tasks and they use that code as a seed for LLMs to generate new tasks. By doing that, they move the problem domain from the ARC grids to code and leverage the code capabilities of LLMs to generate new tasks. Asking an LLM to generate new tasks in the grid domain will likely yield poor results. They train the BARC induction model on hundreds of thousands of LLM-generated tasks and this overcomes the problems described in the previous 3.1 section where I could not generate training data with enough diversity to train my own models.</p>"},{"location":"05_Solution_Summary/#331-replicate-results-from-barc-paper","title":"3.3.1 Replicate results from BARC paper","text":"<p>As a first step, I validated that I could get similar results to the numbers reported in the paper. A direct comparison is not possible because their last numbers are obtained doing 20k predictions per task and I only did around 500 predictions due to the constraints imposed by the Kaggle submission (with the current hardware, I do not think it is possible to make much more than 512 predictions per task with a 7B model).</p> <p>In the paper there is a plot that shows a solve rate slightly below 15% for 500 submissions and I got around 22% for the same number of submissions. The differences are probably explained because the plot in the paper is likely obtained with a model trained on less data (not the final model) and possibly due to using data augmentation during inference.</p> <p></p> Click to see examples of solved tasks <pre><code>from common import *\n\nimport numpy as np\nfrom typing import *\n\n# concepts:\n# scaling, color transformation\n\n# description:\n# In the input, you will see a 3x3 sprite with gray pixels scattered randomly. \n# To create the output grid, you should first scale the sprite by a factor of 2, \n# then replace all gray pixels with a pattern of alternating colors (blue and red).\n# The scaled sprite should maintain the original size, and the pattern should cover the gray pixels only.\n\ndef transform(input_grid):\n    # Step 1: Detect the gray pixels in the input grid\n    gray_positions = np.argwhere(input_grid == Color.GRAY)\n\n    # Step 2: Create a new output grid with the same size as the scaled sprite\n    scale_factor = 2\n    output_height = input_grid.shape[0] * scale_factor\n    output_width = input_grid.shape[1] * scale_factor\n    output_grid = np.full((output_height, output_width), Color.BLACK)\n\n    # Step 3: Scale the input grid by the scale factor and place it in the output grid\n    for i in range(input_grid.shape[0]):\n        for j in range(input_grid.shape[1]):\n            if input_grid[i, j] != Color.BLACK:\n                # Blit the original color in the scaled position\n                blit_sprite(output_grid, np.full((scale_factor, scale_factor), input_grid[i, j]), \n                            x=i*scale_factor, y=j*scale_factor)\n\n    # Step 4: Replace gray pixels in the scaled grid with the alternating pattern\n    for x, y in gray_positions:\n        scaled_x, scaled_y = x * scale_factor, y * scale_factor\n        # Create a 2x2 alternating pattern of blue and red\n        pattern = np.array([[Color.BLUE, Color.RED],\n                            [Color.RED, Color.BLUE]])\n        blit_sprite(output_grid, pattern, scaled_x, scaled_y)\n\n    return output_grid\n</code></pre>   ---   <pre><code>from common import *\n\nimport numpy as np\nfrom typing import *\n\n# concepts:\n# pattern generation, lines\n\n# description:\n# In the input you will see two red pixels. \n# To make the output, you should create a pattern of blue squares and red lines that connect the two red pixels.\n# The pattern consists of blue squares filling the area between the two red pixels, \n# and the red lines should extend vertically and horizontally from the red pixels to the edges of the canvas.\n\ndef transform(input_grid):\n    # Find the positions of the two red pixels\n    red_positions = np.argwhere(input_grid == Color.RED)\n    if len(red_positions) != 2:\n        raise ValueError(\"Input grid must contain exactly two red pixels.\")\n\n    (x1, y1), (x2, y2) = red_positions\n\n    # Determine the bounding box for the blue squares\n    min_x, max_x = min(x1, x2), max(x1, x2)\n    min_y, max_y = min(y1, y2), max(y1, y2)\n\n    # Create blue squares in the bounding box\n    output_grid = np.zeros_like(input_grid)\n    output_grid[min_x:max_x + 1, min_y:max_y + 1] = Color.BLUE\n\n    # Draw red lines from the red pixels to the edges of the canvas\n    draw_line(output_grid, x1, y1, color=Color.RED, direction=(1, 0))  # Right from first red pixel\n    draw_line(output_grid, x1, y1, color=Color.RED, direction=(-1, 0)) # Left from first red pixel\n    draw_line(output_grid, x1, y1, color=Color.RED, direction=(0, 1))  # Down from first red pixel\n    draw_line(output_grid, x1, y1, color=Color.RED, direction=(0, -1)) # Up from first red pixel\n\n    draw_line(output_grid, x2, y2, color=Color.RED, direction=(1, 0))  # Right from second red pixel\n    draw_line(output_grid, x2, y2, color=Color.RED, direction=(-1, 0)) # Left from second red pixel\n    draw_line(output_grid, x2, y2, color=Color.RED, direction=(0, 1))  # Down from second red pixel\n    draw_line(output_grid, x2, y2, color=Color.RED, direction=(0, -1)) # Up from second red pixel\n\n    return output_grid\n</code></pre>   ---   <pre><code>from common import *\n\nimport numpy as np\nfrom typing import *\n\n# concepts:\n# circle detection, color transformation\n\n# description:\n# In the input, you will see a grid with random colored pixels on it. \n# To make the output, you should find all circular shapes (of any color) \n# with a diameter greater than or equal to 3 pixels and change their color to yellow.\n\ndef transform(input_grid: np.ndarray) -&gt; np.ndarray:\n    # Plan:\n    # 1. Detect circular shapes in the grid\n    # 2. Change their color to yellow if they meet the size criteria\n\n    output_grid = np.copy(input_grid)\n\n    # Iterate over the grid to find circular shapes\n    for x in range(len(input_grid)):\n        for y in range(len(input_grid[0])):\n            # Check if the pixel is not background\n            if input_grid[x, y] != Color.BLACK:\n                # Check for circle shape using a simple heuristic\n                diameter = 1\n                while True:\n                    # Check the pixels in the current diameter\n                    if (x + diameter &lt; len(input_grid) and\n                        y + diameter &lt; len(input_grid[0]) and\n                        np.all(input_grid[x:x + diameter + 1, y:y + diameter + 1] == input_grid[x, y])):\n                        diameter += 1\n                    else:\n                        diameter -= 1\n                        break\n                if diameter &gt;= 3:\n                    output_grid[x:x + diameter + 1, y:y + diameter + 1] = Color.YELLOW\n\n    return output_grid\n</code></pre> <p>Learning</p> <p>The BARC induction model is able to solve around 22% of the ARC-AGI-1 evaluation tasks with a budget of 512 predictions. However, it only solves 0.8% of the ARC-AGI-2 evaluation tasks.</p> <p>For more information go to iterations 19, 20 and 21.</p>"},{"location":"05_Solution_Summary/#332-hindsight-relabeling-and-barc-induction-model","title":"3.3.2 Hindsight relabeling and BARC induction model","text":"<p>To verify if it was possible to do test-time training on hindsight relabeling tasks for program synthesis on ARC tasks I designed the following experiments: all the experiments have the same inference budget of 512 predictions. The only differences between experiments are whether test-time training with hindsight relabeling is used and its configuration.</p> <p></p> <ul> <li>Orange lines are the baseline. I repeated the experiment 3 times to measure variability.</li> <li>The green line does 256 predictions, learns, and does another 256 predictions. Notice how the green line starts to deviate from the orange lines only after prediction 256.</li> <li>The blue line learns every 128 predictions. Notice how the blue line deviates from the orange line after   prediction 128.</li> </ul> <p>The table below summarizes the experiment and the results.</p> initial predictions epochs predictions per epoch pass@n 512 0 0 23.3% 256 1 256 26.0% 128 3 128 28.3% <p>We obtain an improvement of 5% with the best configuration. It is not a huge improvement like the one observed on ARC24 with transduction and test-time training where I improved from 11% to 33%. But it validates the idea of search and learn.</p> <p>I would argue that the improvement will be bigger if we had an inference budget larger than 512 predictions, but we are constrained by the Kaggle submission hardware and time (I already know that the improvement is smaller if we use a smaller number of predictions).</p> <p>In this initial implementation, the model is fine-tuned independently for each task and all the predictions that generated valid outputs are used for training. A more compute efficient implementation would only use the best predictions for training.</p> <p>Learning</p> <p>We have validated that the search and learn approach works. By doing test-time training with hindsight relabeling on the BARC induction model we were able to solve 28.3% of the ARC-AGI-1 evaluation tasks compared to the 23.3% of the baseline model without test-time training.</p> <p>For more information go to iterations 22 and 23.</p>"},{"location":"05_Solution_Summary/#4-can-we-get-a-stronger-base-model-with-reinforcement-learning","title":"4. Can we get a stronger base model with reinforcement learning?","text":"<p>After validating that the search and learn approach could work, I realized that I need a stronger base model to be able to beat ARC-AGI-2. The BARC induction model only solves 22% and 0.8% of the evaluation tasks of ARC-AGI-1 and ARC-AGI-2 respectively when doing 512 predictions.</p> <p>I thought that trying reinforcement learning could be a good idea. As an outsider, it seems that recent advances in math and coding abilities of LLMs have come from using RL. I had experience with RL in different competitions (Animal AI Olympics, Lux AI and Hungry Geese), but not with LLMs, so I thought it was a good idea to give it a try.</p> <p>I verified that RL fine-tuning improved the solving rate of the base model, but I could not train for long because all the trainings eventually collapsed. The reward improved during the training until suddenly it collapsed. On the first experiments the model entered a loop in the predictions, repeating the same tokens over and over. When adding repetition penalty to avoid this behaviour, the model simply predicted gibberish.</p> <p>I have tried many things, but so far I haven't solved the problem of training collapse:</p> <ul> <li>Using a bigger training dataset (from ARC-AGI-1 400 training samples to BARC 100k samples)</li> <li>Using a bigger number of generations per step (from 8 to 128)</li> <li>Increasing the KL penalty</li> <li>Decreasing the max grad norm</li> <li>Simplifying or changing the reward function</li> <li>Adding repetition penalty</li> <li>Disabling 4bit quantization for training</li> <li>Changing the LoRA rank</li> </ul> <p></p> <p>In the best experiment the solve rate for the ARC-AGI-1 evaluation set improved from 22.3% of the base model to 27% for the model fine-tuned with RL when doing 480 predictions. It would be interesting to train for longer if I'm able to avoid training collapse.</p> <p>Learning</p> <p>Reinforcement learning improves the solving rate of the model (22% -&gt; 27%), but I have been unable to train for long due to training collapse.</p> <p>For more information go to iterations 24, 25, 29, 30, 33 and 35.</p>"},{"location":"05_Solution_Summary/#5-can-we-improve-the-search-accuracy-by-doing-prediction-refinement","title":"5. Can we improve the search accuracy by doing prediction refinement?","text":""},{"location":"05_Solution_Summary/#51-can-the-barc-induction-model-refine-its-predictions","title":"5.1 Can the BARC induction model refine its predictions?","text":"<p>Currently, I am doing independent predictions with the BARC induction model. Each prediction is independent of the others. This is different from how humans solve tasks. We have in memory the history of the search: what we tried, how well it worked...</p> <p>One way to achieve this with LLMs is by asking to refine some incorrect solution. Given some prediction from the model, we can execute the code, add the outputs to the prompt along with some metrics, and request the model to analyze the problems of the generated code and create a refined version of it. Public approaches with frontier LLMs by Ryan Greenblatt and Jeremy Berman rely on this ability of frontier models to refine code given feedback from execution.</p> <p>However, when I tried to refine predictions with the BARC induction model, I found that the model did not have that ability. I compared a run doing 128 independent predictions per task versus doing 64 independent predictions, selecting the best 8 predictions, and trying to refine those. I did not find any significant difference in accuracy.</p> <p>Learning</p> <p>Frontier models have the ability to refine their predictions given feedback from execution, but this 8B Llama model fine-tuned on ARC tasks does not have that ability.</p> <p>For more information go to iteration 28.</p>"},{"location":"05_Solution_Summary/#52-can-the-barc-induction-model-learn-to-refine-its-predictions-using-rl","title":"5.2 Can the BARC induction model learn to refine its predictions using RL?","text":"<p>I have fine-tuned the BARC induction model with the BARC dataset to refine its own predictions. On a first step I generated predictions for the dataset, and selected the best ones that did not solve the task. Those were given in the training along feedback from execution. The model was trained for 17k steps.</p> <p>When evaluating the model we observed a small improvement in the solved tasks from 16.3% to 17.8%. If we had a stable RL training and enough time and compute, maybe this small improvement could be made bigger.</p> <p>Learning</p> <p>Fine-tuning a model with RL to refine its predictions yields small improvements.</p> <p>For more information go to iteration 34.</p>"},{"location":"05_Solution_Summary/#conclusions-and-next-steps","title":"Conclusions and next steps","text":"<p>The ARC25 challenge is over, and despite not being able to improve on the transduction test-time training baseline with the search-and-learn approach, I have enjoyed all these months of research and learning. In retrospect, I believe I could have achieved a better leaderboard score and position by further optimizing the TTT approach, but I prioritized working on an approach that I believe could ultimately solve ARC. The top teams on the public leaderboard scored around 27%, so there is still a long way to go to reach the 85% goal, and I hope to keep enjoying this research journey.</p> <p>If search and learn is the right approach, why haven\u2019t I been able to beat the transduction test-time training approach? There are many reasons, but let\u2019s point out the main ones:</p> <ul> <li>A stronger induction model is needed to beat ARC. How to craft that model remains an open question.</li> <li>My search method was very basic, relying only on independent predictions. That would only work on trivial tasks; to solve complex tasks, refinement is needed.</li> <li>More work is also needed to learn as much as possible from the failed attempts.</li> </ul> <p>In the coming days, I will thoroughly review all the work done by other teams and rethink my approach for the next ARC challenge edition. I have some vague ideas in mind that I want to reflect on: Do humans have a continuous model of the world, or do we have a discrete, ever-growing model where we apply targeted edits when evidence contradicts our beliefs? Is deep learning and gradient descent the best learning method, or could there be more sample-efficient alternatives?</p> <p>See you on ARC26!</p>"},{"location":"05_Solution_Summary/#acknowledgements","title":"Acknowledgements","text":"<ul> <li>Thanks to my wife and my family for taking care of our children many times so I could do research without small AGIs disturbing me.</li> <li>Thanks to Veridas for allowing me to do research on ARC during part of my job time and for providing me access to its compute cluster.</li> <li>Thanks to Strong Compute for providing compute for some of the RL experiments.</li> </ul>"},{"location":"05_Solution_Summary/#links","title":"Links","text":"<ul> <li>Github repo</li> <li>Github documentation</li> <li>Notebook 1</li> <li>Notebook 2</li> </ul>"},{"location":"06_Winning_Model_Documentation/","title":"Winning model documentation","text":""},{"location":"06_Winning_Model_Documentation/#winning-model-documentation","title":"Winning model documentation","text":"<p>Winning Model Documentation Guidelines</p>"},{"location":"06_Winning_Model_Documentation/#a-model-summary","title":"A. MODEL SUMMARY","text":""},{"location":"06_Winning_Model_Documentation/#a1-background-on-youyour-team","title":"A1. Background on you/your team","text":"<ul> <li>Competition Name:</li> <li>Team Name:</li> <li>Private Leaderboard Score:</li> <li>Private Leaderboard Place:</li> <li>Name: Guillermo Barbadillo</li> <li>Location: Pamplona, SPAIN</li> <li>Email: guilllermobarbadillo@gmail.com</li> </ul>"},{"location":"06_Winning_Model_Documentation/#a2-background-on-youyour-team","title":"A2. Background on you/your team","text":""},{"location":"06_Winning_Model_Documentation/#what-is-your-academicprofessional-background","title":"What is your academic/professional background?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-have-any-prior-experience-that-helped-you-succeed-in-this-competition","title":"Did you have any prior experience that helped you succeed in this competition?","text":""},{"location":"06_Winning_Model_Documentation/#what-made-you-decide-to-enter-this-competition","title":"What made you decide to enter this competition?","text":""},{"location":"06_Winning_Model_Documentation/#how-much-time-did-you-spend-on-the-competition","title":"How much time did you spend on the competition?","text":""},{"location":"06_Winning_Model_Documentation/#if-part-of-a-team-how-did-you-decide-to-team-up","title":"If part of a team, how did you decide to team up?","text":""},{"location":"06_Winning_Model_Documentation/#if-you-competed-as-part-of-a-team-who-did-what","title":"If you competed as part of a team, who did what?","text":""},{"location":"06_Winning_Model_Documentation/#a3-summary","title":"A3. Summary","text":""},{"location":"06_Winning_Model_Documentation/#a4-features-selection-engineering","title":"A4. Features Selection / Engineering","text":""},{"location":"06_Winning_Model_Documentation/#what-were-the-most-important-features","title":"What were the most important features?","text":""},{"location":"06_Winning_Model_Documentation/#how-did-you-select-features","title":"How did you select features?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-make-any-important-feature-transformations","title":"Did you make any important feature transformations?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-interactions-between-features","title":"Did you find any interesting interactions between features?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-use-external-data-if-permitted","title":"Did you use external data? (if permitted)","text":""},{"location":"06_Winning_Model_Documentation/#a5-training-methods","title":"A5. Training Method(s)","text":""},{"location":"06_Winning_Model_Documentation/#what-training-methods-did-you-use","title":"What training methods did you use?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-ensemble-the-models","title":"Did you ensemble the models?","text":""},{"location":"06_Winning_Model_Documentation/#if-you-did-ensemble-how-did-you-weight-the-different-models","title":"If you did ensemble, how did you weight the different models?","text":""},{"location":"06_Winning_Model_Documentation/#a6-interesting-findings","title":"A6. Interesting findings","text":""},{"location":"06_Winning_Model_Documentation/#what-was-the-most-important-trick-you-used","title":"What was the most important trick you used?","text":""},{"location":"06_Winning_Model_Documentation/#what-do-you-think-set-you-apart-from-others-in-the-competition","title":"What do you think set you apart from others in the competition?","text":""},{"location":"06_Winning_Model_Documentation/#did-you-find-any-interesting-relationships-in-the-data-that-dont-fit-in-the-sections-above","title":"Did you find any interesting relationships in the data that don't fit in the sections above?","text":""},{"location":"06_Winning_Model_Documentation/#a7-simple-features-and-methods","title":"A7. Simple Features and Methods","text":""},{"location":"06_Winning_Model_Documentation/#a8-model-execution-time","title":"A8. Model Execution Time","text":""},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-your-model","title":"How long does it take to train your model?","text":""},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-using-your-model","title":"How long does it take to generate predictions using your model?","text":""},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-train-the-simplified-model-referenced-in-section-a6","title":"How long does it take to train the simplified model (referenced in section A6)?","text":""},{"location":"06_Winning_Model_Documentation/#how-long-does-it-take-to-generate-predictions-from-the-simplified-model","title":"How long does it take to generate predictions from the simplified model?","text":""},{"location":"06_Winning_Model_Documentation/#a9-references","title":"A9. References","text":""},{"location":"06_Winning_Model_Documentation/#b-submission-model","title":"B. SUBMISSION MODEL","text":""},{"location":"06_Winning_Model_Documentation/#b1-all-code-data-and-your-trained-model-goes-in-a-single-archive","title":"B1. All code, data, and your trained model goes in a single archive","text":""},{"location":"06_Winning_Model_Documentation/#b2-readmemd","title":"B2. README.md","text":""},{"location":"06_Winning_Model_Documentation/#b3-configuration-files","title":"B3. Configuration files","text":""},{"location":"06_Winning_Model_Documentation/#b4-requirementstxt","title":"B4. requirements.txt","text":""},{"location":"06_Winning_Model_Documentation/#b5-directory_structuretxt","title":"B5. directory_structure.txt","text":""},{"location":"06_Winning_Model_Documentation/#b6-settingsjson","title":"B6. SETTINGS.json","text":""},{"location":"06_Winning_Model_Documentation/#b7-serialized-copy-of-the-trained-model","title":"B7. Serialized copy of the trained model","text":""},{"location":"06_Winning_Model_Documentation/#b8-entry_pointsmd","title":"B8. entry_points.md","text":""},{"location":"modeling/Iteration_01_architects_baseline/","title":"Iteration 1. Architects baseline","text":""},{"location":"modeling/Iteration_01_architects_baseline/#iteration-1-architects-baseline","title":"Iteration 1. Architects baseline","text":"<p>28-03-2025</p>"},{"location":"modeling/Iteration_01_architects_baseline/#goal","title":"Goal","text":"<p>How far can we go using the Architects' solution from ARC24?</p>"},{"location":"modeling/Iteration_01_architects_baseline/#motivation","title":"Motivation","text":"<p>ARC25 has just started and I believe establishing a baseline would be a good starting point. Moreover I would like to know better the Architects' solution.</p>"},{"location":"modeling/Iteration_01_architects_baseline/#development","title":"Development","text":"<p>I have created my own notebook that modifies the original solution to work with 4 GPUs.</p>"},{"location":"modeling/Iteration_01_architects_baseline/#hyperparameter-tuning","title":"Hyperparameter tuning","text":""},{"location":"modeling/Iteration_01_architects_baseline/#min_prob","title":"min_prob","text":"<p>I have seen that increasing the value of min_prob results on faster inference, but at the same time I believe that less valid predictions are made. If I use <code>min_prob=None</code> all the predictions are valid, probably that uses a greedy approach whereas in the other case it only creates a prediction if the probability is higher than the parameter value.</p> <pre><code># parameter values and inference times\nmin_prob=0.10 [63, 242, 277, 492]\nmin_prob=0.17 [35, 147, 195, 309]\nmin_prob=0.25 [23, 108, 154, 259]\nmin_prob=0.35 [17, 82, 118, 235]\nmin_prob=0.5  [13, 59, 66, 191]\n</code></pre>"},{"location":"modeling/Iteration_01_architects_baseline/#effect-of-n","title":"Effect of <code>n</code>","text":"<p>The inference time is proportional to how many predictions we do per task.</p> <pre><code>n=1 [19, 77, 97, 162]\nn=2 [35, 147, 195, 309]\n</code></pre>"},{"location":"modeling/Iteration_01_architects_baseline/#model-mode","title":"Model mode","text":"<p>It does not have any effect. My guess is that I need a more recent version of unsloth. I should also add flash attention to be able to work with transformers models.</p> <pre><code>unsloth_4bit [35, 147, 195, 309]\nunsloth_8bit [33, 151, 194, 335]\n</code></pre>"},{"location":"modeling/Iteration_01_architects_baseline/#max_seq_length_train","title":"max_seq_length_train","text":"<p>By default it is set to <code>4224</code> but we could increase it to <code>8192</code> without having memory issues (60% VRAM). This will make training slower for the longest tasks.</p>"},{"location":"modeling/Iteration_01_architects_baseline/#gradient-checkpointing","title":"Gradient checkpointing","text":"<p>I didn't see any significative change in training speed after disabling it.</p>"},{"location":"modeling/Iteration_01_architects_baseline/#batch-size","title":"Batch size","text":"<p>By default <code>batch_size=2, gradient_accumulation_steps=2</code> is used, I have seen that it is possible to use <code>batch_size=4, gradient_accumulation_steps=1</code> without having memory problems. It does not speedup training, but the calculation of gradients is probably better because there is a warning message about faulty gradient accumulation.</p> <p>On this notebook run I have verified with the longest evaluation tasks that even when using <code>max_seq_len=8192</code> I can use <code>batch_size=4</code> without memory problems (86% VRAM usage in the worst case.)</p>"},{"location":"modeling/Iteration_01_architects_baseline/#better-sorting-algorithm","title":"Better sorting algorithm","text":"<p>I have tried creating a better algorithm for sorting tasks between GPUs but at the end runtime was worse than the original algorithm. I have seen that training time is proportional to the training tokens, but inference time is not easy to predict. This might be explained by not knowing the output tokens and the depth first search algorithm used at inference.</p>"},{"location":"modeling/Iteration_01_architects_baseline/#2-inference-runs-per-gpu","title":"2 inference runs per GPU","text":"<p>Looking at the GPU usage I noticed that when training GPU usage was 100%, but on inference many times it was below 50% (And memory usage was always below 50%). This opens the door for running two inference process on the same GPU.</p> <p>On experiments I have seen a reduction of 30% of inference time (670s to 470s).</p> <p>After the change GPU usage is almost 100%.</p> <p></p>"},{"location":"modeling/Iteration_01_architects_baseline/#results","title":"Results","text":"<p>Gsheet with results</p> <p>According to the official ARC documentation the solution from the Architects should score around 2.5%, but my first successful submission scored 7%. I guess the improvement comes from splitting the data in 4 folds and using 4 gpus (instead of 2 folds on 2 gpus).</p> <p>I have made more submissions with different hyperparameters values but none scored higher, notice that half of the submissions gave timeout error.</p>"},{"location":"modeling/Iteration_01_architects_baseline/#conclusion","title":"Conclusion","text":"<p>The architects solution can score 7% on the new ARC-AGI-2 public test set. On the following iterations I'm going to try to push it further.</p>"},{"location":"modeling/Iteration_01_architects_baseline/#next-steps","title":"Next steps","text":"<ul> <li>Probably the easiest way to keep improving the baseline is to split the data in more folds. Using 8 folds would naturally fit with using 8 processes at inference. I would have to see how to coordinate the training runs.</li> <li>Everyday we are allowed to make submissions with a compute cost of $50. Over 7 months that will be around $10k. That is a lot of money, maybe I should use the private test set as my development set and use the evaluation set as the test set (to decide the final submission).</li> </ul>"},{"location":"modeling/Iteration_01_architects_baseline/#todo","title":"TODO","text":"<ul> <li> Create a more recent environment to see if we can speedup training and/or inference</li> <li> Is it helpful to train for longer?</li> <li> Is it helpful to increase <code>max_seq_length_train</code>?</li> </ul>"},{"location":"modeling/Iteration_02_8_fold/","title":"Iteration 2. Architects solution with 8 data splits","text":""},{"location":"modeling/Iteration_02_8_fold/#iteration-2-architects-solution-with-8-data-splits","title":"Iteration 2. Architects solution with 8 data splits","text":"<p>04-04-2025</p>"},{"location":"modeling/Iteration_02_8_fold/#goal","title":"Goal","text":"<p>Can I improve the leaderboard score by doing 8 splits to the data instead of 4?</p>"},{"location":"modeling/Iteration_02_8_fold/#motivation","title":"Motivation","text":"<p>I believe that one of the reasons that my adaptation of the architects code is scoring higher than expected is that I'm using 4 data splits instead of the original 2 splits. On this iteration I want to see if 8 splits give even better results, if that is the case I will study how to do an arbitrary number of splits (ideally equal to the number of tasks).</p> <p>It might be enough to be the first team to break the barrier of 10% on the ARC-AGI-2 benchmark.</p>"},{"location":"modeling/Iteration_02_8_fold/#development","title":"Development","text":""},{"location":"modeling/Iteration_02_8_fold/#arc24-vs-arc25-parameters","title":"ARC24 vs ARC25 parameters","text":"<p>On ARC24 there were 100 training tasks, they were split in two folds and they train for 4 epochs. The effective batch size was 4, thus if I take data augmentation into account this means it trained for 400 steps (<code>100/2*8*4/4</code>). They warmup the learning rate for 100 steps, so 25% of the training time.</p> <p>Now I want to use 8 folds, if I keep the number of epochs constant that means I will train for just 120 steps (<code>120/8*8*4/4</code>). On each fold there would be just 15 tasks.</p> <p>Maybe the easiest solution is to use the <code>warmup_ratio</code> instead of <code>warmup_steps</code> and set it to a value such as 10%.</p>"},{"location":"modeling/Iteration_02_8_fold/#initial-leaderboard-score-is-very-low","title":"Initial leaderboard score is very low","text":"<p>My first submission scores just 3.33 vs 7.08 that scored the submission with 4 folds. I have compared the code of the two submissions and I don't see anything wrong. What could be the cause of this drop in score?</p> <ul> <li>Maybe my first submission was just lucky</li> <li>The biggest change is that trainings are now shorter. Maybe I have to reduce the lora rank and/or increase the training epochs. </li> </ul> <p>After halving the lora rank to 32 and increasing the number of epochs from 4 to 6 the score improved to 6.67, still worse than the first submission but very close. However the submission time was very close to 12 hours. My guess is that there is an interplay between model training and inference, because the 2 additional epochs will likely take around 2000 seconds, and I saw an increase in submission time of around 10800 seconds.</p>"},{"location":"modeling/Iteration_02_8_fold/#training-and-inference-times","title":"Training and inference times","text":"<p>Training is taking around 4 hours for 6 epochs, so that is around 40 minutes per epoch. Training time should be constant. Inference time can change depending on the training, for 6 epochs of training it takes 4 hours to do 8 predictions per task.</p>"},{"location":"modeling/Iteration_02_8_fold/#results","title":"Results","text":"<p>I have not improved over my first submission of 7 but I was close (6.67). Google Sheet with results</p> <p>A submission doing 8 predictions per task instead of 16 scored almost the same (6.25 vs 6.67).</p>"},{"location":"modeling/Iteration_02_8_fold/#conclusion","title":"Conclusion","text":"<p>I have not improved over my first submission of 7 but I was close (6.67). I believe I should push this further and train on each task individually.</p>"},{"location":"modeling/Iteration_02_8_fold/#next-steps","title":"Next steps","text":"<p>Create a notebook that allows to do training and inference on each task independently.</p>"},{"location":"modeling/Iteration_02_8_fold/#todo","title":"TODO","text":"<ul> <li> Try to understand low score of 3.33 -&gt; Are there any errors? Compare with the best submission.</li> <li> Does it help to reduce the lora rank?</li> <li> n=1 with smaller min_prob?</li> <li> How long does it take to train on the private test set?</li> </ul>"},{"location":"modeling/Iteration_03_ideal_test_time_training/","title":"Iteration 3. Ideal test-time training setup","text":""},{"location":"modeling/Iteration_03_ideal_test_time_training/#iteration-3-ideal-test-time-training-setup","title":"Iteration 3. Ideal test-time training setup","text":"<p>07-04-2025</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#goal","title":"Goal","text":"<p>Update the architects' code to be able to make training and inference for each task.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#motivation","title":"Motivation","text":"<p>ARC tasks are independent, thus when doing test-time training is better to focus on each task instead of training on all the tasks at the same time. Knowledge transfer between the different tasks should be very small, so fine-tuning a custom model for each task should be the best strategy.</p> <p>I don't believe ARC can be solved using last year ARC24 solution, but being able to do test-time training for each task efficiently is very likely a part of this year solution.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#development","title":"Development","text":""},{"location":"modeling/Iteration_03_ideal_test_time_training/#how-the-ideal-solution-looks-like","title":"How the ideal solution looks like","text":"<ol> <li>We run 4 or 8 training processes in parallel, with batch size 1. Each training process would pick one remaining task, reset the PEFT model, train and save to disk.</li> <li>We run 8 inference processes. Each inference process would pick one remaining task, load the PEFT, make inference and save results to disk. Ideally each process would do as many predictions as possible during the available time.</li> </ol> <p>The unknown is how to load and unload the PEFT model efficiently. Every delay associated with changing the PEFT will be multiplied by 15 or 30 (depending if I use 8 or 4 processes.) So loading the model from disk, compiling... I need to find a way to do it really fast or even better don't have to do it.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#time-per-task","title":"Time per task","text":"<p>We have 12 hours to solve 240 tasks (submission evaluates both test sets). If we parallelize the system with 4 runs, that means we have 12 minutes per task. So if doing inference for each task introduces an overhead of 1 minute per task, that still leaves 11 minutes per task. So even a non efficient solution that wastes 1 minute to load and compile the model per task will have most of the time for compute.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#implementation","title":"Implementation","text":"<p>In this notebook I have prepared an implementation that uses locks to select the GPU and the task.</p> <p>Loading the model for training could take around 20s, for inference it is around 14s. So in total we could see a delay of around 30s per task, so around 30 minutes in total for a submission time of 12h hours, we can afford that.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#base-model-in-devshm","title":"Base model in <code>dev/shm</code>","text":"<p>I have tried copying the model to <code>dev/shm</code> but did not observed any speedup. Probably when I read the model for the first time it is cached. The model is slightly less than 4GB. Notebook with experiments</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#batch-size-and-training-speed","title":"Batch size and training speed","text":"experiment batch_size=1 batch_size=2 batch_size=4 2 shortest tasks, 4 epochs 72s 63s 58s 2 longest tasks, 1 epoch 125s 122s 136s <p>Clearly it pays to use a batch size of 1 if the gradient has enough information, that will allow to update the model more times.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#comparison-with-my-solution-for-arc24-challenge","title":"Comparison with my solution for ARC24 challenge","text":"<p>In my solution I could do 320 training steps for each task on ARC24 challenge. I was using a model of just 0.5B parameters versus the current 7B parameters. Now if I use 6 epochs that would be just 48 training steps, so training is 10 times shorter.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#increasing-gpu-usage","title":"Increasing GPU usage","text":"<p>After looking at the plots of GPU usage I have noticed that I could increase the number of slots per GPU both on training and inference.</p> train GPU slots inf GPU slots mean GPU usage max VRAM training time (s) inference time (s) 1 2 89.5% 51.4% 7087 7360 2 2 93.9% 50.2% 6864 6748 2 3 95.0% 76.6% 6962 6902 <p>The most reliable metric is mean GPU usage. Inference time we already know that it is not reliable and there is some variability on training times due to the random assignment of the tasks. Using 2 slots per GPU for training and 3 for inference should give a speedup of around 6%, which is 43 minutes for the 12 hour run. Not game changing but very welcome.</p> <p>Link to full results</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#results","title":"Results","text":""},{"location":"modeling/Iteration_03_ideal_test_time_training/#evaluation-vs-test-set","title":"Evaluation vs test set","text":"<p>On this notebook I have run the exact same setup that has scored 10.17 on the leaderboard and took 9 hours to run. </p> <p>If I run the exact same configuration on the evaluation set it only takes 4 hours and scores 10.6 (I'm not sure what the architects prints mean because on them the score is 8.7).</p> <p>The difference in speed is caused because when we are doing the submission the system is evaluated against both partitions of the test set, so that is 240 tasks instead of 120. So I don't have to worry about my system doing timeout on the private test set because the system has already done predictions for it.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#training-epochs","title":"Training epochs","text":"<p>It seems that a small number of training epochs (6) is bad, but also once we reach a certain number of epochs (8-10) increasing the training length is not beneficial. Maybe I have to lower the learning rate when using a bigger number of epochs?</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#train-max-sequence-length","title":"Train Max Sequence Length","text":"<p>The tendency is not very clear, but the best results are obtained when using 8192 which is the maximum training sequence length available for the current model.</p> <p>Submission time increases slightly.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#lora-rank","title":"Lora rank","text":"<p>It might seem that using a bigger lora rank is beneficial.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#uncertainty-on-the-lb-results","title":"Uncertainty on the LB results","text":"<p>Let's submit the same configuration 5 times, just changing the random seed.</p> <p></p> <p>This was very surprising because I wasn't expecting this level of variability. We can see a variation up to 3.5 points within the same submission just by changing the random seed.</p> <p>This probably invalidates all the previous conclusions, because the difference in scores between experiments was not that high to be conclusive.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#learning-rate","title":"Learning rate","text":"<p>I might get better results with a lower learning rate and longer training?</p> <p></p> <p>Clearly the learning rate has great influence on the score, I believe I should do a deeper study on a following iteration.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#inference-parameters","title":"Inference parameters","text":"<p>I have done a few experiments with the number of predictions (<code>n</code>) and the <code>min_prob</code> without conclusive results.</p> <p>This experiment was done with an earlier notebook that used 8 folds for splitting the data, I changed the number of predictions from 8 to 16 with very small variation in score.</p> train epochs n min_prob lora_rank LB score 6 2 0.17 32 6.67 6 1 0.17 32 6.25 <p>This other experiment was done with the single task fine-tuning setup. We modify the <code>min_prob</code> but the effect is unclear.</p> train epochs n min_prob lora_rank LB score 10 1 0.17 16 11.94 10 1 0.13 16 7.92 10 1 0.17 32 11.1 10 1 0.13 32 11.1 <p>Finally I have also done a sweep over <code>min_prob</code> when evaluating the evaluation set.</p> min_prob eval score runtime (h) None 11.4 5.5 0.35 10.1 3.33 0.25 9.3 3.66 0.17 10.1 4.1 0.10 10.7 5.5"},{"location":"modeling/Iteration_03_ideal_test_time_training/#conclusion","title":"Conclusion","text":"<p>On this iteration I have been able to improve the LB score to 11.94, and I was the first team to beat the 10% barrier. However I have noticed that LB scores have a variability of up to 3.5 between submissions of the same configuration. Thus I believe I should stop making submissions and only return when I have made progress on the evaluation set.</p>"},{"location":"modeling/Iteration_03_ideal_test_time_training/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_03_ideal_test_time_training/#todo","title":"TODO","text":"<ul> <li> How GPU usage looks when using batch size 1?</li> <li> What if I copy the base model to <code>dev/shm</code></li> <li> Tune the submission hyperparameters. My intuition is that I should train as long as possible, and make just 8 predictions per task.<ul> <li> Lora rank</li> <li> Number of training epochs (better change epochs than learning rate when possible)</li> <li> Inference parameters (n and min_prob)</li> <li> Learning rate</li> <li> Uncertainty on the results (what if I change the random seed?)</li> <li> Are the training samples correctly sorted? Maybe they are not optimal for single task training. The order is random.</li> </ul> </li> <li> Check the evaluation prints of the architects. They are different to normal scoring</li> <li> Make more evaluations on the evaluation set and compare to test set. I want to see a correlation of runtime and score.</li> <li> What if I use 2 GPU slots for training? Currently just 40% of GPU memory is used.</li> </ul>"},{"location":"modeling/Iteration_04_first_steps_with_code/","title":"Iteration 4. First steps with code","text":""},{"location":"modeling/Iteration_04_first_steps_with_code/#iteration-4-first-steps-with-code","title":"Iteration 4. First steps with code","text":"<p>23-04-2025</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#goal","title":"Goal","text":"<p>Can I teach a model to draw?</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#motivation","title":"Motivation","text":"<p>I want to start working towards my idea of solving ARC using code and reinforcement learning.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#development","title":"Development","text":""},{"location":"modeling/Iteration_04_first_steps_with_code/#qwen25-coder","title":"Qwen2.5 Coder","text":"<p>qwen2.5-coder-family might be a good race-horse for ARC. There are many different model sizes: 0.5B, 1.5B, 3B, 7B, 14B and 32B. The smaller models have a context window of 32k tokens, from the 7B it is 128k.</p> <p>I would start with the smallest model and there is always time to use a bigger model.</p> <p>More information:</p> <ul> <li>Released on November 12, 2024</li> <li>Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o.</li> <li>Code repair is an important programming skill. Qwen2.5-Coder-32B-Instruct can help users fix errors in their code, making programming more efficient. </li> <li>For each size, we open-sourced both Base and Instruct models, where the Instruct model serves as an official aligned model that can chat directly, and the Base model serves as a foundation for developers to fine-tune their own models.</li> </ul> <p></p> <ul> <li>Bigger models score higher in the benchmarks as expected, it resembles a typical log-linear relation.</li> <li>One weird thing is that the 3B model has the Qwen-research license instead of Apache 2.0 license. The Qwen-research license does not allow for commercial use but I could probably use it for the ARC challenge.</li> </ul>"},{"location":"modeling/Iteration_04_first_steps_with_code/#the-right-tool-for-each-job","title":"The right tool for each job","text":"<ul> <li>VLLM seems to be the fasts option for inference. On my ARC24 solution I was able to make 96 predictions per task, and I believe the time used for inference was around 2 hours. Probably the architects didn't use VLLM because they created their own depth first. Dynamically serving LoRA Adapters </li> <li>unsloth enables memory efficient and faster fine-tuning on a single gpu</li> <li>I could create some server to run the generated code on CPU</li> </ul> <p>One interesting implementation would be to have independent services for: inference, fine-tuning and running code. And a master process would call this services, this master process would be very light because all the heavy work will be handed to the services.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#dsl-definition","title":"DSL definition","text":"<ul> <li>There needs to be a document that describes the DSL, it might be a docstring in the python module.</li> <li>Polymorphism (functions that receive different data types) will make for a more simple DSL</li> <li>All primitive functions need tests</li> <li>All primitive functions need training samples of how to use them, we will use them to fine-tune the LLM.</li> <li>The training samples should have different levels of complexity. There should be very simple examples with just one primitive function, and more complex examples with multiple primitive functions. I might need some criteria to validate training samples, for example I could test what would happen if removing lines of code and if the output does not change it means those lines are not necessary.</li> <li>LLMs could be helpful to generate new training samples (This was done in the Transduction and induction paper).</li> <li>I could use my own DSL from ARC24 as a start point, I could also have a look at this other DSL from BARC</li> </ul> <p>The idea is to define a first version of the DSL, train a first model on it and see how it performs on inference.</p> <p>The development of the DSL would follow these steps:</p> <ol> <li>Write the DSL function</li> <li>Add tests and verify they run with <code>make test</code></li> <li>Add examples of how to use the function and visualize them in a jupyter notebook</li> </ol> <p>I'm going to use python 3.10 because it's the same version as the Kaggle notebook.</p> <p>I have been researching and it seems that the way to do dynamic code generation is to generate code in string and use <code>exec</code> to run the code. We can use <code>inspect.get_source</code> to get the code from a function but it is not that flexible.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#training-script","title":"Training script","text":"<p>I'm going to create a first training script that will use the arc24 script as a start point. However I'm going to change how I handled the configuration, I'm going to try the library tyro that seems to have the functionality I want: define a configuration object that can be later modified through arguments.</p> <pre><code>python fine-tuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/debug --no-log-to-wandb --device-map balanced --random-seed 7 --max-steps 31\npython fine-tuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_7 --device-map balanced --random-seed 7 --max-steps 200\n\npython fine-tuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_6 --device-map balanced --random-seed 6 --max-steps 200\n\npython fine-tuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250430_first_trainings/random_seed_5_no_dora --device-map balanced --random-seed 5 --max-steps 200\n</code></pre>"},{"location":"modeling/Iteration_04_first_steps_with_code/#inference-with-vllm","title":"Inference with VLLM","text":"<p>I had problems trying to use LoRAs with VLLM, I updated the version but without too much success. I ended up merging the LoRA with the base model to do predictions with VLLM.</p> <pre><code>(arc) gbarbadillo@africanus:/mnt/hdd0/MEGA/AI/22_Kaggle/arc25$ conda list | grep vllm\nvllm                      0.5.5                    pypi_0    pypi\nvllm-flash-attn           2.6.1                    pypi_0    pypi\n\n(arc) gbarbadillo@africanus:/mnt/hdd0/MEGA/AI/22_Kaggle/arc25$ conda list | grep vllm\nvllm                      0.8.5                    pypi_0    pypi\nvllm-flash-attn           2.6.2                    pypi_0    pypi\n</code></pre>"},{"location":"modeling/Iteration_04_first_steps_with_code/#environment-creation","title":"Environment creation","text":"<pre><code>conda create --name arc25 python=3.10\nconda activate arc25\npip install -r requirements.txt\npip install flash-attn==2.6.3 --no-build-isolation\nmake develop\n</code></pre>"},{"location":"modeling/Iteration_04_first_steps_with_code/#results","title":"Results","text":"<p>I have trained a model to learn to draw up to 5 elements in a small image (side up to 10 pixels).</p> <p></p> <p></p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#accuracy-on-the-train-distribution","title":"Accuracy on the train distribution","text":""},{"location":"modeling/Iteration_04_first_steps_with_code/#effect-of-sampling-temperature","title":"Effect of sampling temperature","text":"<p>This plot shows that when sampling multiple responses is better to use a temperature around 0.5 because it improves the pass rate. The model is able to solve around 97% of the tasks when doing 8 predictions per task.</p> <p>The evaluation is very fast, doing around 44 predictions per second.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#effect-of-the-number-of-predictions","title":"Effect of the number of predictions","text":"<p>We see a nice improvement in pass rate when increasing the number of predictions, as expected.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#number-of-training-steps","title":"Number of training steps","text":"<p>Let's see the effect that the number of training steps has on the accuracy.</p> <p></p> <p>We see a very good trend when increasing the number of training samples. This is expected because we are using in distribution samples for evaluation.</p> <p>The longest training used 1e5 samples and took around 4 hours. I'm pretty sure it can be done much faster because GPU usage was low.</p> <p>Maybe we need around 1e6 samples to do the task with close to 100% accuracy.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#out-of-distribution-results","title":"Out of distribution results","text":"<p>Let's now focus on the out of distribution results, that require higher generalization. These are the most important ones because they are the focus of the ARC challenge.</p> <p>We can think of 3 different dimensions to study the behaviour of the model with out of distribution tasks. The model has been trained with tasks with 1-5 drawings and an image size between 3 and 10.</p> <p>The following plots show a clear degradation of accuracy when we got farther from the training distribution.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#number-of-drawing-functions","title":"Number of drawing functions","text":""},{"location":"modeling/Iteration_04_first_steps_with_code/#image-size","title":"Image size","text":""},{"location":"modeling/Iteration_04_first_steps_with_code/#image-content","title":"Image content","text":"<p>It seems that the model is not comparing the input and output image pixels, it only needs to know the color of the input image.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#conclusion","title":"Conclusion","text":"<p>We have trained a model on around 1e5 random drawings and it reached a pass rate above 98%. Training for longer will likely improve the results. Using a sampling temperature around 0.5 improves the pass rate due to higher variability.</p> <p>We have observed that the model struggles to generalize to out of distribution data.</p>"},{"location":"modeling/Iteration_04_first_steps_with_code/#next-steps","title":"Next steps","text":"<ul> <li>Base or Instruct model? On Qwen they recommend the base model if we are going to fine-tune.</li> <li>lmdeploy seems to be a faster alternative to VLLM</li> <li>Work to solve the OOD samples, that could validate my ideas of hindsight experience replay and RL</li> <li>Qwen3 has been released, but there aren't benchmarks about the smaller models.</li> <li>Optimize hardware utilization during training</li> </ul>"},{"location":"modeling/Iteration_04_first_steps_with_code/#todo","title":"TODO","text":"<ul> <li> Create a first training script, using arc24 as a start point<ul> <li> Check the tokenizer does not merge numbers</li> </ul> </li> <li> Fine-tune a first model to learn to draw</li> <li> Evaluate how good the model is on new tasks (in and out of distribution)<ul> <li> pass@n, acc@n, correct_pixels</li> <li> Temperature?</li> <li> Number of training steps?</li> <li> Robustness to bad code</li> <li> Out of distribution evaluation</li> </ul> </li> <li> Is this approach promising?</li> <li> Install flash-attn <ul> <li> (<code>USE_FLASH_ATTENTION=1</code>), seems to need the cuda toolkit installed.</li> <li> https://discuss.pytorch.org/t/is-cuda-installed-automatically-when-following-pytorch-instlalation-directions/215493</li> <li> <code>pip install flash-attn==2.6.3 --no-build-isolation</code> This works!</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/","title":"Iteration 5. Test-time training with code. Hindsight Experience Replay (HER)","text":""},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#iteration-5-test-time-training-with-code-hindsight-experience-replay-her","title":"Iteration 5. Test-time training with code. Hindsight Experience Replay (HER)","text":"<p>04-05-2025</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#goal","title":"Goal","text":"<p>Explore if Hindsight Experience Replay (HER) can help to solve new tasks with code.</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#motivation","title":"Motivation","text":"<p>My idea is to pick a simple case that the model is not able to solve, for example: all vertical lines with all the colors. I first have to verify that the model is unable to solve it. Then check if using HER helps to solve the task.</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#development","title":"Development","text":""},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#design-ood-tasks","title":"Design OOD tasks","text":"<p>I have prepared 3 simple but OOD tasks where the model is not able to find a solution despite being sampled 256 times.</p> <p> </p> <pre><code># vertical lines\nmean_correct_pixels: 65.32%\nmax_correct_pixels: 88.89%\n\n# squares\nmean_correct_pixels: 64.55%\nmax_correct_pixels: 81.48%\n\n# overlapping squares\nmean_correct_pixels: 62.66%\nmax_correct_pixels: 93.00%\n</code></pre> <p>My guess is that the first two tasks are not solved due to requiring 9 draws (the model was trained with up to 5). The last one might be difficult due to the overlapping squares.</p> <p>\u00bfCould test-time training allow the model to solve this tasks?</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#inference-throughput-with-transformers-library","title":"Inference throughput with transformers library","text":"batch size inference time(s) throughput (preds/s) 1 6.4 0.2 4 7.4 0.5 16 8.5 1.9 64 9 7.1 128 10.9 11.7 256 15.3 16.7 512 30.1 17.0 <p>A batch size of 256 might be the sweet spot. It takes just twice as making two predictions with batch size 1.</p> <p>This shows that the batch size is very important when using the transformers library for inference.</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#algorithm","title":"Algorithm","text":"<p>Experiments were done on the notebook 003_test_time_training_exploration_HER</p> <ol> <li>Given the inputs and outputs the model generates 256 code predictions with a temperature of 0.5</li> <li>The predictions are run to generate outputs images.</li> <li>Keep only one prediction per output</li> <li>Validate the predicted code (remove code that does not affect the output)</li> <li>Create new tasks using the predicted code</li> <li>Sort the tasks by ascending order using the pixel accuracy of the prediction. Worst predictions come first.</li> <li>Fine-tune the model on these new tasks</li> <li>Repeat all the steps above until a perfect solution is achieved or the maximum number of epochs is reached.</li> </ol>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#results","title":"Results","text":""},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#her-allows-to-solve-tasks-with-25-squares","title":"HER allows to solve tasks with 25 squares","text":"<p>HER allows to solve tasks with 25 squares for a model that was trained with just up to 5 objects.</p> <p></p> <p></p> <p>The model starts with a maximum accuracy of less than 50% and in 6 epochs is able to achieve perfect accuracy. This took 900 seconds on a single 3090 GPU without any optimization.</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#weaker-models","title":"Weaker models","text":"<p>If we use weaker models (models trained for a smaller number of iterations) they can still benefit from HER. Their start point is worse, and they cannot solve new tasks as complex as the stronger models but nonetheless HER allows adaptation to novel tasks.</p> <p>For example a model trained for 400 steps can solve a task with 9 vertical lines, but cannot solve a task with 12 squares. A model trained for 800 steps can solve the 12 squares task, and before we have seen that the strongest model (trained for 3200 steps) can solve tasks with 25 squares.</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#is-the-reward-necessary","title":"Is the reward necessary?","text":"<p>On my first implementation I have used the accuracy of the predictions to sort the new tasks. When fine-tuning the model it first uses the worst predictions and sees the best predictions at the end of the training. This was designed with the goal to avoid catastrophical forgetting on the best predictions.</p> <p>If we shuffle the tasks it would mean that we are no longer using rewards for learning. That would be a quite interesting algorithm.</p> <p>I have verified that the algorithm is able to solve tasks with 20 and 25 squares without using any reward information.</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#conclusion","title":"Conclusion","text":"<p>We have probed that Hindsight Experience Replay (HER) is a technique that allows the model to adapt to new tasks. A model trained to draw up to 5 objects, was able to do a task requiring to draw 25 objects. That is quite a generalization gap, although arguably we will see bigger gaps in the ARC challenge.</p> <p>One interesting thing about the HER technique is that it does not require any reward function. On my first implementation I used the accuracy to sort the tasks, but I have probed that the algorithm works fine with random order.</p>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#next-steps","title":"Next steps","text":"<ul> <li>It would probably be better to use different temperatures for the predictions, having a balance between   exploration and explotation. However it seems that transformers library does not allow that.</li> </ul>"},{"location":"modeling/Iteration_05_test_time_training_with_code_HER/#todo","title":"TODO","text":"<ul> <li> Find a simple task that the model is not able to solve</li> <li> Does Hindsight experience replay helps to learn to do the task?</li> <li> Colab pro: https://www.kaggle.com/docs/notebooks#increase-gpu-compute-with-colab-pro, Now I have 45 hours per week at Kaggle!</li> </ul>"},{"location":"modeling/Iteration_06_reinforcement_learning/","title":"Iteration 6. Reinforcement learning","text":""},{"location":"modeling/Iteration_06_reinforcement_learning/#iteration-6-reinforcement-learning","title":"Iteration 6. Reinforcement learning","text":"<p>08-05-2025</p>"},{"location":"modeling/Iteration_06_reinforcement_learning/#goal","title":"Goal","text":"<p>Can we use RL to solve novel tasks?</p>"},{"location":"modeling/Iteration_06_reinforcement_learning/#motivation","title":"Motivation","text":"<p>I have already seen that Hindsight Experience Replay (HER) is helpful to adapt a model to new tasks. On this iteration I want to try with RL, specifically with the popular Group Relative Policy Optimization (GRPO) algorithm.</p> <p>Probably the best option is a combination of the two approaches, but first I have to find if GRPO is helpful. I would also like to see how it compares with HER in terms of efficiency.</p>"},{"location":"modeling/Iteration_06_reinforcement_learning/#development","title":"Development","text":"<p>TRL has already implemented GRPO, so testing it will be very easy.</p> <p>I will be doing the experiments on the notebook 005_GRPO_exploration</p>"},{"location":"modeling/Iteration_06_reinforcement_learning/#reference-grpo-parametrization","title":"Reference GRPO parametrization","text":""},{"location":"modeling/Iteration_06_reinforcement_learning/#reinforcement-learning-for-reasoning-in-large-language-models-with-one-training-example","title":"Reinforcement Learning for Reasoning in Large Language Models with One Training Example","text":"<ul> <li>Qwen2.5-Math-1.5B</li> <li>8 responses per prompt</li> <li>mini-batch size 128</li> <li>Batch size is 128 (number of prompts per step), they repeat the same prompt 128 times.</li> <li>learning rate: 1e-6</li> <li>temperature: 0.6</li> <li>Train for 2000 steps</li> </ul>"},{"location":"modeling/Iteration_06_reinforcement_learning/#llms-for-engineering-teaching-models-to-design-high-powered-rockets","title":"LLMs for Engineering: Teaching Models to Design High Powered Rockets","text":"<ul> <li>Batch size 64</li> <li>Qwen 2.5 7B</li> </ul>"},{"location":"modeling/Iteration_06_reinforcement_learning/#results","title":"Results","text":"<p>After solving the problem with timeouts I'm going to set the maximum number of tokens to 768, solving the task with 25 squares needed less than 600 tokens. That will enforce the model to keep the functions short.</p> <p>wandb</p> <p>9-vertical-lines:</p> <ul> <li>Solved on 61 steps with lr=1e-6 and num_generations=8</li> <li>With lr=2e-6 it is solved at step 21, 3m39s</li> <li>With lr=4e-6 it is solved in 48s, 5 steps</li> </ul> <p>12-squares:</p> <ul> <li>with lr=4e-6 solves the task in 1194s, 73 steps.</li> <li>If I increase the number of generations to 16, it takes 150 steps and almost 100 minutes</li> <li>with lr=2e-5 and 16 generations it does not converge despite spending 140 minutes</li> </ul> <p>16-squares:</p> <ul> <li>with lr=4e-6 the training diverges</li> <li>with lr=2e-6 I have stopped the training at epoch 123 and 70 minutes. It might seem that the training was going to diverge. At least I need something much faster.</li> </ul>"},{"location":"modeling/Iteration_06_reinforcement_learning/#conclusion","title":"Conclusion","text":"<p>Despite doing a lot of experiments changing the learning rate and number of generations, I have not been able to solve a task with 16 squares when using GRPO. It solves the 9 vertical lines very fast, and it is capable of solving the 12 squares task but not consistently.</p> <p>Either GRPO is less sample efficient than HER or I haven't found the right configuration. I believe it is the first hypothesis because GRPO only uses the information of the reward to learn, whereas HER can use the whole output so it uses much more information.</p>"},{"location":"modeling/Iteration_06_reinforcement_learning/#next-steps","title":"Next steps","text":"<ul> <li>I might revisit GRPO in the future. When reading papers that use GRPO, check the parametrization.</li> <li>Using VLLM would speedup the algorithm</li> </ul>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/","title":"Iteration 7. Optimize TTT on the evaluation set","text":""},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#iteration-7-optimize-ttt-on-the-evaluation-set","title":"Iteration 7. Optimize TTT on the evaluation set","text":"<p>10-05-2025</p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#goal","title":"Goal","text":"<p>Optimize the hyperparameters of TTT on the evaluation set, and then make submissions to the test set.</p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#motivation","title":"Motivation","text":"<p>On previous iterations I have seen that there is variability on LB scores. There could be changes up to 3.5 points between submissions of the same configuration. My current best LB score is probably luck.</p> <p>Previously I had the problem that evaluations took around 5 hours, and Kaggle only allows 15 hours per week for the 4xL4 machine. So at max I could do 3 evaluations per week which is very limited.</p> <p>By analyzing all the evaluation set evaluations I have found that only 22 tasks were solved once. Thus I could use just those tasks instead of the whole 120 tasks to speedup evaluation. Moreover I have linked Google Colab Plus to Kaggle and now I have 22 hours per week. This means that now I could do around 22 evaluations per week, and that opens the door to optimize the hyperparameters on the evaluation set.</p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#development","title":"Development","text":""},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#results","title":"Results","text":""},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#lora-rank-and-learning-rate","title":"Lora rank and learning rate","text":"<p>Google sheet</p> <p></p> <p>The best learning rate seems to be around 2e-4, there is no evidence that lora 32 is better.</p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#number-of-predictions","title":"Number of predictions","text":"<p>Let's study if increasing the number of predictions has a significative effect on the accuracy.</p> <p></p> <p>There is no evidence suggesting that using more than 8 predictions is beneficial. Using 2 or 4 predictions is clearly not enough. 8 seems to be the sweet spot.</p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#min_prob","title":"min_prob","text":"<p>For the first time we are scoring above 12 on the evaluation set. Despite the randomness of the scores we see a clear trend of improvement when using a lower value of <code>min_prob</code>. The drawback is that it requires more execution time.</p> <p></p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#max_seq_length","title":"max_seq_length","text":"<p>It seems that if it is too small is it hurtful, otherwise there isn't a relation between the parameters. Thus I believe I should use the maximum possible value to be able to solve tasks with big grids.</p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#repeatability","title":"Repeatability","text":"configuration mean eval score n_experiments epochs=10, n=1, lr=1e-4, r=16, min_prob=0.17 9.8 \u00b1 0.7 13 epochs=10, n=1, lr=2e-4, r=4, min_prob=0.17 10.4 \u00b1 0.8 11 epochs=10, n=1, lr=2e-4, r=4, min_prob=0.1 10.6 \u00b1 0.7 11 <p>Evidence is not very strong, but it seems that for the evaluation set we have found a better configuration than the previous one.</p> <p>However I have made submissions with the new configuration, and on the test set I don't see a difference.</p> configuration mean test score n_experiments epochs=10, n=1, lr=1e-4, min_prob=0.17, r=32 10.5 \u00b1 1.3 5 epochs=10, n=1, lr=1e-4,  min_prob=0.17, r=16 9.9 \u00b1 1.7 5 epochs=10, n=1, lr=2e-4, min_prob=0.17, r=4, 10.3 \u00b1 1.3 5 <p>Link to Google Sheet</p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#conclusion","title":"Conclusion","text":"<p>We have optimized the hyperparameters for the evaluation set, but the improvements didn't transfer to the test set.</p>"},{"location":"modeling/Iteration_07_optimize_ttt_on_evaluation_set/#next-steps","title":"Next steps","text":"<ul> <li>It might be helpful to initialize the LoRA on the new ARC25 tasks. Last year I observed that it was better to start from pretrained LoRA than to use a fresh new one.</li> </ul>"},{"location":"modeling/Iteration_08_improve_HER/","title":"Iteration 8. Improve HER","text":""},{"location":"modeling/Iteration_08_improve_HER/#iteration-8-improve-her","title":"Iteration 8. Improve HER","text":"<p>10-05-2025</p>"},{"location":"modeling/Iteration_08_improve_HER/#goal","title":"Goal","text":"<p>Can I optimize Hindsight Experience Replay (HER) to draw a chick?</p> <p></p>"},{"location":"modeling/Iteration_08_improve_HER/#motivation","title":"Motivation","text":"<p>I have already probed that HER enables a model trained to solve tasks with up to 5 objects to solve a task with 25 squares, but can it solve arbitrary images?</p> <p>The idea is to optimize the algorithm and the parameters so it is able to draw the chick of the image above. If I can do that I will be confident to the next step that will be extend the DSL. So far I have been unable to solve the chick task, an accuracy of around 97-98% is reached but perfect accuracy is eluding.</p>"},{"location":"modeling/Iteration_08_improve_HER/#development","title":"Development","text":"<p>I will be working on this notebook 006_HER_v2.</p>"},{"location":"modeling/Iteration_08_improve_HER/#train-a-more-powerful-base-model","title":"Train a more powerful base model","text":""},{"location":"modeling/Iteration_08_improve_HER/#improve-gpu-usage","title":"Improve gpu usage","text":"<p>I believe I can speedup the training just by using a bigger batch size per device.</p> <pre><code>python finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250511_optimize_GPU_sage/random_seed_5_no_dora --device-map auto --random-seed 5 --max-steps 50 --n-gpus 1 --per-device-train-batch-size 1 --batch-size 16 --max-seq-len 4096\npython finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250511_optimize_GPU_sage/per-device-batch-size-8 --device-map auto --random-seed 5 --max-steps 20 --n-gpus 1 --per-device-train-batch-size 8 --batch-size 16 --max-seq-len 4096\n\npython finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250511_optimize_GPU_sage/per-device-batch-size-4_2gpus --device-map auto --random-seed 5 --max-steps 20 --n-gpus 2 --per-device-train-batch-size 8 --batch-size 16 --max-seq-len 4096\n\n# https://ironbar.github.io/arc24/modeling/Iteration_50_last_trainings/#steps-to-train-the-model\n\naccelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250511_optimize_GPU_sage/per-device-batch-size-8_2gpus_accelerate --device-map None --random-seed 5 --max-steps 40 --n-gpus 2 --per-device-train-batch-size 8 --batch-size 16 --max-seq-len 512\n\n\n# one gpu\nper-device-batch train_samples/s\n1 6.15\n2 10.1\n4 16\n8 17\n\n# two gpus (not working yet)\n</code></pre>"},{"location":"modeling/Iteration_08_improve_HER/#training-command","title":"Training command","text":"<p>Previously I trained the longest model for 6k steps, that would take just 1h15 with the new setup. So 16k would be around 3 hours and 32k would be around 6 hours.</p> <p>I'm using the same LoRA configuration as the previous trainings.</p> <pre><code>export CUDA_VISIBLE_DEVICES=0\npython finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250511_longer_trainings/32k_steps \\\n--device-map auto \\\n--max-steps 32000 \\\n--n-gpus 1 \\\n--per-device-train-batch-size 8 \\\n--batch-size 16 \\\n--max-seq-len 512 \\\n--logging-steps 100 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\n</code></pre>"},{"location":"modeling/Iteration_08_improve_HER/#improvements-to-the-algorithm","title":"Improvements to the algorithm","text":"<ul> <li>When multiple new tasks share the same output, choose the task with the shortest code</li> <li>Do not train multiple times on the same tasks</li> <li>Add wanbd with images for logging</li> </ul>"},{"location":"modeling/Iteration_08_improve_HER/#results","title":"Results","text":"<p>The updated implementation is able to solve consistently all the tasks from iteration 5. For example it solves the 25-squares task in 10-12 minutes whereas that took 15 minutes in the previous implementation and I believe it was not as consistent.</p> <p>Moreover I have been able to solve the chick task, but not consistently. I believe that I might need a better model to solve the task consistently, because it does not require more lines of code, but higher precision.</p>"},{"location":"modeling/Iteration_08_improve_HER/#using-a-model-trained-for-32k-steps","title":"Using a model trained for 32k steps","text":"<p>Can a model trained for 32k steps (instead of 6k steps) solve the chick task consistently?</p> <p>The table below shows the number of epochs needed to solve each task.</p> task 6k steps model 32k steps model 9-vertical-lines 2 2 12-squares 4 2 16-squares 7 4 20-squares 7 7 25-squares 10 11 <p>In general it seems that the model trained for longer solves the task earlier as expected.</p> <p>However it does not solve the chick task consistently. After inspecting the best predictions for each epoch I see that it had all the details right but never on the same epoch.</p>"},{"location":"modeling/Iteration_08_improve_HER/#exploration-and-exploitation","title":"Exploration and exploitation","text":"<p>Maybe I have to use both high temperature for exploration and low temperature for exploitation.</p> <pre><code>inference_params=[\n    InferenceParams(num_return_sequences=8, temperature=0.1),\n    InferenceParams(num_return_sequences=128, temperature=0.9),\n]\n</code></pre> <p>When using just high temperature the success rate of the chick task was 2/9, after combining exploration and exploitation it increased to 4/5.</p> <p>Thus we could say that we have achieved the goal of solving consistently the chick task.</p>"},{"location":"modeling/Iteration_08_improve_HER/#number-of-generations","title":"Number of generations","text":"<p>It might be better to use even smaller number of generations, because that would make the policy to change more smoothly. My current implementation is not efficient for small batch sizes, but a future implementation might be.</p> <p></p> <p>The optimal number of generations per epoch depends on the complexity of the task. It might seem that 16 could be a good choice because works well in all the cases. It requires between 2 and 5 less generation than generating 128 predictions per epoch.</p> <p>We could thought that the best option was to do a single prediction and generation per epoch, but that does not seem to be the case considering that 4 generations per epoch needs more generations than 16 generations per epoch. Or maybe is a problem with my implementation.</p> <p>The problem is that with my current implementation using large batch sizes is much more efficient, so despite this plots it is faster to make 128 generation per epoch.</p>"},{"location":"modeling/Iteration_08_improve_HER/#visualization-of-solving-the-chick-task","title":"Visualization of solving the chick task","text":""},{"location":"modeling/Iteration_08_improve_HER/#conclusion","title":"Conclusion","text":"<p>On this iteration I have improved the HER algorithm and solved consistently the chick task. I have seen that combining low and high temperature when sampling is helpful to be consistent, that is the old exploration-exploitation dilemma.</p>"},{"location":"modeling/Iteration_08_improve_HER/#next-steps","title":"Next steps","text":"<ul> <li>Start working with ARC tasks</li> <li>Make the script work with accelerate</li> <li>There might be a problem with the train dataset, the generator function is called 4-5 times. This might require to set random seed to None.</li> </ul>"},{"location":"modeling/Iteration_09_improve_training_script/","title":"Iteration 9. Improve training script","text":""},{"location":"modeling/Iteration_09_improve_training_script/#iteration-9-improve-training-script","title":"Iteration 9. Improve training script","text":"<p>14-05-2025</p>"},{"location":"modeling/Iteration_09_improve_training_script/#goal","title":"Goal","text":"<p>Improve the training script so I can start working towards solving real ARC tasks with code.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#motivation","title":"Motivation","text":"<p>I have seen that Hindsight Experience Replay (HER) allows to generalize to novel tasks. Next step is to probe that it can solve real ARC tasks, not just toy tasks. But previously I have to make some updates to the training script. That will allow me to iterate faster on the next steps.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#development","title":"Development","text":""},{"location":"modeling/Iteration_09_improve_training_script/#fix-the-problem-with-repeated-calls-to-training-generator","title":"Fix the problem with repeated calls to training generator","text":"<pre><code>conda activate arc25\nexport CUDA_VISIBLE_DEVICES=0\npython finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/debug_training_generator --device-map auto --random-seed 5 --max-steps 11 --n-gpus 1 --per-device-train-batch-size 1 --batch-size 16 --max-seq-len 1024 --no-log-to-wandb --no-resume-from-checkpoint\n</code></pre> <ul> <li>IterableDataset</li> <li>SFTTrainer</li> </ul> <p>It seems that it is the expected behaviour, however I have modified the generator to just yield samples. The setting of the random seed and printing the first sample is now outside.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#make-the-script-work-with-accelerate","title":"Make the script work with accelerate","text":"<pre><code>accelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/debug_accelerate --device-map None --random-seed 5 --max-steps 40 --n-gpus 2 --per-device-train-batch-size 8 --batch-size 16 --max-seq-len 512 --no-log-to-wandb --no-resume-from-checkpoint\n</code></pre> <p>I'm using the latest version of accelerate: <code>1.6.0</code>, the thing is that previously the <code>SFTConfig</code> class had a <code>dispatch_batches=False</code> parameter that now is missing.</p> <ul> <li>https://huggingface.co/docs/accelerate/en/package_reference/accelerator</li> <li>https://huggingface.co/docs/accelerate/v1.6.0/en/package_reference/utilities#accelerate.DataLoaderConfiguration</li> <li>https://github.com/huggingface/transformers/issues/34699</li> <li>https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments</li> </ul> <p>The solution was easy, but difficult to find: <code>accelerator_config=dict(dispatch_batches=False</code></p>"},{"location":"modeling/Iteration_09_improve_training_script/#training-speed-test","title":"Training speed test","text":"<p>By using 2 GPUs and the right batch size we can improve the training speed by a factor of 5.</p> Number of GPUs Per Device Batch Size Train Samples per Second 2 8 44.25 1 8 25.69 1 4 22.27 1 2 14.70 1 1 8.85 <pre><code>accelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/debug_accelerate --device-map None --random-seed 5 --max-steps 100 --n-gpus 2 --per-device-train-batch-size 8 --batch-size 16 --max-seq-len 512 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# {'train_runtime': 36.1583, 'train_samples_per_second': 44.25, 'train_steps_per_second': 2.766, 'train_loss': 0.2923687481880188, 'epoch': 1.0}\nexport CUDA_VISIBLE_DEVICES=0\naccelerate launch --num_processes 1 --num_machines 1 --mixed_precision bf16 \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/debug_accelerate --device-map None --random-seed 5 --max-steps 100 --n-gpus 1 --per-device-train-batch-size 8 --batch-size 16 --max-seq-len 512 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# {'train_runtime': 63.3117, 'train_samples_per_second': 25.272, 'train_steps_per_second': 1.579, 'train_loss': 0.2931043267250061, 'epoch': 1.0}\npython finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/debug_accelerate --device-map None --random-seed 5 --max-steps 100 --n-gpus 1 --per-device-train-batch-size 8 --batch-size 16 --max-seq-len 512 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# {'train_runtime': 62.2894, 'train_samples_per_second': 25.687, 'train_steps_per_second': 1.605, 'train_loss': 0.29407034754753114, 'epoch': 1.0}\npython finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/debug_accelerate --device-map None --random-seed 5 --max-steps 100 --n-gpus 1 --per-device-train-batch-size 4 --batch-size 16 --max-seq-len 512 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n#{'train_runtime': 71.8484, 'train_samples_per_second': 22.269, 'train_steps_per_second': 1.392, 'train_loss': 0.29404119253158567, 'epoch': 1.0}\npython finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/debug_accelerate --device-map None --random-seed 5 --max-steps 100 --n-gpus 1 --per-device-train-batch-size 2 --batch-size 16 --max-seq-len 512 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n#{'train_runtime': 108.8354, 'train_samples_per_second': 14.701, 'train_steps_per_second': 0.919, 'train_loss': 0.29236586928367614, 'epoch': 1.0}\npython finetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/debug_accelerate --device-map None --random-seed 5 --max-steps 100 --n-gpus 1 --per-device-train-batch-size 1 --batch-size 16 --max-seq-len 512 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# {'train_runtime': 180.7981, 'train_samples_per_second': 8.85, 'train_steps_per_second': 0.553, 'train_loss': 0.29323326468467714, 'epoch': 1.0}\n</code></pre>"},{"location":"modeling/Iteration_09_improve_training_script/#training-speed-vs-input-size","title":"Training speed vs input size","text":"<p>Even after changing the per device batch size between experiments we can see a clear linear relation between the input tokens and the training speed.</p> <pre><code>accelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 100 --n-gpus 2 --per-device-train-batch-size 8 --batch-size 16 --max-seq-len 512 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# 1x10x10 5 draws, 'train_samples_per_second': 43.004,\naccelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 50 --n-gpus 2 --per-device-train-batch-size 4 --batch-size 16 --max-seq-len 1024 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# 2x10x10 5 draws, 'train_samples_per_second': 23.6\naccelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 50 --n-gpus 2 --per-device-train-batch-size 2 --batch-size 16 --max-seq-len 2048 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# 4x10x10 5 draws, 'train_samples_per_second': 13.6\n# 1x20x20 5 draws, 'train_samples_per_second': 16.0\naccelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 25 --n-gpus 2 --per-device-train-batch-size 2 --batch-size 16 --max-seq-len 4096 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# 1x30x30 5 draws, 'train_samples_per_second': 9.607\n# 2x20x20 5 draws, 'train_samples_per_second': 9.815\naccelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 20 --n-gpus 2 --per-device-train-batch-size 1 --batch-size 16 --max-seq-len 8192 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# 3x20x20 5 draws,  'train_samples_per_second': 6.323\n# 4x20x20 5 draws, 'train_samples_per_second': 5.014\n# 2x30x30 5 draws, 'train_samples_per_second': 5.178\n# 3x30x30 5 draws, 'train_samples_per_second': 3.234\n# 5x20x20 5 draws, 'train_samples_per_second': 4.045\n# 6x20x20 5 draws, 'train_samples_per_second': 3.293\n# 4x30x30 5 draws, OOM\n# 4x27x27 5 draws, OOM\n# 4x26x26 5 draws, 'train_samples_per_second': 3.101\n</code></pre>"},{"location":"modeling/Iteration_09_improve_training_script/#training-speed-vs-output-size","title":"Training speed vs output size","text":"<pre><code>accelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 20 --n-gpus 2 --per-device-train-batch-size 1 --batch-size 16 --max-seq-len 8192 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# 3x20x20 1 draws, 'train_samples_per_second': 7.024\n# 3x20x20 5 draws,  'train_samples_per_second': 6.323\n# 3x20x20 10 draws,  'train_samples_per_second': 5.503\n# 3x20x20 20 draws,  'train_samples_per_second': 3.885\n</code></pre> <p>A function with 20 drawings is around 400 tokens, so the same as a single 20x20 image. ChatGPT says that the backpropagation step is 2-3 more expensive than the forward step, and that could explain the changes in training speed that we are observing when using a longer output.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#mixed-sizes-training","title":"Mixed-sizes training","text":""},{"location":"modeling/Iteration_09_improve_training_script/#first-experiments","title":"First experiments","text":"<p>Let's see how the speed is affected when we mix different input sizes. I will be using a single sample and 5 draws for this experiment. I will only change the side of the image.</p> <pre><code>accelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 25 --n-gpus 2 --per-device-train-batch-size 2 --batch-size 16 --max-seq-len 4096 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# 30, 'train_samples_per_second': 8.809\n# 5-30, 'train_samples_per_second': 13.018\n# 5, 'train_samples_per_second': 22.967\naccelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 25 --n-gpus 2 --per-device-train-batch-size 2 --batch-size 16 --max-seq-len 3072 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100\n# Packing\n# I should probably test this longer and check the loss\n# 5-30, packing=True, 'train_samples_per_second': 6.87\n# 5-30, packing=False, train_samples_per_second': 12.626\n# liger-kernel\n# 5-30, use_liger_kernel=True, 'train_samples_per_second': 9.95, 46% VRAM\n# 5-30, use_liger_kernel=False, 'train_samples_per_second': 13.069, 86% VRAM\n# 5-30, use_liger_kernel=True, x2 batch size, 'train_samples_per_second': 12.786, 63% VRAM\n# 5-30, use_liger_kernel=True, x4 batch size, 'train_samples_per_second': 13.883, 80% VRAM\n</code></pre> <p>These initial experiments show that when training with mixed sizes the training is faster. On this 3090 GPU liger kernels do not seem to add speed, although they reduce GPU memory usage and that is something interesting.</p> <p>I believe I need to do additional experiments with packing because in the documentation says:</p> <p>Note that if you use a packed dataset and if you pass max_steps in the training arguments you will probably train your models for more than few epochs, depending on the way you have configured the packed dataset and the training protocol.</p> <p>So maybe packing is slower but it is training with more data.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#packing-experiment","title":"Packing experiment","text":"<pre><code>accelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 25 --n-gpus 2 --per-device-train-batch-size 2 --batch-size 16 --max-seq-len 3072 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100 --no-packing --epochs 1\n# 25 steps, \n# {'train_runtime': 24.6856, 'train_samples_per_second': 16.204, 'train_steps_per_second': 1.013, 'train_loss': 0.5946265602111817, 'num_tokens': 420352.0, 'mean_token_accuracy': 0.8707410991191864, 'epoch': 1.0}\naccelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/speed_test --device-map None --random-seed 5 --max-steps 25 --n-gpus 2 --per-device-train-batch-size 2 --batch-size 16 --max-seq-len 3072 --no-log-to-wandb --no-resume-from-checkpoint --save-steps 100 --packing --epochs 1\n# 9 steps, there is an error on one sample, the training does not end\n# 20.54s\n</code></pre> <p>The training is not ending, the speedup is not that large and it seems to be doing weird thing with the examples, so I won't recommend using packing.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#shards-in-iterable-dataset","title":"Shards in iterable dataset","text":"<p>To be able to use multiple workers, I have to add shards to the IterableDataset.</p> <pre><code># Dataset\n{'train_runtime': 198.5722, 'train_samples_per_second': 16.115, 'train_steps_per_second': 1.007, 'train_loss': 0.3144468629360199, 'epoch': 1.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [03:18&lt;00:00,  1.01it/s]\n2025-05-15 15:54:47,738 - arc25.logging - INFO - wrapper - Executed fine_tuning_main in 218.9584 seconds\n2025-05-15 15:54:47,739 - arc25.logging - INFO - wrapper - Executed fine_tuning_main in 219.0947 seconds\n\n# IterableDataset\n{'train_runtime': 219.8297, 'train_samples_per_second': 14.557, 'train_steps_per_second': 0.91, 'train_loss': 0.3229031562805176, 'epoch': 1.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [03:39&lt;00:00,  1.10s/it]\n2025-05-15 15:48:18,818 - arc25.logging - INFO - wrapper - Executed fine_tuning_main in 222.6067 seconds\n2025-05-15 15:48:18,818 - arc25.logging - INFO - wrapper - Executed fine_tuning_main in 222.9027 seconds\n\n# With workers\n{'train_runtime': 216.0631, 'train_samples_per_second': 14.81, 'train_steps_per_second': 0.926, 'train_loss': 0.31195030570030213, 'epoch': 1.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [03:35&lt;00:00,  1.08s/it]\n2025-05-15 16:06:55,624 - arc25.logging - INFO - wrapper - Executed fine_tuning_main in 218.7438 seconds\n2025-05-15 16:06:55,626 - arc25.logging - INFO - wrapper - Executed fine_tuning_main in 218.7610 seconds\n\n# With shards and 4 workers\n{'train_runtime': 203.6106, 'train_samples_per_second': 15.716, 'train_steps_per_second': 0.982, 'train_loss': 0.31994509100914004, 'epoch': 1.0}\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 200/200 [03:23&lt;00:00,  1.02s/it]\n2025-05-15 17:30:28,515 - arc25.logging - INFO - wrapper - Executed fine_tuning_main in 206.2465 seconds\n2025-05-15 17:30:28,526 - arc25.logging - INFO - wrapper - Executed fine_tuning_main in 206.3744 seconds\n</code></pre>"},{"location":"modeling/Iteration_09_improve_training_script/#add-validation-dataset","title":"Add validation dataset","text":"<p>I will simply sample from the same training distribution, but do it once at the start of the training and with a different random seed.</p> <pre><code>accelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu \\\nfinetuning.py --output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250514/add_validation --device-map None --random-seed 5 --max-steps 500 --n-gpus 2 --per-device-train-batch-size 2 --per-device-eval-batch-size 4 --batch-size 16 --max-seq-len 3072 --no-resume-from-checkpoint --save-steps 200 --no-packing --eval-steps 50 --no-log-to-wandb\n</code></pre>"},{"location":"modeling/Iteration_09_improve_training_script/#train-a-model-on-multiple-tasks","title":"Train a model on multiple tasks","text":"<pre><code>export K_STEPS=32\naccelerate launch --num_processes 2 --num_machines 1 --mixed_precision bf16 --multi_gpu finetuning.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/20250515_baseline_painter/${K_STEPS}k_steps \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps ${K_STEPS}000 \\\n--n-gpus 2 \\\n--per-device-train-batch-size 2 \\\n--per-device-eval-batch-size 4 \\\n--batch-size 16 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 100 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\n</code></pre>"},{"location":"modeling/Iteration_09_improve_training_script/#train-on-the-cluster","title":"Train on the cluster","text":""},{"location":"modeling/Iteration_09_improve_training_script/#docker-image","title":"Docker image","text":"<p>I'm going to use the code from ARC24 as a start point.</p> <p>References:</p> <ul> <li>https://ironbar.github.io/arc24/modeling/Iteration_08_code_improvements/#scale-compute</li> <li>https://github.com/ironbar/arc24/tree/main/docker</li> </ul> <pre><code># relevant packages\n# Name                    Version                   Build  Channel\ncupy-cuda12x              13.4.1                   pypi_0    pypi\nflash-attn                2.6.3                    pypi_0    pypi\nliger-kernel              0.5.9                    pypi_0    pypi\nllguidance                0.7.19                   pypi_0    pypi\nllvmlite                  0.44.0                   pypi_0    pypi\nnumba                     0.61.2                   pypi_0    pypi\nnumpy                     2.2.5                    pypi_0    pypi\nnvidia-cublas-cu12        12.4.5.8                 pypi_0    pypi\nnvidia-cuda-cupti-cu12    12.4.127                 pypi_0    pypi\nnvidia-cuda-nvrtc-cu12    12.4.127                 pypi_0    pypi\nnvidia-cuda-runtime-cu12  12.4.127                 pypi_0    pypi\nnvidia-cudnn-cu12         9.1.0.70                 pypi_0    pypi\nnvidia-cufft-cu12         11.2.1.3                 pypi_0    pypi\nnvidia-curand-cu12        10.3.5.147               pypi_0    pypi\nnvidia-cusolver-cu12      11.6.1.9                 pypi_0    pypi\nnvidia-cusparse-cu12      12.3.1.170               pypi_0    pypi\nnvidia-cusparselt-cu12    0.6.2                    pypi_0    pypi\nnvidia-ml-py3             7.352.0                  pypi_0    pypi\nnvidia-nccl-cu12          2.21.5                   pypi_0    pypi\nnvidia-nvjitlink-cu12     12.4.127                 pypi_0    pypi\nnvidia-nvtx-cu12          12.4.127                 pypi_0    pypi\npython                    3.10.16              he870216_1  \ntorch                     2.6.0                    pypi_0    pypi\ntorchaudio                2.6.0                    pypi_0    pypi\ntorchvision               0.21.0                   pypi_0    pypi\ntransformers              4.51.3                   pypi_0    pypi\ntriton                    3.2.0                    pypi_0    pypi\ntrl                       0.18.0.dev0              pypi_0    pypi\nvllm                      0.8.5                    pypi_0    pypi\nxformers                  0.0.29.post2             pypi_0    pypi\n</code></pre> <p>I'm going to create a new docker image with a more recent cuda version.</p> <pre><code>cd docker\ndocker build -t cuda-python:python3.10-cuda14.1 .\ndocker tag cuda-python:python3.10-cuda14.1 gbarbadillo/cuda-python:python3.10-cuda14.1\ndocker push gbarbadillo/cuda-python:python3.10-cuda14.1\n</code></pre>"},{"location":"modeling/Iteration_09_improve_training_script/#problems-with-pip","title":"Problems with pip","text":"<pre><code>export BATCH_SIZE=4\ncondor_submit train.condor command=\"\naccelerate launch --num_processes 1 --num_machines 1 --mixed_precision bf16 \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-0.5B-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-20-batch-size/batch-size-${BATCH_SIZE} \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 1000 \\\n--n-gpus 1 \\\n--per-device-train-batch-size ${BATCH_SIZE} \\\n--per-device-eval-batch-size 4 \\\n--batch-size 16 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 100 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\"\n\nexport N_GPUS=2\ncondor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-0.5B-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-20-batch-size/batch-size-4-GPUS${N_GPUS} \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 1000 \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 4 \\\n--per-device-eval-batch-size 8 \\\n--batch-size 16 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 100 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS}\n\nexport N_GPUS=8\ncondor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-0.5B-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-20-batch-size/batch-size-4-GPUS${N_GPUS} \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 500 \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 4 \\\n--per-device-eval-batch-size 8 \\\n--batch-size 32 \\\n--max-seq-len 3072 \\\n--logging-steps 50 \\\n--eval-steps 50 \\\n--save-steps 500 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n</code></pre> <ul> <li>I'm getting this error when running the training</li> </ul> <pre><code>ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n</code></pre> <ul> <li>Weirdly I have tried with last year's script and gives the same error</li> <li>Trying to create the environment inside the docker, but does not work.</li> <li>It seems that the --user flag is being used, I don't understand why.</li> <li>Maybe it is related to write permissions? https://stackoverflow.com/questions/79608713/getting-could-not-install-packages-due-to-an-oserror-when-installing-python-pa</li> </ul> <p>Running the docker locally I have been able to reproduce the error by giving the <code>--user</code> flag. The weird thing is that I'm not using that flag when running the training.</p> <pre><code>docker run -ti -u 1000:1000 gbarbadillo/cuda-python:python3.10-cuda14.1\npython3 -m venv debug\nsource debug/bin/activate\npip3 install --upgrade pip --user\nERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.\n\n[notice] A new release of pip is available: 23.0.1 -&gt; 25.1.1\n[notice] To update, run: pip install --upgrade pip\n# this works\npip3 install --upgrade pip\n</code></pre> <p>After deleting <code>rm -r /mnt/scratch/users/gbarbadillo/.config/pip</code> problems with pip where solved. I found that configuration running the command <code>pip config debug</code>, suggested on this Github issue</p>"},{"location":"modeling/Iteration_09_improve_training_script/#problems-with-flash-attn","title":"Problems with flash-attn","text":"<p>It seems that it requires more than 64GB to create the environment and install flash-attn. The problem seems to be related to flash-attn installation. I'm trying to reproduce myself the problem locally.</p> <p>I'm seeing that it takes around one hour to install flash-attn on my PC inside the docker and requires 60% of the CPU (20 cores) and between 20 and 30 GB of RAM. So it might be possible that in a more powerful machine in the cluster uses more threads and even more RAM.</p> <p>If your machine has less than 96GB of RAM and lots of CPU cores, ninja might run too many parallel compilation jobs that could exhaust the amount of RAM.</p> <p>https://github.com/Dao-AILab/flash-attention</p> <pre><code>docker run -ti -u 1000:1000 gbarbadillo/cuda-python:python3.10-cuda14.1\npython3 -m venv debug\nsource debug/bin/activate\npip3 install --upgrade pip\ncat &gt; requirements.txt &lt;&lt;EOF\n...\nEOF\npip3 install -r requirements.txt\nMAX_JOBS=40 pip install flash-attn==2.6.3 --no-build-isolation\n</code></pre> <p>This required more than 128GB of RAM.</p> <p>I have set <code>MAX_JOBS=2</code> on the cluster, and installation took around 5 hours. Seems that required less than 32GB of RAM.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#scaling-to-multiple-gpus","title":"Scaling to multiple GPUs","text":""},{"location":"modeling/Iteration_09_improve_training_script/#naive-accelerate","title":"Naive accelerate","text":"GPU n GPUs batch size training time steps per second speedup A6000 1 1 62.7 0.27 0.47 A6000 1 2 40.7 0.41 0.73 A6000 1 4 29.6 0.56 1.00 A6000 2 4 17.9 0.93 1.65 RTX 3090 2 2 17.9 0.93 1.65 A6000 4 4 13.6 1.23 2.18 A6000 8 4 7.6 2.19 3.89 <ul> <li>A6000 and 3090 seem to have identical speed, the main difference is that A6000 has double VRAM (48GB vs 24GB). More detailed benchmark</li> <li>Speedup is not perfect, I would like to see something closer to the number of GPUs</li> </ul>"},{"location":"modeling/Iteration_09_improve_training_script/#trying-other-parameters-in-accelerate","title":"Trying other parameters in accelerate","text":"<p>o4-mini-high suggestions</p> n GPUs baseline training time (m) deepspeed training time (m) speedup 2 17.4 15.3 14% 4 13.6 11.7 16% 8 8 6.5 23% <p>Using deepspeed improves the training speed, but the improvement is small. I have not been able to find a configuration for deepspeed or accelerate that is better.</p> <pre><code>export N_GPUS=2\n# deepspeed\ncondor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --use_deepspeed \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-0.5B-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-21-accelerate/A6000-GPUS${N_GPUS}-deepspeed \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 500 \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 4 \\\n--per-device-eval-batch-size 8 \\\n--batch-size 32 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 100 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n\n# baseline\ncondor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-0.5B-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-21-accelerate/A6000-GPUS${N_GPUS}-baseline \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 500 \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 4 \\\n--per-device-eval-batch-size 8 \\\n--batch-size 32 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 100 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n\n# config\nexport N_GPUS=2\ncondor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --use_deepspeed \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-0.5B-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-21-accelerate/A6000-GPUS${N_GPUS}-deepspeed-config \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 500 \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 4 \\\n--per-device-eval-batch-size 8 \\\n--batch-size 32 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 100 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n</code></pre> <p>I have tried running <code>accelerate config default</code> and it only has created a file <code>/mnt/scratch/users/gbarbadillo/.cache/huggingface/accelerate/default_config.yaml</code> with this information:</p> <pre><code>{\n  \"compute_environment\": \"LOCAL_MACHINE\",\n  \"debug\": false,\n  \"distributed_type\": \"MULTI_GPU\",\n  \"downcast_bf16\": false,\n  \"enable_cpu_affinity\": false,\n  \"machine_rank\": 0,\n  \"main_training_function\": \"main\",\n  \"mixed_precision\": \"no\",\n  \"num_machines\": 1,\n  \"num_processes\": 2,\n  \"rdzv_backend\": \"static\",\n  \"same_network\": false,\n  \"tpu_use_cluster\": false,\n  \"tpu_use_sudo\": false,\n  \"use_cpu\": false\n}\n</code></pre> <p>I cannot run  <code>accelerate config</code> inside the job because I get <code>termios.error: (25, 'Inappropriate ioctl for device')</code></p>"},{"location":"modeling/Iteration_09_improve_training_script/#bigger-models","title":"Bigger models","text":"<pre><code>export N_GPUS=8\nexport PARAMETERS=0.5B\ncondor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-21-model-size/A6000-GPUS${N_GPUS}-${PARAMETERS} \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 500 \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 4 \\\n--per-device-eval-batch-size 8 \\\n--batch-size 32 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 0 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n\nexport PARAMETERS=1.5B\ncondor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-21-model-size/A6000-GPUS${N_GPUS}-${PARAMETERS} \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 500 \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 2 \\\n--per-device-eval-batch-size 4 \\\n--batch-size 32 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 0 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n\nexport PARAMETERS=7B\ncondor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-05-21-model-size/A6000-GPUS${N_GPUS}-${PARAMETERS} \\\n--random-seed 5 \\\n--device-map None \\\n--max-steps 500 \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--per-device-eval-batch-size 2 \\\n--batch-size 32 \\\n--max-seq-len 3072 \\\n--logging-steps 100 \\\n--eval-steps 0 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n</code></pre> <p>I don't have a single number, but it seems that GPU utilization is higher for bigger models. So maybe I should not worry about GPU utilization when using small LLMs.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#results","title":"Results","text":"<p>I have trained a new model on a few drawing task for 32k steps (512k samples) in around 9 hours. This model has been trained on images up to 30x30 pixels, the previous models were trained on up to 10x10 pixels. This opens the door to test tasks with a larger number of elements.</p> <p>Although the model has been trained with up to 5 drawings, with Hindsight Experience Replay (HER) it has been able to solve task up to 100 drawings.</p> number of squares epochs solution lines solution tokens 16 5 15 382 25 8 23 528 36 11 29 732 49 14 43 1082 64 21 55 1382 81 19 70 1757 100 28 81 2032 <p></p> <p></p> <p>It seems that there is no ceiling, and as long as predictions can get closer to the result the model could be able to solve the task. We only need with a model with an initial intuition of how to do the task and which functions to call.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#conclusion","title":"Conclusion","text":"<p>I have been able to train with multiple GPUs and on the cluster. Probably for small models such as 0.5B parameters it does not have sense to a lot of GPUs because speedup is not perfect.</p>"},{"location":"modeling/Iteration_09_improve_training_script/#next-steps","title":"Next steps","text":"<ul> <li>Solve the training set, then the evaluation set, then the new tasks from ARC25.</li> <li>Should I upgrade to 5090? Improvement seems relevant</li> </ul>"},{"location":"modeling/Iteration_09_improve_training_script/#todo","title":"TODO","text":"<ul> <li> Fix the problem with repeated calls to the train dataset generator</li> <li> Make the script work with accelerate</li> <li> Measure training speed vs batch size and number of gpus</li> <li> Measure training speed vs input size</li> <li> Does it have sense to use packing?</li> <li> Measure data sampling speed to verify is fast enough</li> <li> Add validation</li> <li> Enable multi-task training, currently only trains on a single task</li> <li> Bonus: Now that I have trained a model on bigger images, can it solve tasks with more than 25 squares?</li> <li> Train on the cluster</li> </ul>"},{"location":"modeling/Iteration_10_solve_arc_tasks/","title":"Iteration 10. Try to solve real ARC tasks","text":""},{"location":"modeling/Iteration_10_solve_arc_tasks/#iteration-10-try-to-solve-real-arc-tasks","title":"Iteration 10. Try to solve real ARC tasks","text":"<p>22-05-2025</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#goal","title":"Goal","text":"<p>Can I solve real ARC tasks with code and HER?</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#motivation","title":"Motivation","text":"<p>I have already seen that HER allows to generalize to novel toy tasks, I need to check if it can solve real ARC tasks.</p> <p>I know that my primitive functions defined on ARC24 solved 285 training tasks. So probably the easiest path is to review those transformations add them and modify if needed.</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#development","title":"Development","text":""},{"location":"modeling/Iteration_10_solve_arc_tasks/#add-safety-and-determinism-checks","title":"Add safety and determinism checks","text":"<p>Inspired by Absolute Zero Reinforced Self-play Reasoning with Zero Data I'm going add safety and determinism checks.</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#generation-functions","title":"Generation functions","text":"<p>LLM are incredible useful to write generation functions. For example I have asked o3 to write a function to create ARC images with random objects and it worked perfectly.</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#stats-about-current-implementation","title":"Stats about current implementation","text":"<pre><code>Found 23 training tasks\n\nThere are 17 DSL functions defined in arc25.dsl:\n    DSL functions used in 1000 tasks:\ndetect_objects                   621 times\ndraw_object                      425 times\ncreate_img                       395 times\ncrop                             123 times\ndraw_rectangle                   122 times\ndraw_vertical_line               111 times\ndraw_horizontal_line              98 times\ndraw_line                         94 times\ndraw_pixel                        91 times\nmode                              49 times\napply_colormap                    46 times\ndownscale                         33 times\nrotate_90                         32 times\nflip                              28 times\nupscale                           27 times\npad                               17 times\ntrim                              15 times\n\nThere are 13 DSL attributes defined in arc25.dsl:\n    DSL attributes used in 1000 tasks:\nchange_color                     380 times (Object)\narea                              89 times (Object)\nheight                            72 times (BoundingBox, Object)\nwidth                             66 times (BoundingBox, Object)\nis_horizontal_line                48 times (Object)\nis_rectangle                      46 times (Object)\nmove                              45 times (Object)\nis_square                         42 times (Object)\nis_vertical_line                  40 times (Object)\ncenter                            38 times (Object)\nis_line                           32 times (Object)\nis_point                          31 times (Object)\ncopy                               0 times (Object)\n</code></pre> <p>This is clearly not enough, but I want to train a model on these tasks and see if it can solve any of the ARC training tasks.</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#training","title":"Training","text":""},{"location":"modeling/Iteration_10_solve_arc_tasks/#local-experiments","title":"Local experiments","text":"Click to see the bash commands <pre><code>export N_GPUS=2\nexport PARAMETERS=0.5B\nexport STEPS=10\nexport MAXSEQLEN=3072\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\nscripts/finetuning.py \\\n--model_path /home/gbarbadillo/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-06-10-first-real-trainings/3090-GPUS${N_GPUS}-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${MAXSEQLEN}msl \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 2 \\\n--per-device-eval-batch-size 4 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 100 \\\n--eval-steps 0 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora \\\n--no-resume_from_checkpoint\n\n\nexport MAXSEQLEN=8192\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\nscripts/finetuning.py \\\n--model_path /home/gbarbadillo/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-06-10-first-real-trainings/3090-GPUS${N_GPUS}-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${MAXSEQLEN}msl \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--per-device-eval-batch-size 2 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 100 \\\n--eval-steps 0 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\n\nexport N_GPUS=1\nexport CUDA_VISIBLE_DEVICES=0\nexport MAXSEQLEN=8192\npython scripts/finetuning.py \\\n--model_path /home/gbarbadillo/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-06-10-first-real-trainings/3090-GPUS${N_GPUS}-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${MAXSEQLEN}msl \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--per-device-eval-batch-size 2 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 100 \\\n--eval-steps 0 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\n</code></pre> <p>It is training around 2.66s/it, task generation does not seem to be the bottleneck. At the beginning of the training used multiple cores to sample, but then CPU usage was low. Probably the queue was filled.</p> <pre><code># --max-seq-len 3072\n{'train_runtime': 298.5646, 'train_samples_per_second': 10.718, 'train_steps_per_second': 0.335, 'train_loss': 0.24244340896606445, 'epoch': 1.0}\n# export MAXSEQLEN=6144\n{'train_runtime': 363.6891, 'train_samples_per_second': 8.799, 'train_steps_per_second': 0.275, 'train_loss': 0.23476869583129883, 'epoch': 1.0}\n# export MAXSEQLEN=8192\n2025-06-11 06:32:08,621 - __main__ - INFO - log_prompt_length_percentiles -     train number of prompts: 1000, max number of tokens : 5108, percentiles: {50: 1249, 75: 1567, 90: 1960, 95: 2069, 97: 2139}\n{'train_runtime': 374.4698, 'train_samples_per_second': 8.545, 'train_steps_per_second': 0.267, 'train_loss': 0.23707759857177735, 'epoch': 1.0}\n\nexport N_GPUS=1\nexport CUDA_VISIBLE_DEVICES=0\nexport MAXSEQLEN=8192\n{'train_runtime': 671.3956, 'train_samples_per_second': 4.766, 'train_steps_per_second': 0.149, 'train_loss': 0.23653331756591797, 'epoch': 1.0} \n</code></pre> <p>To be safe I should probably use <code>max-seq-len=8192</code>, otherwise we will be missing some training tasks.</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#cluster","title":"Cluster","text":"<pre><code>export N_GPUS=2\nexport PARAMETERS=0.5B\nexport LEARNING_RATE=2e-4\nexport STEPS=32000; condor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-13-first-real-trainings/${N_GPUS}xA6000-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${LEARNING_RATE}lr \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 2 \\\n--per-device-eval-batch-size 4 \\\n--batch-size 32 \\\n--max-seq-len 8192 \\\n--logging-steps 10 \\\n--eval-steps 50 \\\n--save-steps 200 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n\nexport N_GPUS=2\nexport PARAMETERS=0.5B\nexport STEPS=32000\nexport LEARNING_RATE=4e-5; condor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-14-full-fine-tuning/${N_GPUS}xA6000-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${LEARNING_RATE}lr \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 2 \\\n--per-device-eval-batch-size 4 \\\n--batch-size 32 \\\n--max-seq-len 6144 \\\n--logging-steps 10 \\\n--eval-steps 50 \\\n--save-steps 200 \\\n--no-use-lora\" -append request_gpus=${N_GPUS} -append request_cpus=12\n\nexport N_GPUS=1\nexport PARAMETERS=0.5B\nexport STEPS=1000\nexport LEARNING_RATE=4e-5; condor_submit train.condor command=\"\npython  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-13-first-real-trainings/${N_GPUS}xA6000-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${LEARNING_RATE}lr \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 2 \\\n--per-device-eval-batch-size 4 \\\n--batch-size 32 \\\n--max-seq-len 8192 \\\n--logging-steps 10 \\\n--eval-steps 50 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS}\n\n\n# Copy the results to MEGA\nrsync -P -r calculon01:/mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-13-first-real-trainings /mnt/data/MEGA/TEMP --exclude wandb/*\n</code></pre>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#debugging","title":"Debugging","text":"Click to expand/collapse this section <pre><code>\nexport N_GPUS=2\nexport PARAMETERS=0.5B\nexport STEPS=10\nexport MAXSEQLEN=8192\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\nscripts/finetuning.py \\\n--model_path /home/gbarbadillo/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-06-10-first-real-trainings/3090-GPUS${N_GPUS}-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${MAXSEQLEN}msl \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--per-device-eval-batch-size 2 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 100 \\\n--eval-steps 0 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora \\\n--no-resume_from_checkpoint\n\nexport N_GPUS=1\nexport PARAMETERS=0.5B\nexport STEPS=10\nexport MAXSEQLEN=8192\npython \\\nscripts/finetuning.py \\\n--model_path /home/gbarbadillo/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-06-10-first-real-trainings/3090-GPUS${N_GPUS}-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${MAXSEQLEN}msl \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--per-device-eval-batch-size 2 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 100 \\\n--eval-steps 0 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora \\\n--no-resume_from_checkpoint \\\n--dataloader-num-workers 0\n\n# this works --dataloader-num-workers 0\n# this does not: --dataloader-num-workers 1\n# it is unrelated from: os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n</code></pre>   I'm seeing a new error on the cluster.   <pre><code>AttributeError: Can't pickle local object 'SFTTrainer._prepare_dataset.&lt;locals&gt;.add_eos'\n\n\n# local libraries\naccelerate                1.6.0                    pypi_0    pypi\ntorch                     2.6.0                    pypi_0    pypi\ntransformers              4.51.3                   pypi_0    pypi\ndatasets                  3.5.1                    pypi_0    pypi\ntrl                       0.18.0.dev0\n\n# cluster libraries (experiments run on docker)\naccelerate==1.7.0\ntorch==2.6.0\ntransformers==4.52.4\ndatasets==3.6.0\ntrl==0.18.1\n\n# local experiments updating library versions\ntrl==0.18.0 -&gt; works\ntrl==0.18.1 -&gt; works\naccelerate==1.7.0 -&gt; works\ntransformers==4.52.4 -&gt; works\ndatasets==3.6.0 -&gt; works\n\n# adding this line at the start of the script reproduces the problem locally\nimport multiprocessing as mp\nmp.set_start_method(\"spawn\", force=True)\n&gt; [rank0]: AttributeError: Can't pickle local object 'SFTTrainer._prepare_dataset.&lt;locals&gt;.add_eos'\n\n# adding this other line to see what it is printed\nimport multiprocessing as mp, os\nprint(\"&gt;&gt;&gt; multiprocessing start-method:\", mp.get_start_method(), \"PID:\", os.getpid())\n# local response\n&gt;&gt;&gt; multiprocessing start-method: fork PID: 19840\n&gt;&gt;&gt; multiprocessing start-method: fork PID: 19841\n# cluster response\n&gt;&gt;&gt; multiprocessing start-method: fork PID: 57\n&gt;&gt;&gt; multiprocessing start-method: fork PID: 58\n\n# adding this line at the start does not solve the problem in the cluster\nmp.set_start_method(\"fork\", force=True)\n</code></pre>   - https://github.com/pytorch/pytorch/blob/v2.7.0/torch/utils/data/dataloader.py#L173 - https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader - https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods - https://wandb.ai/guillermobarbadillo/2025-05-21-model-size/runs/3g8opphj/files/requirements.txt, on the last successful training in the cluster I used trl=0.17.0  Trying with trl=0.17.0 on job 204192. No success.  What is the problem? The problem seems to be that pickle cannot work with functions defined inside functions.  <pre><code>AttributeError: Can't pickle local object 'SFTTrainer._prepare_dataset.&lt;locals&gt;.add_eos'\nAttributeError: Can't pickle local object 'truncate_dataset.&lt;locals&gt;.truncate'\n</code></pre>  So I have changed how the dataset is generator to avoid entering in that 'SFTTrainer._prepare_dataset..add_eos' place, but then another similar error has arised. The biggest question is why hasn't happened this before.  This is the most similar problem found online, but there is no solution: https://github.com/huggingface/trl/issues/2979  This guy says that spawn might be used by default sometimes: https://discuss.pytorch.org/t/the-default-value-of-dataloader-multiprocessing-context-is-spawn-in-a-spawned-process/107494  https://github.com/pytorch/pytorch/issues/44687 Slightly related. Setting the DataLoader(..., multiprocessing_context='fork') fixes the issue for me.  This seems to solve the problem, change this line:   <pre><code>/home/gbarbadillo/miniconda3/envs/arc25-clone/lib/python3.10/site-packages/torch/utils/data/dataloader.py\nmultiprocessing_context=None,\nmultiprocessing_context='fork',\nsed -i.bak \"0,/multiprocessing_context[[:space:]]*=[[:space:]]*None,/s//multiprocessing_context='fork',/\" \\\n/home/gbarbadillo/miniconda3/envs/arc25-clone/lib/python3.10/site-packages/torch/utils/data/dataloader.py\n\nsed -i.bak \"0,/multiprocessing_context[[:space:]]*=[[:space:]]*None,/s//multiprocessing_context='fork',/\" \\\n/mnt/scratch/users/gbarbadillo/arc25/cached-environments/venv_07bdecf0b823319f4d2fcbe9cdc354d9/lib/python3.10/site-packages/torch/utils/data/dataloader.py\n</code></pre>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#results","title":"Results","text":""},{"location":"modeling/Iteration_10_solve_arc_tasks/#training-hyperparameters","title":"Training Hyperparameters","text":"<p>For a batch size of 32 and lora rank of 32 a learning rate of 2e-4 seems to be good. 1e-3 is too much,  4e-4 also works but for longer trainings 2e-4 might be a better option.</p> <p>Since I only have 23 training tasks, I don't expect to see relevant improvements by using a batch size bigger than 32. So I'm not going to do experiments with the batch size.</p> <p>Wandb experiment, filter by <code>1000steps</code>.</p> <p>For longer trainings 1e-4 might be a better option.</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#fine-tuning-model-capacity","title":"Fine-tuning model capacity","text":"<p>The following table shows the validation loss for trainings of different length.</p> steps LoRA 32 full fine-tuning 8k 0.0198 0.0178 16k 0.0165 0.0155 32k 0.0149 0.0123 <p>The full fine-tuning achieves a lower validation loss, while being faster. F.e. training for 32k steps took 24h when doing the full fine-tuning and 30h when using LoRA.</p> <p>I have seen that at the end of the training learning stops, but there is not a quick fix to this because the schedulers don't have a minimum learning rate value.</p> <p>Finally the full fine-tuning required a slightly lower learning rate (4e-5 vs 1e-4).</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#trying-to-solve-real-arc-tasks","title":"Trying to solve real ARC tasks","text":"<ul> <li><code>08ed6ac</code>, does not understand that the task is about sorting and changing colors</li> <li><code>0b148d6</code>, Understands that the task is about detecting objects and cropping, but does not know to use the color</li> <li><code>0ca9ddb</code>, seems to understand that the task is about drawing pixels, but it does not use the center as a reference. Neither it uses the color to select certain objects</li> <li><code>0d3d703e</code>, the model does not recognize that the task is about apply_colormap. Create more tasks showing how to change colors, not changing all the colors always.</li> <li><code>178fcbfb</code>, does understand that the task is about drawing horizontal and vertical lines, but does not know to use the center as a reference</li> <li><code>1bfc4729</code>, understands that it needs to draw some pattern, but does not have a way to make a different drawing for each image</li> <li><code>1c786137</code>, does not understand that the task is about selecting the object</li> </ul> <p>It seems that the main problem is that the model does not have a good intuition of how to solve the tasks. It simply does not use the correct primitive function in some cases, in other cases it is missing examples of how to use them. If the initial direction is not correct, it is not possible that HER can help to achieve the desired goal.</p> <p>I might introduce diversity in the generations by suggesting to use some DSL primitive functions.</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#conclusion","title":"Conclusion","text":"<p>I have run the first experiments to try to solve real ARC tasks. So far no task has been solved, but I have evaluated just a tiny subset of the training ARC tasks. On the following iteration I would like to solve at least the 7 tasks analyzed on this iteration.</p>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#next-steps","title":"Next steps","text":"<ul> <li>Hypothesis: If I implement a DSL that covers the whole training and evaluation set, it should generalize to the test set.</li> </ul>"},{"location":"modeling/Iteration_10_solve_arc_tasks/#todo","title":"TODO","text":"<ul> <li> Add safety and determinism checks</li> <li> Add more primitive functions and training tasks to learn to use them</li> <li> I would like to have a list of all the primitive functions from the DSL, and how many times are they used in the training tasks. A correlation plot would also be nice to see which connections are missing.</li> <li> Is the sampling speed enough?</li> <li> Stats about the input tokens distribution, what should be the max-seq-len?</li> <li> Optimize learning rate and batch size for 2 GPUs.</li> <li> Create a notebook to evaluate the trained models on real ARC tasks</li> <li> I need a way to do evaluation at scale, using multiple GPUs, and saving all the generated tasks when searching for a solution.</li> <li> If possible I should use Kaggle compute for evaluation. It is almost free and is a good way to store and visualize results.</li> <li> When studying how the method is working on real ARC tasks, I believe I should reuse the DSL analysis from the training tasks.   That way I can see if it is using the right abstractions, and if it combining them correctly</li> <li> Compositionality, can the model solve the task that selects the biggest object, crop and trim? That   would be a good example of compositionality because those functions were not used together in the dataset</li> <li> Sequential solving. Try also solving the tasks in multiple steps, not just once. It could help   with compositionality.</li> </ul>"},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/","title":"Iteration 11. Pretrain LoRA on new tasks","text":""},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#iteration-11-pretrain-lora-on-new-tasks","title":"Iteration 11. Pretrain LoRA on new tasks","text":"<p>29-05-2025</p>"},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#goal","title":"Goal","text":"<p>Check if pretraining the LoRA on the new ARC25 tasks can bring improvements on the leaderboard.</p>"},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#motivation","title":"Motivation","text":"<p>While I'm working on the new approach for ARC25 I have had an idea to improve the current leaderboard score of 11.94: instead of starting from a random LoRA pretrain it on the new ARC25 tasks.</p> <p>On my experiments during ARC24 I already noticed that it was better to fine-tune a pretrained LoRA instead of fine-tuning a fresh new LoRA. If that finding was correct, this experiment might work. Additionally there might be some shared information between the new tasks and the test sets.</p>"},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#development","title":"Development","text":""},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#create-subsets-from-arc25-with-the-new-tasks","title":"Create subsets from ARC25 with the new tasks","text":"<p>https://www.kaggle.com/code/ironbar/new-arc-agi-2-tasks</p> <p>So we have 233 new training tasks and 114 new evaluation tasks, 347 new tasks in total.</p>"},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#fine-tune-loras-with-the-new-tasks","title":"Fine-tune LoRAs with the new tasks","text":"<p>I have to check the original fine-tuning parameters used by the architects. Probably use a batch size bigger than one because I'm training on multiple tasks. The idea is to create a new notebook to do this task. Probably the most efficient implementation is to train a different LoRA rank on each GPU, thus each experiment will produce 4 LoRAs.</p> <ul> <li>https://www.kaggle.com/code/ironbar/pretrain-loras-for-the-architects</li> <li>https://www.kaggle.com/code/ironbar/the-architects-baseline-with-4-gpus</li> </ul> <p>These are the parameters that I used on my script for 4 GPUs, I changed the default parameters from the architects, but they can be seen as comments.</p> <pre><code>max_seq_length_train = 8192 # default 4224\n\nper_device_train_batch_size=4, # default=2\ngradient_accumulation_steps=1, # default=2\nlearning_rate=1e-4,\nembedding_learning_rate=1e-5,\n</code></pre>"},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#check-if-results-improve-on-the-evaluation-set","title":"Check if results improve on the evaluation set","text":"<p>I have to prepare a new notebook that uses pretrained LoRA. As far as I see, I simply have to provide the path to the pretrained LoRA when loading the model. I could remove all the peft configuration completely.</p> <p>https://www.kaggle.com/code/ironbar/the-architects-single-task-ttt-from-pretrained</p> lora_rank baseline eval score pretrain eval score 2 11.8 11.4 4 11.8 9.3 8 11.4 8.6 <p>Although I know that there is variability in the results, first evaluations are not promising. I'm not going to devote more time for this task.</p>"},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#conclusion","title":"Conclusion","text":"<p>First results were not promising, so I decided to stop working on this iteration.</p>"},{"location":"modeling/Iteration_11_pretrain_lora_on_new_tasks/#todo","title":"TODO","text":"<ul> <li> Check where random seed is used. Seed is on <code>infer_aug_params</code></li> </ul>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/","title":"Iteration 12. Solve a few ARC tasks","text":""},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#iteration-12-solve-a-few-arc-tasks","title":"Iteration 12. Solve a few ARC tasks","text":"<p>17-06-2025</p>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#goal","title":"Goal","text":"<p>Probe that I can solve a few selected ARC tasks by using an LLM to write code.</p>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#motivation","title":"Motivation","text":"<p>On the previous Iteration 10 I tried to solve a few ARC tasks without success: <code>08ed6ac7, 0b148d64, 0ca9ddb6, 0d3d703e, 178fcbfb, 1bfc4729, 1c786137</code>. The goal of this iteration is to solve all those tasks by implementing new training tasks and/or improving the solving algorithm.</p> <p>I should avoid creating training tasks that are clones from the real ARC tasks, otherwise I cannot measure the generalization capability of the model. My goal should be to write training tasks that teach the core knowledge that is needed for ARC.</p>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#development","title":"Development","text":""},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#new-tasks-to-implement","title":"New tasks to implement","text":"<ul> <li> Sort objects and do something to them based on the order. I can sort objects based on: area, x, y. I can move the objects, change their colors. This requires more control over the input images.</li> <li> Learn to use the color of the object. Let's focus on monochrome objects by now. Based on the color of the object something is done (move, change color, crop)</li> <li> Aggregate properties and use them to select, f.e. most/least popular color/area/shape...</li> <li> Learn to draw using object center as a reference, points, lines (also vertical and horizontal), rectangles...</li> <li> Create more tasks with apply_colormap</li> <li> Learn to draw using color of the objects as a reference</li> <li> More tasks about selecting an object that has some unique or extreme property</li> </ul>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#training","title":"Training","text":"Click to expand/collapse this section <pre><code>export N_GPUS=2\nexport PARAMETERS=0.5B\nexport LEARNING_RATE=1e-4\nexport STEPS=2000; condor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-18-more-training-tasks/${N_GPUS}xA6000-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${LEARNING_RATE}lr \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 2 \\\n--per-device-eval-batch-size 4 \\\n--batch-size 32 \\\n--max-seq-len 6144 \\\n--logging-steps 10 \\\n--eval-steps 50 \\\n--save-steps 200 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nexport N_GPUS=2\nexport PARAMETERS=1.5B\nexport LEARNING_RATE=1e-4\nexport STEPS=16000; condor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-18-more-training-tasks/${N_GPUS}xA6000-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${LEARNING_RATE}lr \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--per-device-eval-batch-size 2 \\\n--batch-size 32 \\\n--max-seq-len 6144 \\\n--logging-steps 10 \\\n--eval-steps 50 \\\n--save-steps 200 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nexport N_GPUS=2\nexport PARAMETERS=3B\nexport LEARNING_RATE=1e-4\nexport STEPS=16000; condor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-18-more-training-tasks/${N_GPUS}xA6000-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${LEARNING_RATE}lr \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--per-device-eval-batch-size 2 \\\n--batch-size 32 \\\n--max-seq-len 6144 \\\n--logging-steps 10 \\\n--eval-steps 50 \\\n--save-steps 200 \\\n--lora-r 32 \\\n--use-dora \\\n--use-liger-kernel \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nexport N_GPUS=2\nexport PARAMETERS=7B\nexport LEARNING_RATE=1e-4\nexport STEPS=16000; condor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning.py \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Qwen2.5-Coder-${PARAMETERS}-Instruct/ \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-18-more-training-tasks/${N_GPUS}xA6000-Qwen2.5-Coder-${PARAMETERS}-${STEPS}steps-${LEARNING_RATE}lr \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--per-device-eval-batch-size 2 \\\n--batch-size 32 \\\n--max-seq-len 5120 \\\n--logging-steps 10 \\\n--eval-steps 50 \\\n--save-steps 200 \\\n--lora-r 32 \\\n--use-dora \\\n--use-liger-kernel \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nrsync -P -r calculon01:/mnt/scratch/users/gbarbadillo/arc25/trainings/2025-06-18-more-training-tasks /mnt/data/MEGA/TEMP --exclude wandb/* --exclude *.pt\n</code></pre>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#results","title":"Results","text":""},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#influence-of-training-steps-and-diversity-of-predictions","title":"Influence of training steps and diversity of predictions","text":"<p>When making predictions with a model trained for 8k steps I was surprised to see that only produced 1 unique prediction (making a ton of repeated predictions)</p> task \\ steps (k) 1 2 4 8 16 1bfc4729 2 16 10 1 14 0ca9ddb6 69 31 20 17 11 178fcbfb 5 10 5 8 6 0d3d703e 121 120 115 14 88 <p>The table below shows the number of unique and valid predictions for the <code>Qwen2.5-Coder-0.5</code> model. The total number of predictions was 136. The relation is unclear and inconsistent between tasks.</p> <p>Thus so far does not seem that training for longer reduces the model predictions diversity.</p>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#analysis-of-trying-to-solve-the-tasks","title":"Analysis of trying to solve the tasks","text":"task \\ model 0.5B@1k steps 1.5B@1k steps 1.5B@16k steps 3B@1k steps 7B@1k steps 08ed6ac7 does not understand that the task is about changing colors, sorting the objects by area does not understand that the task is about changing colors, sorting the objects by area 0b148d64 the most succesfull approach is downscaling instead of selecting and cropping OOM 0ca9ddb6 draws 3 points, tries to use area for color. Tried an attempt to use the color as input draws 2 points, tries to use area for color draws 2 points, tries to use area for color draws 4 points, but doesn't understand that color depends on the object color, tries to use the area draws 3 points, then starts to draw lines 0d3d703e does not understand that  is about colormaps does not understand that is about colormaps Solved at epoch 6 Solved at epoch 2 Solved at epoch 3 178fcbfb draws vertical or horizontal lines, but not both draws vertical and horizontal lines, but does not understand there is a condition only vertical lines, very low diversity draws vertical and horizontal lines, but does not understand there is a condition draws vertical and horizontal lines, but does not understand there is a condition 1bfc4729 only horizontal lines only horizontal lines does not understand the task, draws horizontal lines on the points and the rest is garbage low diversity in predictions, does not improve over horizontal lines many different predictions, but not in the correct direction 1c786137 chooses the object using height instead of area, maybe another property is needed. Probably color should be used does not understand the task OOM"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#thoughts","title":"Thoughts","text":"<ul> <li>I have the feeling that bigger models do better</li> <li>I have solved the first real ARC task, although it was very simple it required adaptation with HER</li> <li>But the lack of generalization is worrying, maybe the training data generation strategy is not the best</li> <li>Lack of creativity, only does what it has learned to do during training</li> <li>HER works, but needs a model with diverse predictions and good intuition</li> </ul> <p>If the model is in the right direction, I believe it's very likely that HER will help to achieve the correct solution. However so far the model is lacking that ability to understand the tasks and use the appropriate DSL primitives to solve the problem.</p> <p>Another problem is the low diversity in the proposed solutions. For some tasks-model combinations it is as low as proposing the same solution over and over. Reinforcement learning requires exploration to solve a problem, and in many cases the solution space is not being explored correctly.</p> <p>Deep learning works when the training set densely covers the space. That is not the case for the current training tasks. It was the case for the toy drawing problem, because the space was small. However when the DSL grows that becomes more and more difficult.</p>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#conclusion","title":"Conclusion","text":"<p>On this iteration I have prepared new sample tasks to learn how to use the DSL. Despite of doing this job only one real ARC task was solved (and it was simply applying a colormap). </p> <p>I have to rethink the approach, because the current implementation does not correctly explore the solution space.  Only explores a small fraction of the solution space and repeats the same errors over and over.</p>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#next-steps","title":"Next steps","text":"<ul> <li>Better sampling strategy. Could play with temperature, top_k and top_p to create more diverse samples. https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/text_generation#transformers.GenerationConfig.temperature</li> <li>Better training objective. label_smoothing_factor might be used to preserve entropy. https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/trainer#transformers.TrainingArguments.label_smoothing_factor</li> <li>Validation might be solved ARC tasks. That way I could better measure the effect of the training tasks.</li> <li>Reread transduction and induction paper, and code.</li> <li>What if I give hints of how to solve the problem? Is the model capable on that case?</li> </ul>"},{"location":"modeling/Iteration_12_solve_a_few_arc_tasks/#todo","title":"TODO","text":"<ul> <li> Write new training tasks to solve the current knowledge gaps of the model</li> <li> I need a way to do evaluation at scale, using multiple GPUs, and saving all the generated tasks when searching for a solution.</li> <li> If possible I should use Kaggle compute for evaluation. It is almost free and is a good way to store and visualize results.</li> <li> Compositionality, can the model solve the task that selects the biggest object, crop and trim? That   would be a good example of compositionality because those functions were not used together in the dataset</li> <li> Sequential solving. Try also solving the tasks in multiple steps, not just once. It could help   with compositionality.</li> </ul>"},{"location":"modeling/Iteration_13_reflections/","title":"Iteration 13. Reflections","text":""},{"location":"modeling/Iteration_13_reflections/#iteration-13-reflections","title":"Iteration 13. Reflections","text":"<p>23-06-2025</p>"},{"location":"modeling/Iteration_13_reflections/#goal","title":"Goal","text":"<p>Think about the current approach and propose next steps.</p>"},{"location":"modeling/Iteration_13_reflections/#motivation","title":"Motivation","text":"<p>I need a solid theoretical foundation before I continue working to solve ARC.</p>"},{"location":"modeling/Iteration_13_reflections/#analysis-of-the-current-problems","title":"Analysis of the current problems","text":"<p>Despite adding more sample training tasks it only solved one real ARC task that is very easy.</p>"},{"location":"modeling/Iteration_13_reflections/#bad-exploration","title":"Bad exploration","text":"<p>The solution space is not fully explored, seems to be limited to repeat what it saw on training. This is a huge problem, because without exploration (trying new approaches) we cannot solve any novel task.</p> <p>On a game the action space is small, but the search space of an LLM is huge.</p>"},{"location":"modeling/Iteration_13_reflections/#bias-on-the-training-data","title":"Bias on the training data","text":"<p>There might be problems (bias) with the training data. If each primitive function is used just on a different data distribution, the model might learn the association between code and inputs, ignoring the outputs completely. Ideally the same input would be reused on all tasks.</p> <p>If this problem exists, it's difficult for the model to develop the intuition needed to solve new tasks.</p>"},{"location":"modeling/Iteration_13_reflections/#compositionality","title":"Compositionality","text":"<p>Could a model trained on single step tasks learn to combine the steps to create multi-step tasks? I'm not sure. Probably we should train for n steps, and hope that it could do novel combinations at test-time.</p>"},{"location":"modeling/Iteration_13_reflections/#training-data-generation","title":"Training data generation","text":"<p>Could an LLM generate novel tasks given code examples? Maybe, because if we just give the information as code, is a higher level abstraction than the ARC tasks and the domain of LLMs.</p> <p>That was done in the transduction and induction paper, I need to revisit it. It would be a more direct development because I would just write solutions for the ARC tasks and generators for the inputs.</p> <p>And if later I see holes, I could create new tasks.</p>"},{"location":"modeling/Iteration_13_reflections/#thoughts-about-arc","title":"Thoughts about ARC","text":"<p>To solve new tasks we need to explore, learn and exploit the learning. The connection with reinforcement learning is obvious.</p> <p>Let's analyze the two main approaches from ARC24.</p>"},{"location":"modeling/Iteration_13_reflections/#test-time-training-transduction","title":"Test-time training. Transduction","text":"<p>Given enough input-output pairs, we can train a model to learn the mapping. With deep learning this usually requires a big dataset or a pretrained model (or both). Generalization to new input samples is not guaranteed.</p> <p>This is similar to system 1, intuition. To solve a problem very fast without explaining the solution, just having a glance at the data and predicting the new output.</p> <p>This approach can be improved by:</p> <ul> <li>Pretraining the model on more data</li> <li>Adding inductive biases to the model (so it learns faster the new tasks)</li> </ul> <p>This approach is less likely to work on complex tasks that require multiple steps, or rules interacting with each other. Thus it does not seem the most promising way to solve ARC-AGI-2.</p>"},{"location":"modeling/Iteration_13_reflections/#search-induction","title":"Search. Induction","text":"<p>It seems that o3 does search in the space of natural language programs. It describes the task with natural language and generates the output using that description. It tries different approaches for the same task and answers with the most promising solution. We know that o3 was trained with reinforcement learning on the ARC training set. This RL trained gave o3 the abilities to explore different solutions to the tasks and to select the one that is most likely correct.</p> <p>This approach has some problems:</p> <ul> <li>Since it generates the grids directly, it can make errors despite describing the tasks correctly. There are not correction guarantees.</li> <li>Search won't be enough if the policy has wrong beliefs</li> </ul>"},{"location":"modeling/Iteration_13_reflections/#the-bitter-lesson","title":"The bitter lesson","text":"<p>Search and learn, the two methods that can scale arbitrarily with compute, noticed brilliantly by Richard Sutton.</p> <p>Search and learn are the two methods that allow to adapt to novelty. We can search a program to do a new task, or we can learn the new task directly.</p>"},{"location":"modeling/Iteration_13_reflections/#search-in-the-program-space","title":"Search in the program space","text":"<p>I like the search approach, but I believe it's better to search in the space of python programs. That allows to execute the code and if the code is correct the output will be correct. This gives much more stronger guarantees than learning or searching in the space of natural language programs.</p> <p>However this requires to develop or to have a domain specific language (DSL). The expressivity of the system will be tied to this DSL, if the DSL is not complete the system won't be able to solve ARC.</p> <p>But I would argue that the same limitation applies to o3. Those transformations that weren't learned on the training set, won't be able to be applied at test time. Because during training o3 has developed its own natural language DSL.</p> <p>If we want to search, we should probably train the model to do search. That would be done with RL. Currently, I'm not doing that, just supervised fine-tuning on single turn conversation.</p>"},{"location":"modeling/Iteration_13_reflections/#synthesis-search-and-learn","title":"Synthesis: Search and learn","text":"<p>My bet is to combine search and learning. I believe that it should be the most efficient approach and that is how human intelligence works.</p> <p>The system would search python programs that implement the new task. And it will refine its search policy using hindsight experience replay. I believe that should reduce the number of search steps dramatically.</p> <p>My current approach is failing at search (exploration). It does not explore the full solution space, and repeats the errors over and over. This happens because:</p> <ul> <li>The model has not been trained to search, that requires RL or distillation from a model that knows   how to search</li> <li>The current generation setup has no memory. All the predictions are independent, and that allows   to repeat the errors over and over. I'm doing parallel search, when probably for ARC has more   sense to do sequential search.</li> </ul>"},{"location":"modeling/Iteration_13_reflections/#conclusion","title":"Conclusion","text":"<p>After this reflections I have a much clearer view of the challenges that need to be addressed to solve ARC. I have to do a follow-up iteration to gain more information, and after that I would redefine the strategy. I should start with humble goals, like solving a few real ARC tasks, but at the same time with approaches that can scale once I verify that they work.</p>"},{"location":"modeling/Iteration_13_reflections/#next-steps","title":"Next steps","text":"<ul> <li>In the next iteration<ul> <li>What is the max sequence length for training and inference on a GPU with 24GB of VRAM?</li> <li>Better sampling strategy. Could play with temperature, top_k and top_p to create more diverse samples. https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/text_generation#transformers.GenerationConfig.temperature</li> <li>What if I give hints of how to solve the problem int the prompt? Is the model capable on that case?</li> <li>What if I have a multi-turn conversation with the model to improve its own code?</li> </ul> </li> <li>Reread transduction and induction paper, and code.</li> <li>Learn how to do RL with tool use</li> <li>Define the new data strategy. I might refactor an existing DSL. Given the typed hints to the model could allow to learn how to use it. I could try to generate tasks with LLMs.</li> <li>Better training objective. label_smoothing_factor might be used to preserve entropy. https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/trainer#transformers.TrainingArguments.label_smoothing_factor</li> <li>Validation might be solved ARC tasks. That way I could better measure the effect of the training tasks.</li> </ul>"},{"location":"modeling/Iteration_14_optimize_inference/","title":"Iteration 14. Optimize inference","text":""},{"location":"modeling/Iteration_14_optimize_inference/#iteration-14-optimize-inference","title":"Iteration 14. Optimize inference","text":"<p>26-06-2025</p>"},{"location":"modeling/Iteration_14_optimize_inference/#goal","title":"Goal","text":"<p>Is there room for improvement with the current approach if I modify the inference to better explore the solution space?</p>"},{"location":"modeling/Iteration_14_optimize_inference/#motivation","title":"Motivation","text":"<p>On Iteration 12, solve a few arc tasks I saw very little exploration of the solution space. I want to try different ideas to see if we can fix the problem without redesigning the training and the strategy.</p>"},{"location":"modeling/Iteration_14_optimize_inference/#development","title":"Development","text":""},{"location":"modeling/Iteration_14_optimize_inference/#results","title":"Results","text":""},{"location":"modeling/Iteration_14_optimize_inference/#max-sequence-length","title":"Max sequence length","text":"<p>I could generate and train on up to 32k sequences with a GPU of 24GB of VRAM.</p>"},{"location":"modeling/Iteration_14_optimize_inference/#inference","title":"Inference","text":"<p>I have verified that all the Qwen-Coder models listed below can generate 32k tokens on a 4090GPU. They differ in speed and on GPU utilization.</p> model tokens/s 0.5B 49.8 1.5B 39.6 3B 27.4 7B 18.6"},{"location":"modeling/Iteration_14_optimize_inference/#training","title":"Training","text":"<p>If we use liger kernels and gradient checkpoint we can train the 0.5B model in a GPU with 24GB of VRAM and a sequence length of 32000.</p> <p></p> <p>We can train with up to 32k tokens with the 3B model, for the 7B model we can only reach 16k tokens. Notice how the training speed decreases with the sequence length.</p> <p></p>"},{"location":"modeling/Iteration_14_optimize_inference/#better-sampling-parameters","title":"Better sampling parameters","text":"<p>We can play with the temperature or top_p to induce more variability in the predictions, but still not enough. In the best scenario the rate of unique predictions is just 17% (45/256).</p>"},{"location":"modeling/Iteration_14_optimize_inference/#temperature","title":"Temperature","text":"<p>Increasing the temperature decrease the number of valid predictions, but also can increase the number of unique predictions (there is a sweet spot)</p> <p></p>"},{"location":"modeling/Iteration_14_optimize_inference/#top_p","title":"Top_p","text":"<p>The same effect can be observed with top_p, there is a sweet spot for unique predictions.</p> <p></p>"},{"location":"modeling/Iteration_14_optimize_inference/#conclusion","title":"Conclusion","text":"<p>A GPU with 24GB of VRAM is enough to make inference with a window size of 32k tokens, and we can train with 32k tokens for models up to 3B and 16k for the 7B models.</p> <p>Playing with inference parameters was not enough to increase output diversity.</p>"},{"location":"modeling/Iteration_14_optimize_inference/#todo","title":"TODO","text":"<ul> <li> What is the max sequence length for training and inference on a GPU with 24GB of VRAM?</li> <li> Better sampling parameters. Could play with temperature, top_k and top_p to create more diverse samples. https://huggingface.co/docs/transformers/v4.52.3/en/main_classes/text_generation#transformers.GenerationConfig.temperature</li> <li> What if I give hints of how to solve the problem int the prompt? Is the model capable on that case?</li> <li> What if I have a multi-turn conversation with the model to improve its own code?</li> </ul>"},{"location":"modeling/Iteration_15_the_path_forward/","title":"Iteration 15. The path forward: Search &amp; Learn","text":""},{"location":"modeling/Iteration_15_the_path_forward/#iteration-15-the-path-forward-search-learn","title":"Iteration 15. The path forward: Search &amp; Learn","text":"<p>17-06-2025</p>"},{"location":"modeling/Iteration_15_the_path_forward/#goal","title":"Goal","text":"<p>Define the solution I want to implement in the following months.</p>"},{"location":"modeling/Iteration_15_the_path_forward/#search-and-learn","title":"Search and Learn","text":"<p>Search and learn are the two mechanisms to adapt to novelty, and I bet that to beat ARC we need both. Thus my proposal for ARC25 is a system that is able to explore the program space of the solutions and learn during the exploration.</p> <p>The core of the system would be an LLM that uses a DSL to write python code to solve the ARC tasks. The LLM will be used to search solutions for the task, and the model itself will learn to guide the search process balancing the depth/width dilemma. The generated programs that do not solve the tasks will be relabeled with hindsight and the model will be trained on those to adjust its prior beliefs and search more efficiently.</p>"},{"location":"modeling/Iteration_15_the_path_forward/#search","title":"Search","text":"<p>As a human I know everything that I have tried (at least at a high level summary) when solving a new task. I also know when to change direction when reaching a dead end. The search history guides the next steps of the search to avoid repeating previous failures. This was one of the weaknesses of my previous iterations, where all the generated solutions were independent.</p> <p>When searching a solution for an ARC task there are 3 high level actions that we can take:</p> <ol> <li>Generate a new function</li> <li>Refine an existing function</li> <li>Combine multiple existing functions into a new one</li> </ol> <p>Search can be visualized in a graph like in the Sakana blogpost</p> <p></p> <p>During training we can sample the actions randomly or exhaustively, and if we find a solution we could label the actions in retrospective. That would allow to train a model to acquire the taste to guide the search intelligently.</p> <p>When generating new functions we should give the previous functions as context to encourage generating novel functions. This could also be trained, with some novelty loss function. Another way to foster diversity would be to exclude some of the DSL functions in the prompt (because typically we would give the footprint of the available functions in the DSL)</p>"},{"location":"modeling/Iteration_15_the_path_forward/#learning","title":"Learning","text":"<p>All the predicted functions can be treated as new task, with hindsight relabelling. On previous iterations I have already seen that on toy tasks this enables broader generalization, and on the SOAR paper a small improvement was measured when using this technique.</p>"},{"location":"modeling/Iteration_15_the_path_forward/#alternating-cycles","title":"Alternating cycles","text":"<p>One possible implementation would alternate between search and learning phases in a cycling fashion. For example the search phase could involve sampling 32 new functions, then switch to training on those new 32 tasks.</p>"},{"location":"modeling/Iteration_15_the_path_forward/#other-details","title":"Other details","text":""},{"location":"modeling/Iteration_15_the_path_forward/#data-augmentation","title":"Data augmentation","text":"<p>Each task variation would require a different function most of the cases. Thus when searching we should treat each task variation generated with data augmentation as a different task. The value of using data augmentation during search would be adding diversity. But since compute is limited, it should be investigated if it is beneficial.</p> <p>It could be used for evaluation and to generate more training data.</p>"},{"location":"modeling/Iteration_15_the_path_forward/#continuous-metric","title":"Continuous metric","text":"<p>Evolutionary methods require a continuous metric. The nature of ARC is binary in essence, but maybe we can define some proxy metrics that are continuous and allow to better explore the search space. This should be investigated using the search traces of the model.</p>"},{"location":"modeling/Iteration_15_the_path_forward/#data-generation","title":"Data generation","text":"<p>I believe that the best data generation mechanism is the one used on the transduction and induction paper. This requires implementing a DSL and writing solutions and generator for the ARC tasks. But once we have that data we can use frontier LLMs to generate an arbitrary number of new tasks.</p> <p>The other source of data would be the data generating during evaluation of the system.</p>"},{"location":"modeling/Iteration_15_the_path_forward/#summary","title":"Summary","text":"<p>Generate python code with an LLM to solve ARC tasks. The system uses an intelligent search process and learns from its mistakes.</p>"},{"location":"modeling/Iteration_15_the_path_forward/#next-steps","title":"Next steps","text":"<ul> <li>Start by using the DSL defined in the BARC repo. Do search with base models</li> <li>Fine-tune on BARC and repeat search experiments. Analyze unsolved tasks</li> </ul>"},{"location":"modeling/Iteration_16_search_with_base_models/","title":"Iteration 16. Search with base models","text":""},{"location":"modeling/Iteration_16_search_with_base_models/#iteration-16-search-with-base-models","title":"Iteration 16. Search with base models","text":"<p>17-06-2025</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#goal","title":"Goal","text":"<p>Can I solve training ARC tasks using a base model with access to a DSL?</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#motivation","title":"Motivation","text":"<p>The SOAR paper has shown that it is possible to use Qwen-Coder models to solve ARC tasks without the need of fine-tuning (although fine-tuning improves the scores.)</p> <p>On this iteration I will try to replicate that work but instead of using plain python I will give access to a DSL that hopefully will ease the search process.</p> <p>I could explore how to generate new functions, refine existing ones or combine multiple functions.</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#development","title":"Development","text":""},{"location":"modeling/Iteration_16_search_with_base_models/#public-dsls","title":"Public DSLs","text":"<ul> <li>BARC: 54 primitive functions, solve around 160 training tasks.   Does not have typed hints.</li> <li>RE-ARC: 160 primitive functions, verified to be complete   for the ARC-AGI-1 train set. The problem of this DSL is that the verifiers that implement the training   task are very hard to understand. Maybe I can refactor it using Codex but seems like a very difficult task.</li> </ul> <p>Having a big number of primitive functions will result in a bigger prompt if we want to give the LLM the footprint of all the functions. Also make the combinatorial problem harder. The DSL should be as minimal as possible.</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#vllm-vs-transformers","title":"VLLM vs transformers","text":"Model VLLM VLLM quantized transformers transformers quantized speedup speedup quantized Qwen2.5-Coder-0.5B-Instruct 1107 563 48 36 23.1 15.6 Qwen2.5-Coder-1.5B-Instruct 426 282 42 29 10.1 9.7 Qwen2.5-Coder-3B-Instruct 452 180 32 24 14.1 7.5 Qwen2.5-Coder-7B-Instruct 200 100 - 27 - 3.7 <p>VLLM is much faster than transformer when doing sequential inference (batch size 1).</p> <p>And I also have noticed that VLLM benefits from making multiple predictions for the same prompt. For example for the the following models we can increase the throughput (tokens/s) x8.</p> batch size Qwen2.5-Coder-0.5B-Instruct Qwen2.5-Coder-7B-Instruct 1 1107 100 2 1571 135 4 3199 248 8 3837 354 16 5845 609 32 6094 870 <p>So for the smaller model we could generate around 6k tokens per second. That would be 1e9 tokens if we generate with 4 GPUs for 12 hours. As a reference OpenAI generated 5.7e9 tokens to solve the test set from ARC-AGI-1.</p> <p>After seeing this huge differences in speed I will be using only VLLM on this iteration.</p> <p>I have also seen that <code>enable_prefix_caching</code> allows faster execution the second time we call the model. If the number of output tokens is small we can notice the inference speed, for example increasing to 150 tokens/second from an initial 75. For longer generations the effect is not that big.</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#try-the-qwen25-coder-14b-instruct-model","title":"Try the Qwen2.5-Coder-14B-Instruct model","text":"<p>Just loading the 14B model with VLLM takes 4 minutes, although is a 4-bit quantized GGUF version. I had to also download the tokenizer from the normal version</p> <p>It is capable of generating text at 100 tokens/s.</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#results","title":"Results","text":""},{"location":"modeling/Iteration_16_search_with_base_models/#model-size-matters","title":"Model size matters","text":""},{"location":"modeling/Iteration_16_search_with_base_models/#10-tasks-64-predictions-per-task","title":"10 tasks, 64 predictions per task","text":"<p>The plot below shows the valid outputs probability and the dsl usage of Qwen2.5-Coder with different sizes. The size is really important (at least for base models). Bigger models generate valid outputs more frequently and use the dsl more frequently as well. All predictions are independent.</p> <p></p> <p>To be able to use the 14B model I had to use <code>tensor_parallel_size=2</code> and halve the <code>max_model_len=16000</code>.</p> <p>I have also tried Qwen3 but it was slower and worse, so I will be using Qwen2.5-Coder-7B for the experiments in this iteration.</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#400-training-tasks-8-predictions-per-task","title":"400 training tasks, 8 predictions per task","text":"parameters 0.5B 1.5B 3B 7B 14B valid code 87.6% 98.5% 96.3% 99.6% 99.9% valid outputs 15.6% 45.6% 47.6% 78.0% 85.3% unique outputs 13.2% 39.5% 39.6% 50.6% 28.4% dsl usage 29.4% 30.9% 40.3% 56.5% 87.6% mean pixel similarity score 32.5% 43.4% 47.8% 52.9% 59.1% correct grids 0.1% 0.2% 0.8% 1.7% 1.8% solved tasks 0.0% 0.0% 0.8% 2.5% 3.8% inference time 460 425 1008 744 5505 <p>This table shows the same experiment but with a bigger number of tasks. The sweet spot for my hardware seems to be the Qwen2.5-Coder-7B model.</p> <p>It can be seen that using bigger models leads to better metrics.</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#effect-of-the-number-of-predictions-with-independent-search","title":"Effect of the number of predictions with independent search","text":"<p>The following table shows results for the <code>Qwen2.5-Coder-7B</code> model when changing the number of predictions and doing independent search. No data augmentation was used (which would likely increase the diversity of the predictions).</p> predictions 2 4 8 16 32 64 128 256 512 valid code 99.1% 99.1% 99.6% 99.7% 99.8% 99.7% 99.8% 99.8% 99.8% valid outputs 75.4% 78.2% 78.0% 79.2% 79.0% 78.8% 79.1% 79.0% 79.1% unique outputs 65.9% 59.8% 50.6% 42.1% 35.7% 30.8% 26.9% 23.6% 20.8% dsl usage 61.0% 56.9% 56.5% 57.6% 58.7% 58.6% 58.3% 58.4% 58.4% mean pixel similarity score 50.4% 53.5% 52.9% 53.1% 53.5% 53.1% 53.2% 53.1% 53.2% correct grids 2.1% 1.8% 1.7% 1.9% 2.0% 1.8% 1.8% 1.8% 1.8% solved tasks 2.0% 2.0% 2.5% 4.8% 6.0% 7.5% 7.3% 9.5% 11.0% inference time (s) 266 395 744 1366 2802 5681 10553 22596 41579 inference time (h) 0.1 0.1 0.2 0.4 0.8 1.6 2.9 6.3 11.5 <ul> <li>The inference time increases ~linearly with the number of predictions</li> <li>The ratio of unique outputs decreases with the number of predictions, being just 20% with 512 predictions. Thus we could get the same results with around 100 predictions if we can always generate novel outputs.</li> <li>The number of solved tasks increases log-linearly with the number of predictions</li> </ul> <p></p> <p>This uses only one GPU, and we were able to do 512 predictions for 400 tasks in 12 hours. With 4 GPUs and 240 tasks, we could be making close to 4096 predictions per task. Or if we devote half of the time to training, we could do 2048 predictions per task. Sounds like a big enough number.</p> <p>In the next experiments I have to answer the question: can I improve this results using a more advanced search? For the same compute budget (number of predictions), can we improve the metrics and/or the unique outputs ratio?</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#can-we-increase-the-unique-outputs-by-conditioning-on-already-generated-code","title":"Can we increase the unique outputs by conditioning on already generated code?","text":"<p>I have the belief that if we provide the model the already generated code and ask to implement different approaches to the problem, the unique outputs will increase.</p> <p>However the results below show that my intuition was wrong. In fact I get the opposite effect, the model does not understand the instruction and the diversity is reduced. Furthermore in many of the tasks copied the sample code exactly.</p> experiment valid outputs unique outputs exact duplicates independent search 87.3% 77.1% 0.0% sequential search v1 91.8% 66.4% 13.0% sequential search v2 92.6% 69.0% 8.0% sequential search v3 91.0% 69.9% 5.8% sequential search v4 (system prompt) 89.5% 71.8% 0.8% v4 + repetition_penalty=1.05 88.1% 72.7% 0.5% v4 + repetition_penalty=1.1 81.8% 72.1% 0.0% v4 + repetition_penalty=1.2 63.3% 61.1% 0.0% v1 +  repetition_penalty=1.05 91.9% 71.5% 4.8% v1 +  repetition_penalty=1.1 87.5% 70.3% 1.3% refine prompt v1 95.5% 63.9% 19.0% <p>The results show that none of the sequential search experiments gets a similar unique output ratio to simply do independent search. I was able to tune the prompt in the different versions of the sequential search, and I saw improvements but the problem persisted.</p> <p>Thus this is not a good strategy for searching (at least with this base model). I believe RL could solve this, and maybe bigger models do better.</p> <p>I have to look for diversity elsewhere. Maybe more worrying is that I have tried a prompt with the goal of refining the existing code, and the exact duplicates rate was the highest of all the experiments: 19%. Thus it seems that if I want to refine code, I'm going to throw many attempts to the trash can.</p> <ul> <li>Gemini 2.5 Pro explanation</li> <li>o3 explanation</li> </ul>"},{"location":"modeling/Iteration_16_search_with_base_models/#conclusion","title":"Conclusion","text":"<p>This experimentation successfully demonstrated that a base code-generation model using a DSL can solve a subset of ARC tasks.  The best experiment solved 11.5% of the training tasks. The two most critical factors for success were model size and prediction volume.  Larger models consistently produced higher-quality solutions and solved more tasks. </p> <p>The most effective strategy was a simple brute-force approach, generating hundreds of independent solutions per task, as this method significantly outperformed more complex search techniques aimed at refinement or increasing diversity. These \"smarter\" search prompts were counterproductive, often reducing the variety of generated outputs. While effective, the brute-force method is inefficient, suffering from diminishing returns in solution uniqueness.</p>"},{"location":"modeling/Iteration_16_search_with_base_models/#next-steps","title":"Next steps","text":"<ul> <li>Try to increase search diversity</li> <li>Refine solution</li> </ul>"},{"location":"modeling/Iteration_16_search_with_base_models/#todo","title":"TODO","text":"<ul> <li> How many primitive functions does BARC have? And Hodel?</li> <li> Create a prompt with the available DSL functions</li> <li> What is the best way to do inference?<ul> <li> VLLM</li> <li> Pure python with caching</li> <li> SGLang (I'm not going to try it this iteration)</li> </ul> </li> <li> What is the effect of the model size?</li> </ul>"},{"location":"modeling/Iteration_17_increase_search_diversity/","title":"Iteration 17. Increase search diversity","text":""},{"location":"modeling/Iteration_17_increase_search_diversity/#iteration-17-increase-search-diversity","title":"Iteration 17. Increase search diversity","text":"<p>30-07-2025</p>"},{"location":"modeling/Iteration_17_increase_search_diversity/#goal","title":"Goal","text":"<p>Can I increase search diversity by doing variations to the prompt?</p>"},{"location":"modeling/Iteration_17_increase_search_diversity/#motivation","title":"Motivation","text":"<p>On the previous iteration I tried to increase diversity by feeding previously generated functions to the model in the prompt and asking for new approaches. The effect was the opposite and diversity was reduced. LLMs (at least small LLMs) struggle with negation.</p> <p>In this issue I will take a different approach and instead I will add variation to the prompt:</p> <ul> <li>Add hints about which DSL functions to use</li> <li>Shuffle the order of the training samples</li> <li>Prompt variations (I could prepare different prompts with LLMs)</li> <li>Data augmentation</li> <li>Temperature or other sampling parameters</li> </ul> <p>The goal is to maximize diversity, but at the same time measure generation speed because generating multiple predictions with the same prompt is more efficient than making multiple predictions with different prompts.</p>"},{"location":"modeling/Iteration_17_increase_search_diversity/#development","title":"Development","text":"<p>The work is done on the notebook 009_search_with_base_models</p>"},{"location":"modeling/Iteration_17_increase_search_diversity/#results","title":"Results","text":"<p>I have done 8 predictions for each of the 400 training tasks from ARC-AGI-1. The metric of interest is the number of unique outputs, that is the best way to measure diversity.</p> experiment valid code valid outputs unique outputs dsl usage pixel similarity correct grids solved task inference time (s) unique ratio baseline 99.7% 78.6% 50.8% 56.3% 53.0% 1.9% 3.0% 667 64.64% shuffle train samples 98.5% 74.1% 48.4% 56.8% 53.2% 2.0% 3.3% 1591 65.29% prompt variations (8) 98.8% 73.7% 55.4% 41.1% 51.5% 1.9% 3.0% 2162.0 75.19% dsl suggestions 98.9% 71.7% 40.8% 70.3% 54.0% 1.5% 2.3% 1706 56.91% data augmentation 98.2% 71.5% 46.3% 57.3% 52.4% 1.8% 3.3% 1699 64.80% data augmentation + shuffle train samples 98.2% 72.0% 45.4% 57.8% 52.9% 1.7% 3.0% 1674 63.09% shuffle train samples + remove last train sample 98.5% 76.4% 46.8% 61.3% 54.1% 1.8% 3.0% 1336 61.33% 2 functions per prediction 99.1% 68.5% 46.2% 55.9% 52.2% 1.7% 3.3% 1058.0 67.44% 4 functions per prediction 98.0% 66.4% 45.9% 57.8% 50.1% 1.2% 1.5% 683 69.11% 8 functions per prediction 97.6% 73.5% 33.8% 36.5% 46.4% 0.8% 0.5% 535.0 45.92% <ul> <li>None of the experiments yielded an improvement in output diversity. When using prompt variations we   can see an increase in 5% the unique outputs ratio, but at the cost of not using the dsl 15% less.</li> <li>Doing multiple inferences per prompt is faster and has more variability than the other techniques tried</li> <li>I find weird that data augmentation of shuffling the train samples does not increase the diversity.</li> </ul> <p>I have to take in mind that the model can write different functions that generate the same output. Thus the relation between the metric and the generated code is complex and requires understanding of the effect of the code in the input data.</p>"},{"location":"modeling/Iteration_17_increase_search_diversity/#conclusion","title":"Conclusion","text":"<p>Surprisingly none of the tried techniques were able to increase the diversity of the outputs. I'm afraid I don't have yet a method to exhaustively search the solution space.</p>"},{"location":"modeling/Iteration_17_increase_search_diversity/#next-steps","title":"Next steps","text":"<ul> <li>How can we effectively explore the search space<ul> <li>Funsearch</li> <li>Alphacode (Google DeepMind\u2019s AlphaCode shows that this simple pipeline yields &gt;90\u202f% unique clusters even with off\u2011the\u2011shelf Transformer samplers)</li> </ul> </li> <li>Another source of variability is using multiple LLMs</li> </ul>"},{"location":"modeling/Iteration_19_search_with_BARC/","title":"Iteration 19. Search with BARC","text":""},{"location":"modeling/Iteration_19_search_with_BARC/#iteration-19-search-with-barc","title":"Iteration 19. Search with BARC","text":"<p>18/08/2025</p>"},{"location":"modeling/Iteration_19_search_with_BARC/#goal","title":"Goal","text":"<p>Use the induction model from Boostrapping ARC (BARC) to search code solutions. This will stablish a baseline score that could be later be used to explore methods to improve the efficiency of the search.</p> <p>Warning</p> <p>There was a bug where only the training samples from each task were used on this iteration. The conclusions are still valid but go to Iteration 21 to see the results after fixing the bug.</p>"},{"location":"modeling/Iteration_19_search_with_BARC/#motivation","title":"Motivation","text":"<p>I believe I can validate my ideas using the model from BARC. That saves me the process of generating a DSL and training a model on it. Later if I validate my ideas and find that the DSL is incomplete, I could devote to the task of creating the ultimate DSL. But first I have to validate that we can improve dramatically the accuracy of an induction model by learning from the search results.</p>"},{"location":"modeling/Iteration_19_search_with_BARC/#development","title":"Development","text":"<p>Links:</p> <ul> <li>https://github.com/xu3kev/BARC/tree/master</li> <li>https://huggingface.co/barc0/Llama-3.1-ARC-Potpourri-Induction-8B</li> </ul>"},{"location":"modeling/Iteration_19_search_with_BARC/#grid-encoder","title":"Grid encoder","text":"<p>I have developed a new grid encoder that represents the grids with colors with their names:</p> <pre><code>Black Blue Red Green Yellow Gray Pink Orange Purple Brown\n</code></pre> <p>After being tokenized this does not use more tokens than my implementation, the spaces are encoded with the words.</p> <p>However I have noticed that the first words of each row are represented differently by the encoder. Probably this is not the best option but the model was trained this way. Below you can see how the tokenizer tokenizes a 2x2 matrix with zeros.</p> <pre><code>['Black', '\u0120Black', '\u010a', 'Black', '\u0120Black']\n</code></pre>"},{"location":"modeling/Iteration_19_search_with_BARC/#results","title":"Results","text":""},{"location":"modeling/Iteration_19_search_with_BARC/#comparison-of-passn-in-the-different-datasets","title":"Comparison of pass@n in the different datasets","text":"<p>The plot below shows the pass@n for the different datasets versus the number of predictions. I believe this validates my current implementation because I get similar or better numbers to the ones reported in the BARC paper. This could be happening due to using a higher temperature and/or using more input samples.</p> <p></p> <ul> <li>2024 datasets do not show signs of stopping if we increase the number of predictions, however the   2025 dataset does not improve when increasing the number of predictions from 256 to 1024. This might   be a sign that the 2025 dataset is much harder than the previous one.</li> <li>Interesting to see that the training set is not easily solved, this might indicate that there is room   for improvement</li> </ul>"},{"location":"modeling/Iteration_19_search_with_BARC/#other-metrics-analysis","title":"Other metrics analysis","text":"dataset n_preds valid code valid outputs unique outputs pixel similarity correct grids pass_rate pass@n training-2024 248 100.0% 82.0% 38.1% 61.9% 15.0% 12.40% 58.00% evaluation-2024 568 100.0% 75.9% 40.9% 57.1% 3.0% 1.96% 21.00% evaluation-2025 1560 100.0% 72.8% 39.9% 50.3% 0.1% 0.051% 1.67% <ul> <li>The ratio of unique outputs is quite good, when searching with base models I would only get around 40% unique outputs when doing 16 predictions, and the rate lowered to 20% when doing 512 predictions.</li> <li>Pixel similarity might not be a good metric, the difference between the datasets is small, but the differences in pass rate are huge.   This could be caused by the binary nature of the arc tasks, they are either correct or wrong.</li> </ul>"},{"location":"modeling/Iteration_19_search_with_BARC/#optimizing-the-number-of-predictions-for-throughput","title":"Optimizing the number of predictions for throughput","text":"<p>The plot below shows the throughput vs the number of predictions per prompt for the evaluation-2024 dataset.</p> <p></p> <p>16 predictions per prompt might be the sweet spot for my hardware (RTX 3090). So when applying data augmentation or other tricks on the following iterations, I should be doing 8 or 16 predictions per prompt.</p>"},{"location":"modeling/Iteration_19_search_with_BARC/#conclusion","title":"Conclusion","text":"<p>We have validated that the BARC induction model has the ability to solve ARC tasks using code. This is a good baseline to compare against methods that combine search and test-time training.</p>"},{"location":"modeling/Iteration_19_search_with_BARC/#next-steps","title":"Next steps","text":"<ul> <li>Induction results from the BARC paper are obtained with 20k samples, that gets 38% on the validation set.   With the hardware available at Kaggle I could make around 2k predictions per task. That would yield   a score around 20% on the validation set. 500 predictions yields around 15%, 200 around 10%, not sure how many are needed to get 5% score.   So maybe my approach from last year was not that bad, I was simply not making that many predictions.   Maybe reinforcement learning could increase the efficiency of the model at test-time, reducing the number   of required predictions. </li> </ul>"},{"location":"modeling/Iteration_19_search_with_BARC/#todo","title":"TODO","text":"<ul> <li>[ ]</li> </ul>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/","title":"Iteration 20. Data augmentation with BARC","text":""},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#iteration-20-data-augmentation-with-barc","title":"Iteration 20. Data augmentation with BARC","text":"<p>21-08-2025</p>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#goal","title":"Goal","text":"<p>Does using data augmentation increases the diversity of the predictions and improves the pass@n metric?</p> <p>Warning</p> <p>There was a bug where only the training samples from each task were used on this iteration. The conclusions are still valid but go to Iteration 21 to see the results after fixing the bug.</p>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#motivation","title":"Motivation","text":"<p>On a previous iteration with base models I found that data augmentation was not helpful. That result was weird, so I want to repeat the experiments with BARC.</p>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#development","title":"Development","text":""},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#arc-agi-1-evaluation-set-is-chosen","title":"ARC-AGI-1 evaluation set is chosen","text":"<p>For these experiments I believe that evaluation set from ARC-AGI-1 has the greatest signal. The BARC model was able to solve around 20% of the tasks. The scores on the training set are not trustable because the model was trained on those or similar tasks, while the ARC-AGI-2 evaluation set is more difficult and only 2 out of 120 tasks were solved.</p>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#experimental-setup","title":"Experimental setup","text":"<p>The idea is to reuse all the data augmentation implemented on iteration 17. I will make predictions in batches of 8 or 16 predictions per task, and later I will aggregate all the predictions to estimate the accuracy of the system. I will have to save the data augmentation configuration alongside each prediction to be able to undo it when executing the code.</p>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#results","title":"Results","text":""},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#data-augmentation-improves-the-accuracy-of-the-model-by-increasing-the-diversity-of-the-predictions","title":"Data augmentation improves the accuracy of the model by increasing the diversity of the predictions","text":"<p>The pass@n metric improves when using data augmentation. The difference is bigger when the number of predictions grows. This could explain why my previous experiments with just 8 predictions per task did not show improvements.</p> experiment n_preds valid code valid outputs unique outputs pixel similarity correct grids pass_rate pass@n baseline 568 100.0% 75.9% 40.9% 57.1% 3.0% 1.96% 21.00% data augmentation 584 100.0% 76.5% 44.4% 56.4% 2.9% 1.98% 24.50% <p>This is probably caused by having more diversity on the outputs, the metric that measure the unique outputs improves from 40.9% to 44.4%.</p>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#trustability-of-the-metrics","title":"Trustability of the metrics","text":"<p>I have seen that we can only trust the metrics for a number of predictions around 1/4 of the total of predictions run (at least for pass@n metric). There is a bias to underestimate the pass@n rate when the number of predictions is small.</p> <p></p> <p>Thus when making comparisons between experiments we should try to have a similar number of predictions.</p>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#distribution-of-output-tokens","title":"Distribution of output tokens","text":"<p>I was using the <code>max_tokens=2048</code> from the previous iterations and it seems it is a good value. The median output tokens seems to be around 400, and we can see that the datasets are sorted by inference speed as expected. We could probably be using 1024 output tokens without much impact on the results. The important takeaway is that the current configuration is not hurting the accuracy of the model.</p>"},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_20_data_augmentation_with_BARC/#todo","title":"TODO","text":"<ul> <li> Have a look at some of the solutions to verify they are legit implementations</li> <li> Document some of the predictions</li> <li> Check that I'm using the correct number of training samples. Maybe I should decouple from the Task object. Maybe I'm not giving all the training samples and making the problem harder.   Indeed that is the case, I'm using just the training samples. So I have to fix that bug and repeat the experimentation.</li> <li> Distribution of prediction length</li> </ul>"},{"location":"modeling/Iteration_21_fix_bug_with_data/","title":"Iteration 21. Fix bug with data","text":""},{"location":"modeling/Iteration_21_fix_bug_with_data/#iteration-21-fix-bug-with-data","title":"Iteration 21. Fix bug with data","text":"<p>23-08-2025</p>"},{"location":"modeling/Iteration_21_fix_bug_with_data/#goal","title":"Goal","text":"<p>How good is the BARC induction model on the different ARC datasets?</p>"},{"location":"modeling/Iteration_21_fix_bug_with_data/#motivation","title":"Motivation","text":"<p>I have discovered that I wasn't using the test samples when evaluating the BARC model. This make the problem harder in a way (because not all the training samples were given) and easier in another way (maybe the test samples are more difficult or cover some edge cases). On this iteration I need to stablish a good baseline so I can later check if test-time adaptation improves the scores.</p> <p>I already know that data augmentation is helpful, so I will be using it by default on this iteration.</p>"},{"location":"modeling/Iteration_21_fix_bug_with_data/#results","title":"Results","text":""},{"location":"modeling/Iteration_21_fix_bug_with_data/#accuracy-on-the-different-datasets","title":"Accuracy on the different datasets","text":"dataset n_preds valid code valid outputs unique outputs pixel similarity correct grids train_pass_rate train_pass@n pass_rate pass@n training-arc-agi-1 240 100.00% 81.42% 43.10% 61.40% 14.66% 12.41% 61.75% 12.17% 61.25% evaluation-arg-agi-1 464 100.00% 73.62% 45.19% 56.56% 2.85% 1.98% 23.00% 1.95% 22.25% evaluation-arg-agi-2 264 100.00% 71.29% 51.58% 50.43% 0.11% 0.07% 0.83% 0.06% 0.83% <p>To be able to solve ARC, we need a model that has the right intuitions about how to solve a task. That seems to be the case for the ARC-AGI-1 datasets where we see a constant improvement when making more predictions. But the dynamics for the ARC-AGI-2 dataset are different.  It is worth mentioning that it seems that we would need around 32768 to solve the training set. That would be around 4 hours per task (a prediction takes 0.4 seconds). So even for the training set the resources allowed in the Kaggle submission are not enough.</p> <p>These numbers are very similar to the ones that I have previously to solving the bug.</p> <p>One good property of this model is that the pass rate of the task is almost identical to the pass rate of just the training samples. This implies that a function that is able to solve the training samples is very likely to solve also the test samples.</p>"},{"location":"modeling/Iteration_21_fix_bug_with_data/#effect-of-data-augmentation","title":"Effect of data augmentation","text":"<p>I have repeated the experiments without data augmentation and the results show the same tendency: data augmentation at inference is helpful.</p> <p></p> experiment n_preds valid code valid outputs unique outputs pixel similarity correct grids train_pass_rate train_pass@n pass_rate pass@n baseline 464 100.00% 74.62% 42.63% 57.03% 2.73% 1.85% 20.00% 1.83% 19.50% data augmentation 464 100.00% 73.62% 45.19% 56.56% 2.85% 1.98% 23.00% 1.95% 22.25%"},{"location":"modeling/Iteration_21_fix_bug_with_data/#examples-of-solved-tasks","title":"Examples of solved tasks","text":"<p>In general it seems that the model understands the solved tasks and the solutions look nice.</p> Click to expand/collapse this section  ![alt text](res/1756096296254_image.png)   <pre><code>from common import *\n\nimport numpy as np\nfrom typing import *\n\n# concepts:\n# scaling, color transformation\n\n# description:\n# In the input, you will see a 3x3 sprite with gray pixels scattered randomly. \n# To create the output grid, you should first scale the sprite by a factor of 2, \n# then replace all gray pixels with a pattern of alternating colors (blue and red).\n# The scaled sprite should maintain the original size, and the pattern should cover the gray pixels only.\n\ndef transform(input_grid):\n    # Step 1: Detect the gray pixels in the input grid\n    gray_positions = np.argwhere(input_grid == Color.GRAY)\n\n    # Step 2: Create a new output grid with the same size as the scaled sprite\n    scale_factor = 2\n    output_height = input_grid.shape[0] * scale_factor\n    output_width = input_grid.shape[1] * scale_factor\n    output_grid = np.full((output_height, output_width), Color.BLACK)\n\n    # Step 3: Scale the input grid by the scale factor and place it in the output grid\n    for i in range(input_grid.shape[0]):\n        for j in range(input_grid.shape[1]):\n            if input_grid[i, j] != Color.BLACK:\n                # Blit the original color in the scaled position\n                blit_sprite(output_grid, np.full((scale_factor, scale_factor), input_grid[i, j]), \n                            x=i*scale_factor, y=j*scale_factor)\n\n    # Step 4: Replace gray pixels in the scaled grid with the alternating pattern\n    for x, y in gray_positions:\n        scaled_x, scaled_y = x * scale_factor, y * scale_factor\n        # Create a 2x2 alternating pattern of blue and red\n        pattern = np.array([[Color.BLUE, Color.RED],\n                            [Color.RED, Color.BLUE]])\n        blit_sprite(output_grid, pattern, scaled_x, scaled_y)\n\n    return output_grid\n</code></pre>   ---  ![alt text](res/1756096371982_image.png)   <pre><code>from common import *\n\nimport numpy as np\nfrom typing import *\n\n# concepts:\n# pattern generation, lines\n\n# description:\n# In the input you will see two red pixels. \n# To make the output, you should create a pattern of blue squares and red lines that connect the two red pixels.\n# The pattern consists of blue squares filling the area between the two red pixels, \n# and the red lines should extend vertically and horizontally from the red pixels to the edges of the canvas.\n\ndef transform(input_grid):\n    # Find the positions of the two red pixels\n    red_positions = np.argwhere(input_grid == Color.RED)\n    if len(red_positions) != 2:\n        raise ValueError(\"Input grid must contain exactly two red pixels.\")\n\n    (x1, y1), (x2, y2) = red_positions\n\n    # Determine the bounding box for the blue squares\n    min_x, max_x = min(x1, x2), max(x1, x2)\n    min_y, max_y = min(y1, y2), max(y1, y2)\n\n    # Create blue squares in the bounding box\n    output_grid = np.zeros_like(input_grid)\n    output_grid[min_x:max_x + 1, min_y:max_y + 1] = Color.BLUE\n\n    # Draw red lines from the red pixels to the edges of the canvas\n    draw_line(output_grid, x1, y1, color=Color.RED, direction=(1, 0))  # Right from first red pixel\n    draw_line(output_grid, x1, y1, color=Color.RED, direction=(-1, 0)) # Left from first red pixel\n    draw_line(output_grid, x1, y1, color=Color.RED, direction=(0, 1))  # Down from first red pixel\n    draw_line(output_grid, x1, y1, color=Color.RED, direction=(0, -1)) # Up from first red pixel\n\n    draw_line(output_grid, x2, y2, color=Color.RED, direction=(1, 0))  # Right from second red pixel\n    draw_line(output_grid, x2, y2, color=Color.RED, direction=(-1, 0)) # Left from second red pixel\n    draw_line(output_grid, x2, y2, color=Color.RED, direction=(0, 1))  # Down from second red pixel\n    draw_line(output_grid, x2, y2, color=Color.RED, direction=(0, -1)) # Up from second red pixel\n\n    return output_grid\n</code></pre>   ---  ![alt text](res/1756096418602_image.png)   <pre><code>from common import *\n\nimport numpy as np\nfrom typing import *\n\n# concepts:\n# circle detection, color transformation\n\n# description:\n# In the input, you will see a grid with random colored pixels on it. \n# To make the output, you should find all circular shapes (of any color) \n# with a diameter greater than or equal to 3 pixels and change their color to yellow.\n\ndef transform(input_grid: np.ndarray) -&gt; np.ndarray:\n    # Plan:\n    # 1. Detect circular shapes in the grid\n    # 2. Change their color to yellow if they meet the size criteria\n\n    output_grid = np.copy(input_grid)\n\n    # Iterate over the grid to find circular shapes\n    for x in range(len(input_grid)):\n        for y in range(len(input_grid[0])):\n            # Check if the pixel is not background\n            if input_grid[x, y] != Color.BLACK:\n                # Check for circle shape using a simple heuristic\n                # We will consider a circle if it has a certain diameter\n                diameter = 1\n                while True:\n                    # Check the pixels in the current diameter\n                    if (x + diameter &lt; len(input_grid) and\n                        y + diameter &lt; len(input_grid[0]) and\n                        np.all(input_grid[x:x + diameter + 1, y:y + diameter + 1] == input_grid[x, y])):\n                        diameter += 1\n                    else:\n                        # We found the maximum diameter\n                        diameter -= 1\n                        break\n\n                # If the diameter is 2 or more, we consider it a circle\n                if diameter &gt;= 3:\n                    output_grid[x:x + diameter + 1, y:y + diameter + 1] = Color.YELLOW\n\n    return output_grid\n</code></pre>"},{"location":"modeling/Iteration_21_fix_bug_with_data/#conclusion","title":"Conclusion","text":"<p>BARC induction model is a strong foundation. I believe I should be able to validate my ideas to combine search and learn with it.</p>"},{"location":"modeling/Iteration_21_fix_bug_with_data/#next-steps","title":"Next steps","text":"<ul> <li>I need to experiment with test-time training methods to see if I'm able to improve this metrics</li> <li>We need to increase the prediction efficiency of this model. This would be very likely achieved with reinforcement learning.</li> <li>I'm not exploiting the interactive nature of programming. The model should refine the code using execution feedback. Maybe the model can do this natively, but very likely needs also training.</li> <li>It is possible that ARC-AGI-2 data is out of distribution, and maybe we should label all ARC-AGI-2 tasks and generate new data a la BARC to be able to solve those tasks.</li> </ul>"},{"location":"modeling/Iteration_21_fix_bug_with_data/#todo","title":"TODO","text":"<ul> <li> Visualize the solved tasks</li> <li> Add warnings in the previous iterations</li> </ul>"},{"location":"modeling/Iteration_22_ttt_BARC/","title":"Iteration 22. Test-time Training with BARC induction model","text":""},{"location":"modeling/Iteration_22_ttt_BARC/#iteration-22-test-time-training-with-barc-induction-model","title":"Iteration 22. Test-time Training with BARC induction model","text":"<p>25-08-2025</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#goal","title":"Goal","text":"<p>Can I improve the results on ARC-AGI-1 evaluation with the BARC induction model using test-time training?</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#motivation","title":"Motivation","text":"<p>I have the intuition that we need to combine search and learn to be able to solve novel tasks. Using toy tasks I probed that a model was able to generalize outside its training distribution by training on hindsight relabeled wrong attempts to solve the task. I need to probe that the same technique is helpful for ARC.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#development","title":"Development","text":"<p>My initial idea is to take the predictions from the previous iteration and fine-tune the BARC model on those using hindsight relabel. Then I will do inference again and hopefully I will see improvements.</p> <p>I believe I should see improvements with just one epoch (train + inference) but that doing multiple epochs would yield the best results. I'm not going to worry about efficiency on this iteration, I just want to see if the technique works or it doesn't.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#data-generation","title":"Data generation","text":"<p>The first step is generate the data for training. The fastest way could be to generate the data directly with the chat template from the BARC model.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#first-trainings","title":"First trainings","text":"<pre><code># better work with a single gpu for debugging\nexport CUDA_VISIBLE_DEVICES=0\nexport N_GPUS=1\nexport STEPS=10\nexport MAXSEQLEN=4096\npython scripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-08-25-hr-trainings/3090-GPUS${N_GPUS}-BARC-${STEPS}steps-${MAXSEQLEN}msl \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora \\\n--no-resume_from_checkpoint\n\nexport N_GPUS=2\nexport STEPS=1000\nexport MAXSEQLEN=8192\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\nscripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-08-25-hr-trainings/3090-GPUS${N_GPUS}-BARC-${STEPS}steps-${MAXSEQLEN}msl \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\n</code></pre> <p>I had to solve a bug of my implementation when using gradient checkpointing, and modify the tokenizer from Llama to add the pad token.</p> <ul> <li>Around 3s per instance when training with batch size 32 and 4096 max sequence length.</li> <li>That reduces to 1.6 seconds when using 2 GPUS, so scaling is nice because GPU usage is almost 100% all the time.</li> <li>If I increase the max_seq_len to 8192 the training time per sample increases to 2 seconds, but the memory   seems to increase just from 13GB to 15GB so there might be room for bigger training sequences.</li> <li>Training on 3200 samples would take around 1h40min on my 2x3090 setup. I had to use 4 bit quantization,   liger kernels and gradient checkpoint to avoid the OOM errors.</li> </ul>"},{"location":"modeling/Iteration_22_ttt_BARC/#data-collator","title":"Data collator","text":"<p>The data collator adds a new labels field to the batch that allows to skip the user text.</p> <pre><code>print(tokenizer(text))\n{'input_ids': tensor([[128000, 128000, 128006,  ...,    198,  74694, 128009]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}\ndata_collator([tokenizer(text)])\n{'input_ids': tensor([[128000, 128000, 128006,  ...,    198,  74694, 128009]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  ...,   198, 74694,  -100]])}\n</code></pre> <p>In this case it is ignoring the end of text token because it is the same as the padding token. I have solved it by changing the pad token to <code>&lt;|finetune_right_pad_id|&gt;</code>.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#training-again-in-the-cluster","title":"Training again in the cluster","text":""},{"location":"modeling/Iteration_22_ttt_BARC/#first-steps","title":"First steps","text":"<p>I have updated the requirements of the environment, so the environment will have to be regenerated in the cluster.</p> <pre><code>rsync -P /mnt/data/MEGA/TEMP/2025-08-25_evaluation-85640.json calculon01:/mnt/scratch/users/gbarbadillo/arc25/data\n\nexport N_GPUS=2\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=1000; condor_submit train.condor command=\"\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-25-hr-trainings/${N_GPUS}xA6000--${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr \\\n--max-steps ${STEPS} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--lora-r 32 \\\n--use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\ncondor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-25-hr-trainings/${N_GPUS}xA6000--${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-no-dora \\\n--max-steps ${STEPS} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--lora-r 32 \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nexport N_GPUS=2\nexport LEARNING_RATE=5e-4\nexport MAXSEQLEN=8192\nexport STEPS=1000;\ncondor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-25-hr-trainings/${N_GPUS}xA6000--${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-plain-lora \\\n--max-steps ${STEPS} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--lora-r 32 \\\n--no-use-dora \\\n--no-use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nrsync -P -r calculon01:/mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-25-hr-trainings /mnt/data/MEGA/TEMP --exclude wandb/* --exclude *.pt\n</code></pre> <p>If I remove the gradient checkpointing I get OOM error when using the A6000 GPUs.</p> <p>Training speed comparison (when using plain LoRA):</p> <ul> <li>2xH100: 15.4s/it</li> <li>2xA6000: 40.8s/it</li> <li>2x3090: 46.7s/it</li> </ul> <p>It is possible that we can speedup the H100 training because only 17% of the VRAM memory is being used when using the same configuration as the other GPUs.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#speed-tests","title":"Speed tests","text":"<pre><code>export N_GPUS=1\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=20;\nexport BATCH_SIZE=1\ncondor_submit train.condor command=\" \npython \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-26-speed-tests/${N_GPUS}xA6000-${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-plain-lora-pdbs${BATCH_SIZE} \\\n--max-steps ${STEPS} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size ${BATCH_SIZE} \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--lora-r 32 \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\n\nexport N_GPUS=2\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=20;\ncondor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-26-speed-tests/${N_GPUS}xA6000--${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-plain-lora \\\n--max-steps ${STEPS} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--lora-r 32 \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nexport N_GPUS=7\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=20;\ncondor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-26-speed-tests/${N_GPUS}xA6000--${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-plain-lora \\\n--max-steps ${STEPS} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 56 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--lora-r 32 \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=14 -append request_memory=80G\n</code></pre> <ul> <li>1 GPU: 71.63s/it</li> <li>2 GPUS: 42.74s/it</li> <li>4 GPUs: 26.82s/it</li> <li>7 GPUs: Does not run successfully, probably OOM error but I'm not sure.</li> </ul> <p>2 GPUs seems to be the sweet spot.</p> n gpus per-device-batch-size batch time (s) speedup efficiency 1 1 71.6 1 100.00% 2 1 42.7 1.7 83.84% 4 1 26.8 2.7 66.79% 7 1 - #VALUE! #VALUE! 1 2 75.2 1.0 95.21% 1 4 OOM #VALUE! #VALUE! 1 8 OOM #VALUE! #VALUE!"},{"location":"modeling/Iteration_22_ttt_BARC/#lora-rank-sweep","title":"Lora rank sweep","text":"<pre><code>export LORA_RANK=128\nexport N_GPUS=2\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=1000\ncondor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-26-lora-rank/${N_GPUS}xA6000-${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-lora${LORA_RANK} \\\n--max-steps ${STEPS} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nrsync -P -r calculon01:/mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-26-lora-rank /mnt/data/MEGA/TEMP --exclude *.pt --include checkpoint-*000* --exclude checkpoint* --exclude wandb*\n</code></pre>"},{"location":"modeling/Iteration_22_ttt_BARC/#influence-of-the-number-of-training-steps","title":"Influence of the number of training steps","text":"<pre><code>export LORA_RANK=32\nexport N_GPUS=2\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=4000; condor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--max-steps ${STEPS} \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-27-training-steps/${N_GPUS}xA6000-${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-lora${LORA_RANK} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--no-use-4bit-quantization \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nexport N_GPUS=2\nexport LEARNING_RATE=1e-5\nexport MAXSEQLEN=8192\nexport STEPS=8000; condor_submit train_h100.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--max-steps ${STEPS} \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-85640.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-27-training-steps/${N_GPUS}xH100-${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-full-finetuning \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 500 \\\n--no-use-4bit-quantization \\\n--no-use-lora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n</code></pre>"},{"location":"modeling/Iteration_22_ttt_BARC/#train-on-smaller-datasets","title":"Train on smaller datasets","text":"<pre><code>export LORA_RANK=32\nexport N_GPUS=2\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=50; condor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--max-steps ${STEPS} \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-selected8.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-29-smaller-datasets/${N_GPUS}xA6000-${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-lora${LORA_RANK} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--no-use-4bit-quantization \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\n# change optimizer from paged_adamw_8bit to adamw_torch_fused\n# hopefully that will allow to resume from training and doesn't use too much memory\nexport LORA_RANK=32\nexport N_GPUS=2\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=1000; condor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--max-steps ${STEPS} \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-selected8.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-29-smaller-datasets/${N_GPUS}xA6000-${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-lora${LORA_RANK}-adamw-fused \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--no-use-4bit-quantization \\\n--optim adamw_torch_fused \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\nexport LORA_RANK=32\nexport N_GPUS=2\nexport LEARNING_RATE=1e-4\nexport MAXSEQLEN=8192\nexport STEPS=200; condor_submit train.condor command=\" \naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\n/mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/finetuning_hr.py \\\n--max-steps ${STEPS} \\\n--train_dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/2025-08-25_evaluation-selected8_no-data-augmentation.json \\\n--model_path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-29-smaller-datasets-no-data-augmentation/${N_GPUS}xA6000-${STEPS}steps-${MAXSEQLEN}msl-${LEARNING_RATE}lr-lora${LORA_RANK} \\\n--device-map None \\\n--n-gpus ${N_GPUS} \\\n--learning-rate ${LEARNING_RATE} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers ${N_GPUS} \\\n--logging-steps 1 \\\n--save-steps 100 \\\n--no-use-4bit-quantization \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora\" -append request_gpus=${N_GPUS} -append request_cpus=8\n\n\nexport STEPS=1000; export FOLDER=2025-08-29-smaller-datasets/2xA6000-${STEPS}steps-8192msl-1e-4lr-lora32; condor_submit train_h100.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/inference_with_BARC.py \\\n--n-predictions 512 \\\n--base-model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--lora-path /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/checkpoint-${STEPS} \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--use-data-augmentation \\\n--output-folder /mnt/scratch/users/gbarbadillo/arc25/predictions/${FOLDER}/evaluation\" -append request_gpus=1 -append request_cpus=4\n\n\nexport STEPS=100; export FOLDER=2025-08-29-smaller-datasets-no-data-augmentation/2xA6000-${STEPS}steps-8192msl-1e-4lr-lora32; condor_submit train_h100.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/inference_with_BARC.py \\\n--n-predictions 512 \\\n--base-model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--lora-path /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/checkpoint-${STEPS} \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--no-use-data-augmentation \\\n--output-folder /mnt/scratch/users/gbarbadillo/arc25/predictions/${FOLDER}/evaluation\" -append request_gpus=1 -append request_cpus=4\n</code></pre>"},{"location":"modeling/Iteration_22_ttt_BARC/#qlora-is-saving-the-whole-model","title":"QLoRA is saving the whole model","text":"<p>It seems that when using QLoRA the whole quantized model is saved instead of just the adapter. I might have to save the adapter manually to avoid moving large files.</p> <pre><code>export LORA_RANK=8\nexport CUDA_VISIBLE_DEVICES=0\nexport N_GPUS=1\nexport STEPS=1\nexport MAXSEQLEN=1024\npython scripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-qlora-issue/LoRA_${LORA_RANK} \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 1 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--dataloader_num_workers 1 \\\n--lora-r ${LORA_RANK} \\\n--use-dora \\\n--use-rslora \\\n--no-use-4bit-quantization \\\n--no-resume_from_checkpoint\n\nexport LORA_RANK=8\nexport CUDA_VISIBLE_DEVICES=0\nexport N_GPUS=1\nexport STEPS=1\nexport MAXSEQLEN=1024\npython scripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-qlora-issue/qLoRA_${LORA_RANK} \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 1 \\\n--max-seq-len ${MAXSEQLEN} \\\n--dataloader_num_workers 1 \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--lora-r ${LORA_RANK} \\\n--use-dora \\\n--use-rslora \\\n--use-4bit-quantization \\\n--no-resume_from_checkpoint\n</code></pre> <ul> <li>The saved adapter weights 4.3GB if I use qLoRA, 2.2 if I use LoRA. The first result makes sense if it is saving the whole 4bit quantized model. The second result does not make sense.</li> <li>Reducing the rank from 32 to 8 did not have any effect on the saved weight.</li> <li>Disabling the gradient checkpoint does not have any effect</li> <li>If I save the model manually the cause is clear, it is saving the embeddings layer because the size   is changed when loading the model.</li> </ul> <p>I have solved the issue by reusing the token <code>&lt;|finetune_right_pad_id|&gt;</code> that was already inside the tokenizer instead of creating a new one.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#check-which-lora-versions-are-compatible-with-vllm","title":"Check which LoRA versions are compatible with VLLM","text":"<p>I'm going to run very short train with the different configurations and see if they are compatible with VLLM.</p> <pre><code>export LORA_RANK=8\nexport CUDA_VISIBLE_DEVICES=0\nexport N_GPUS=1\nexport STEPS=1\nexport MAXSEQLEN=8192\npython scripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-lora-compatibility/qLoRA_${LORA_RANK}_dora_rslora \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 1 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--dataloader_num_workers 1 \\\n--lora-r ${LORA_RANK} \\\n--use-dora \\\n--use-rslora \\\n--use-4bit-quantization \\\n--no-resume_from_checkpoint\n\nexport LORA_RANK=8\nexport CUDA_VISIBLE_DEVICES=0\nexport N_GPUS=1\nexport STEPS=1\nexport MAXSEQLEN=8192\npython scripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-lora-compatibility/qLoRA_${LORA_RANK}_rslora \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 1 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--dataloader_num_workers 1 \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora \\\n--use-4bit-quantization \\\n--no-resume_from_checkpoint\n\nexport LORA_RANK=8\nexport CUDA_VISIBLE_DEVICES=0\nexport N_GPUS=1\nexport STEPS=1\nexport MAXSEQLEN=8192\npython scripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-08-26-lora-compatibility/qLoRA_${LORA_RANK} \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 1 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--dataloader_num_workers 1 \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--no-use-rslora \\\n--use-4bit-quantization \\\n--no-resume_from_checkpoint\n</code></pre> <p>VLLM supports LoRA and RSLoRA, it does not support DoRA. Moreover I can give models on the fly, it seems that the first time is slower but otherwise speed looks to be the same.</p> <pre><code>sampling_params = SamplingParams(n=800, temperature=1.0, top_p=0.95, max_tokens=10)\nBase model: 8000 tokens generated in 4.95 seconds (1614.81 tokens/second)\nLoRA model: 8000 tokens generated in 5.78 seconds (1384.20 tokens/second)\nRSLoRA model: 8000 tokens generated in 6.01 seconds (1330.54 tokens/second)\nLoRA model: 8000 tokens generated in 5.26 seconds (1522.17 tokens/second)\nRSLoRA model: 8000 tokens generated in 5.29 seconds (1512.97 tokens/second)\n</code></pre> <p>It seems that the first time a model is called it is slightly slower. And the LoRA model by itself is slightly slower than the base model. But manageable.</p> ChatGPT summary of the 3 techniques  * **LoRA (Low-Rank Adaptation)**   Freeze base weights $W$ and learn a low-rank update $\\Delta W = \\frac{\\alpha}{r} BA$ with $A \\in \\mathbb{R}^{r\\times d_\\text{in}}$, $B \\in \\mathbb{R}^{d_\\text{out}\\times r}$. Cheap to train/serve, drop-in for Q/K/V/O and MLPs.  * **rsLoRA (rank-stabilized / root-scaled LoRA)**   Same idea as LoRA, but changes the scaling (and init) so the update norm is \\~invariant w\\.r.t. rank (often $\\alpha/\\sqrt{r}$ instead of $\\alpha/r$). More stable across different ranks; same runtime cost as LoRA.  * **DoRA (Weight-Decomposed LoRA)**   Decomposes a weight into **direction** and **magnitude**; applies a low-rank update to the direction and learns a small per-channel magnitude (scale) too. Tends to boost quality vs plain LoRA, but needs explicit runtime support because of the decomposition step.   <p>I believe then I should use rsLoRA and don't use DoRA for the following experiments.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#inference-script","title":"Inference script","text":"<pre><code>python scripts/inference_with_BARC.py \\\n--base-model-path /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions/2025-08-27_first-finetuning-steps \\\n--lora-path /mnt/hdd0/MEGA/TEMP/2025-08-26-lora-rank/2xA6000--1000steps-8192msl-1e-4lr-lora32/checkpoint-1000\n\npython scripts/inference_with_BARC.py \\\n--n-predictions 1024 \\\n--base-model-path /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation\n\n\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/inference_with_BARC.py \\\n--base-model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-folder /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-26-lora-rank/2xA6000--1000steps-8192msl-1e-4lr-lora32 \\\n--lora-path /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-26-lora-rank/2xA6000--1000steps-8192msl-1e-4lr-lora32/checkpoint-1000\" -append request_gpus=1 -append request_cpus=4\n\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/inference_with_BARC.py \\\n--n-predictions 1024 \\\n--base-model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-folder /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation\" -append request_gpus=1 -append request_cpus=4\n\ncondor_submit train_h100.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/inference_with_BARC.py \\\n--n-predictions 512 \\\n--base-model-path /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-08-27-training-steps/2xH100-8000steps-8192msl-1e-5lr-full-finetuning-continue/checkpoint-8000 \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-folder /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-27-training-steps/2xH100-8000steps-8192msl-1e-5lr-full-finetuning-continue/evaluation\" -append request_gpus=1 -append request_cpus=4\n\nrsync -P -r calculon01:/mnt/scratch/users/gbarbadillo/arc25/predictions /mnt/data/MEGA/TEMP\nrsync -P -r /mnt/hdd0/MEGA/TEMP/predictions/* /mnt/hdd0/Kaggle/arc25/predictions\n</code></pre>"},{"location":"modeling/Iteration_22_ttt_BARC/#overfit-experiment","title":"Overfit experiment","text":"<pre><code>export LORA_RANK=32\nexport N_GPUS=2\nexport STEPS=100\nexport MAXSEQLEN=8192\naccelerate launch --num_processes ${N_GPUS} --num_machines 1 --mixed_precision bf16 --multi_gpu  \\\nscripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-08-27-overfit/LoRA${LORA_RANK}_${STEPS}steps \\\n--train-dataset-path /mnt/hdd0/Kaggle/arc25/data/hindsight_relabeled/2025-08-25_evaluation-no-data-augmentation-77.json \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 32 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--dataloader_num_workers ${N_GPUS} \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora \\\n--use-4bit-quantization\n\npython scripts/inference_with_BARC.py \\\n--base-model-path /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions/2025-08-27_overfit \\\n--lora-path /mnt/hdd0/Kaggle/arc25/trainings/2025-08-27-overfit/LoRA32_100steps/checkpoint-100 \\\n--no-use-data-augmentation \\\n--n-predictions 1\n</code></pre>"},{"location":"modeling/Iteration_22_ttt_BARC/#evaluation-is-not-deterministic","title":"Evaluation is not deterministic","text":"<p>I have to investigate the sources of variability.</p> <ul> <li>The timeout exception is one source of variability. However if properly implemented should not be a problem.</li> <li>Another source of variability was that the model was using random functions.</li> <li>However the problem is that it seems that the order in which I do the predictions affects the result</li> </ul> <p>After adding more observability it seems that the Color object has been modified.</p> <pre><code>00dbd492 AttributeError type object 'Color' has no attribute 'GREEN'\n05a7bcf2 AttributeError type object 'Color' has no attribute 'YELLOW'\n</code></pre> <p>I have fixed the execution of the code so on each execution the dsl is imported. This solves the problem that the dsl was being modified by some rogue solution.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#create-a-smaller-training-dataset","title":"Create a smaller training dataset","text":"<p>Since I have seen that the model is able to overfit to a small dataset, and does not seem to learn from a huge dataset, I believe we should try training on a smaller selected samples.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#make-code-execution-more-robust-to-crashes","title":"Make code execution more robust to crashes","text":"<p>My current code execution works perfectly most of the time but sometimes it crashes. This is currently preventing me from evaluating one of the fine-tuned models so I have to fix it. The crash is not deterministic, if I run the evaluation multiple times the crash does not happen at the same moment.</p> <p>As far as I understand it seems that I need to use subprocess instead of exec so the code run on a separate process that won't affect the main process if it crashes.</p> <p>Possible solution proposed by GPT5-thinking</p> <p>I have to implement tests for the current function, then verify that the same tests run on the new one.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#results","title":"Results","text":""},{"location":"modeling/Iteration_22_ttt_BARC/#speed-tests_1","title":"Speed tests","text":"GPU n GPUs 4bit quantization batch time (s) RTX3090 2 TRUE 46.7 A6000 1 TRUE 71.6 A6000 2 TRUE 41.5 A6000 4 TRUE 26.8 A6000 2 FALSE 28 H100 2 TRUE 15.4 H100 2 FALSE 9.2 <p>In the cluster is better to use the models unquantized since the GPUs have enough memory and it is much faster.</p> <p>With the H100 I'm able to fully finetune the model and that takes around 11.5s per batch of 32 samples (slightly slower than LoRA).</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#verify-that-i-can-overfit-to-the-training-dataset","title":"Verify that I can overfit to the training dataset","text":"<p>I'm going to create a dataset with just the tasks that were solved without data augmentation and finetune the model on those. I should see the loss dropping fast because the training samples should be a few, and on inference the effect should be very visible.</p> <p>The model was trained with 77 samples (19.25% of the evaluation set). The training loss clearly shows that the model is learning and the evaluation also shows a clear difference, although it does not solve all the training samples.</p> <p></p> dataset experiment n_preds valid code valid outputs unique outputs pixel similarity correct grids train_pass_rate train_pass@n pass_rate pass@n evaluation-arg-agi-1 baseline 8 99.47% 77.81% 71.28% 57.48% 2.82% 1.74% 5.75% 1.70% 5.50% evaluation-arg-agi-1 overfited model 8 100.00% 75.20% 49.80% 63.40% 13.30% 12.60% 16.00% 12.60% 16.00% <p>This validates that using a small dataset the model is able to overfit to it.</p> <p>https://wandb.ai/guillermobarbadillo/2025-08-27-overfit</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#lora-rank","title":"LoRA rank","text":"<p>First trainings do not show any effect on the training metrics when changing the LoRA rank. Maybe I should train for longer?</p> <p>https://wandb.ai/guillermobarbadillo/2025-08-26-lora-rank</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#training-steps","title":"Training steps","text":"<p>Let's fix the lora rank to 32 and use different number of training steps.</p> <p>https://wandb.ai/guillermobarbadillo/2025-08-27-training-steps</p> <p>Training for longer is giving better results at least in training metrics. This result suggest that I'm using a too big dataset for this experiment (I used around 480 predictions from each task).</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#the-right-metrics","title":"The right metrics","text":"<p>This plots show the distribution of scores for the unique solutions for all the ARC-AGI-1 evaluation set. This is evidence that the pixel score is not a good guiding metric, in fact it seems that tasks that weren't solved have higher scores. In the other hand we can see a clear separation between the two groups when inspecting the correct grids ratio. Since the beginning we see that solved tasks have higher scores.</p> <p>This has sense because ARC tasks are all or nothing, the metric used in the competition is binary. For each grid we need all the pixels to be correct. Thus simply measuring how many pixels are correct is not a good metric.</p> <p>Thus when selecting samples for training or when doing reinforcement learning we should use the correct grid ratio.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#training-on-smaller-datasets","title":"Training on smaller datasets","text":"<p>I have created datasets with just 8 task variations (compared to previous experiments with more than 400). Using a batch size of 32 it would take on average 100 steps to see the whole dataset.</p> <p></p> <p>The plot shows that for this dataset of 8 predictions per task the optimal finetuning steps is 1000. Training for longer seems to overfit and does not generalize well. The best finetuned model was able to solve 29.25% of the evaluation tasks compared to 22.25% from the baseline. It's quite a nice improvement considering that we have done a single iteration of search and learn, we could do many more.</p> dataset training steps n_preds valid code valid outputs unique outputs pixel similarity correct grids train_pass_rate train_pass@n pass_rate pass@n evaluation baseline (0) 480 100.00% 71.15% 43.93% 56.57% 2.94% 2.03% 23.00% 2.00% 22.25% evaluation 20 512 100.00% 75.31% 45.52% 57.90% 3.60% 2.39% 24.75% 2.36% 23.75% evaluation 50 512 100.00% 75.40% 45.83% 57.90% 3.45% 2.30% 26.75% 2.27% 25.00% evaluation 100 512 100.00% 76.02% 46.12% 58.53% 3.63% 2.42% 26.50% 2.38% 25.75% evaluation 200 512 100.00% 78.08% 46.67% 59.41% 4.06% 2.71% 26.00% 2.64% 25.25% evaluation 400 512 100.00% 77.93% 44.89% 60.80% 4.89% 3.27% 29.25% 3.12% 28.00% evaluation 1000 512 100.00% 80.40% 42.54% 62.88% 6.00% 3.65% 31.00% 3.39% 29.25% evaluation 2000 512 100.00% 82.53% 40.19% 64.22% 6.82% 3.91% 28.00% 3.60% 27.00% Click to see the same results without data augmentation  ![alt text](res/1756814521219_image.png)"},{"location":"modeling/Iteration_22_ttt_BARC/#are-the-improvements-legit","title":"Are the improvements legit?","text":"<p>Let's describe the experiment that we have just done:</p> <ol> <li>Make ~512 predictions with the base BARC model for the 400 evaluation tasks from ARC-AGI-1</li> <li>Select 8 unique predictions that have the highest correct grid score</li> <li>Finetune the BARC model on those predictions for 1000 steps, using hindsight relabeling</li> <li>Make 512 predictions with the finetuned model</li> </ol> <p>That increased the solved tasks from 22.25% to 29.25%.</p> <p>However we could criticise that the finetuned model has done 1024 predictions in total, and that it saw already solved tasks during its training. To check if this concerns are real I have made more than 6k predictions with the base model. That way I could better characterize the difficulty of each task for the base model, that is simply the pass rate for each task.</p> <p>This is what I have found:</p> <ul> <li>117 tasks were solved by the finetuned model, from those 88 were already solved by the base model and   were using for training. That is 22% of the tasks, so almost all the solved tasks by the base model (22.25%)   were solved again by the finetuned model.</li> <li>We are interested in the remaining 29 tasks that were newly solved.</li> </ul> <p></p> <p>The fine-tuned model using TTT was able to solve tasks that on average require more than 1024 predictions to be solved (72% of the newly solved tasks). In fact 17% of the tasks were not solved after doing more than 6000 predictions, they are shown in the plot as requiring more than 10k predictions per task, but that is simply an estimate to represent them.</p> <p>Thus I believe that we can say that test-time training increased the inference efficiency of the model, in some tasks by more than 10 times.</p>"},{"location":"modeling/Iteration_22_ttt_BARC/#conclusion","title":"Conclusion","text":"<p>Now I have evidence that test-time training using hindsight relabelling can boost the accuracy of a model that uses code to solve ARC tasks. This is a huge deal. Starting from a model that solved 22.25% of the evaluation tasks with ~500 predictions we were able to finetune a model that solved 29.25% of the tasks with the same inference budget. For some of the solved tasks the inference efficiency was increased more than 10 times.</p> <p>Other learnings:</p> <ul> <li>Pixel accuracy does not seem to be a good metric, is better to check how many grids are completely correct.</li> <li>Changing some parameters of the training such as the save steps does not allow to continue training afterwards.</li> <li>I have seen errors when trying to resume the training. They might be caused by the optimizer being <code>paged_adamw_8bit</code> but I'm not sure. I will be using <code>adamw_torch_fused</code> from now on.</li> </ul>"},{"location":"modeling/Iteration_22_ttt_BARC/#next-steps","title":"Next steps","text":"<ul> <li>Do multiple iterations of search and learn</li> <li>There is a bug in the estimation of the solve rate probability that overestimates it.</li> </ul>"},{"location":"modeling/Iteration_22_ttt_BARC/#todo","title":"TODO","text":"<ul> <li> Prepare the training data.<ul> <li> Small toy dataset</li> <li> With and without data augmentation</li> <li> With and without solved tasks</li> </ul> </li> <li> Which LoRA parameters are compatible with VLLM? rsLoRA is compatible, DoRA isn't</li> <li> Fix issue with qlora model saving the complete model</li> <li> Train the model on the cluster</li> <li> Verify that I can overfit on a small dataset</li> <li> Script for inference<ul> <li> With support for LoRA</li> <li> Add tests for data augmentation</li> <li> Think if moving the prompt has sense</li> <li> Including evaluation of the predictions, otherwise I have to do it on my computer.</li> <li> Try the script on the cluster</li> <li> There might be a problem with <code>os.environ['CUDA_VISIBLE_DEVICES'] = str(get_least_used_gpu_index())</code> on the cluster or on my computer. Probably it should only do changes if the variable is not set.</li> </ul> </li> <li> Find best training hyperparameters (learning rate, batch size, lora rank, training steps)<ul> <li> Lora rank</li> <li> Training steps</li> <li> Learning rate/batch size</li> </ul> </li> <li> Check training data: the order should be random</li> <li> Evaluation is not deterministic. Investigate the source of variability.</li> <li> Select a small number of samples per training<ul> <li> Can I unify the evaluation script</li> <li> How to select the samples?</li> <li> Train and evaluate</li> </ul> </li> <li> Error when trying to evaluate a lot of predictions<ul> <li> What is the best and safest way to execute a lot of code in parallel?</li> <li> Subprocess could be the way</li> <li> Or maybe there is some way to parallelize that is safe to crashes</li> <li> Save execution results to disk so I can repeat evaluations faster</li> </ul> </li> <li> Evaluate experiments and close iteration<ul> <li> Full fine-tuned model</li> <li> Models trained on small data</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/","title":"Iteration 23. All in with test-time training with BARC induction model","text":""},{"location":"modeling/Iteration_23_ttt_BARC_v2/#iteration-23-all-in-with-test-time-training-with-barc-induction-model","title":"Iteration 23. All in with test-time training with BARC induction model","text":"<p>02/09/2025</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#goal","title":"Goal","text":"<p>Create an efficient implementation of test-time training with BARC that tries to solve each task independently.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#motivation","title":"Motivation","text":"<p>On the previous iteration I have seen that TTT is able to improve the solving rate of the BARC induction model. That experiment was done using all the tasks at once. I already know from the previous competition that is better to solve each task independently, I believe that creates a cleaner gradient signal.</p> <p>Also I know from my toy experiments that multiple iterations of search and learn are needed to solve tasks that are far from the training distribution. Sometimes requiring in the order of tens of epochs.</p> <p>Thus in this iteration I want to implement an efficient way to do search and learn in multiple epochs. If the implementation is successful it will very likely be part of my solution for the 2025 challenge.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#development","title":"Development","text":""},{"location":"modeling/Iteration_23_ttt_BARC_v2/#implementation-ideas","title":"Implementation ideas","text":"<ul> <li>Inference with VLLM is very efficient, and I can use different LoRAs which is convenient for test-time training.</li> <li>trl could be used for training, although I don't know if it is the best option.</li> <li>I believe unsloth is integrated with VLLM, which will make inference as fast and maybe is the   best way to do inference and training in the same process. Otherwise I would have to have a   training service, an inference service and a master service that redirects the traffic between   the two.</li> </ul>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#trying-unsloth","title":"Trying unsloth","text":"<p>Documentation is awesome.</p> <p>I have verified that I can do fast inference and fast training in the same process with unsloth. Thus I'm going to implement the algorithm with unsloth and unless I see performance problems I will stick with it until the end of the challenge.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#training-speed-experiment","title":"Training speed experiment","text":"<pre><code># baseline with huggingface and trl\nexport CUDA_VISIBLE_DEVICES=0\nexport LORA_RANK=32\nexport N_GPUS=1\nexport STEPS=100\nexport MAXSEQLEN=8192\npython scripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-speed-tests/LoRA${LORA_RANK}_${STEPS}steps_baseline-repeat \\\n--train-dataset-path /mnt/hdd0/Kaggle/arc25/data/hindsight_relabeled/2025-08-25_evaluation-no-data-augmentation-77.json \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 1 \\\n--learning-rate 1e-5 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--dataloader_num_workers ${N_GPUS} \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora \\\n--use-4bit-quantization\n</code></pre> <pre><code># repeat with unsloth\nexport CUDA_VISIBLE_DEVICES=0\nexport LORA_RANK=32\nexport N_GPUS=1\nexport STEPS=100\nexport MAXSEQLEN=8192\npython scripts/finetuning_hr.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-03-speed-tests/LoRA${LORA_RANK}_${STEPS}steps_unsloth-remove-dropout \\\n--train-dataset-path /mnt/hdd0/Kaggle/arc25/data/hindsight_relabeled/2025-08-25_evaluation-no-data-augmentation-77.json \\\n--device-map None \\\n--max-steps ${STEPS} \\\n--n-gpus ${N_GPUS} \\\n--per-device-train-batch-size 1 \\\n--batch-size 1 \\\n--learning-rate 1e-5 \\\n--max-seq-len ${MAXSEQLEN} \\\n--logging-steps 1 \\\n--save-steps 1000 \\\n--dataloader_num_workers ${N_GPUS} \\\n--lora-r ${LORA_RANK} \\\n--no-use-dora \\\n--use-rslora \\\n--use-4bit-quantization \\\n--use-unsloth\n</code></pre> <ul> <li>The baseline trains 0.468 samples per second.</li> <li>First run with unsloth train 0.571 samples per second, slightly faster. However it uses just 36% memory instead of 64%</li> <li>When loading unsloth at the top of the script, speed improves to 0.615 samples per second</li> <li>Removing dropout from LoRA improves the speed to 0.629 samples per second</li> <li>Not using liger kernel seems to slow down to 0.618, but change is small3</li> <li>Using 8 bit quantization instead of 4 bit gets 0.605 samples per second</li> <li>Using an unquantized model improves the speed to 0.672 samples per second, 43% faster than the baseline.</li> </ul> <p>So far we are seeing an speedup of 34% and a 50% reduction in VRAM usage when using unsloth. It might be possible to trade that VRAM reduction for speed.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#inference-speed-test","title":"Inference speed test","text":"<p>unsloth has a faster startup of 54s vs 1m51s for VLLM.</p> <p>The table below shows the inference speed in tokens/s when generating 100 tokens per prompt.</p> method \\ n predictions 8 32 128 512 VLLM 140 512 1476 1992 unsloth 138 510 1454 1464 <p>They are very similar except from the last column, where I believe VLLM is using more VRAM memory than unsloth. This is promising because it opens the door to use unsloth both for training and inference in the same process.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#search-and-learn-algorithm","title":"Search and Learn algorithm","text":"<p>This is how one epoch of the search and learn algorithm would look like, the algorithm works on a single task:</p> <ol> <li>Make n predictions with the model. Use data augmentation to increase the diversity of the predictions.</li> <li>Parse the code from the predictions and execute the code to get the output grids</li> <li>Evaluate the outputs on the training samples of the task</li> <li>There could be some stopping criteria, for example if I have two different solutions that solve    all the training tasks.</li> <li>Prepare the data for training. I could sort them by the number of correct grids or other metrics.    I could remove already predicted solutions on previous epochs. Using hindsight relabelling we    generate new tasks for training.</li> <li>Finetune the model</li> <li>Repeat until the stop criteria is met or the number of maximum epochs is reached</li> <li>Select the predictions for submission</li> </ol> <p>Using a smaller number of predictions could be more efficient, according to previous experiments with toy tasks. Once the algorithm is implemented I will have to tune all the hyperparameters. On Kaggle I will have 12 minutes per task when running the algorithm in parallel on the 4 GPUs.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#validate-inference","title":"Validate inference","text":"<p>I have done experiments on the training dataset to validate that the inference is correct and gives the same results as in previous iterations.</p> <pre><code># 400 training tasks, 20m49\nn_preds valid code  valid outputs   unique outputs  train_pixel_score   train_correct_grids train_pass_rate train_is_correct    test_pixel_score    test_correct_grids  test_pass_rate  test_is_correct is_correct\nMEAN    8.0 1.0 0.774   0.616   0.484   0.121   0.107   0.26    0.474   0.113   0.112   0.278   0.258\n# this validates the inference pipeline, results are very similar as the shown below\n\n# baseline, runtime around 21 minutes with batch size 8\n    n_preds valid code  valid outputs   unique outputs  pixel similarity    correct grids   train_pass_rate train_pass@n    pass_rate   pass@n\nMEAN    8.0 1.0 0.753125    0.6175  0.594594    0.129178    0.105435    0.2225  0.104452    0.2175\nMEAN    8.0 1.0 0.758125    0.625313    0.602329    0.12629 0.103494    0.2625  0.101396    0.26\nMEAN    8.0 1.0 0.765625    0.615938    0.611174    0.148103    0.12081 0.27    0.11822 0.265\nMEAN    8.0 1.0 0.761875    0.614688    0.595542    0.130861    0.113375    0.2625  0.111104    0.2625\n</code></pre>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#effect-of-4-bit-quantization-at-inference","title":"Effect of 4 bit quantization at inference","text":"<pre><code># training\n# unquantized, 20m49\n# same but with 4 bit quantization, 24m24\nn_preds valid code  valid outputs   unique outputs  train_pixel_score   train_correct_grids train_pass_rate train_is_correct    test_pixel_score    test_correct_grids  test_pass_rate  test_is_correct is_correct\nMEAN    8.0 1.0 0.774   0.616   0.484   0.121   0.107   0.26    0.474   0.113   0.112   0.278   0.258\nMEAN    8.0 1.0 0.771   0.63    0.468   0.103   0.088   0.248   0.457   0.095   0.095   0.285   0.242\n\n# evaluation\n# unquantized 29m15\n# 4bit quantization, 34m36\n    n_preds valid code  valid outputs   unique outputs  train_pixel_score   train_correct_grids train_pass_rate train_is_correct    test_pixel_score    test_correct_grids  test_pass_rate  test_is_correct is_correct\nMEAN    8.0 1.0 0.709   0.633   0.413   0.021   0.013   0.058   0.402   0.016   0.016   0.07    0.058\nMEAN    8.0 1.0 0.708   0.634   0.415   0.022   0.015   0.058   0.404   0.018   0.018   0.068   0.058\n</code></pre> <p>It seems that quantization makes inference slower, but accuracy seems to be the same.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#cluster-experiments","title":"Cluster experiments","text":""},{"location":"modeling/Iteration_23_ttt_BARC_v2/#first-steps","title":"First steps","text":"<pre><code>export FOLDER=2025-09-07-search-and-learn\nexport N_PREDICTIONS=128; condor_submit train_h100.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions ${N_PREDICTIONS} \\\n--max-epochs 0 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/baseline_${N_PREDICTIONS}\" -append request_gpus=1 -append request_cpus=32\n</code></pre> <pre><code>export FOLDER=2025-09-07-search-and-learn\nexport N_PREDICTIONS=128\nexport LEARNING_RATE=1e-5; condor_submit train_h100.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions 64 \\\n--predictions-per-epoch 64 \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs 1 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/2partitions_${N_PREDICTIONS}_lr${LEARNING_RATE}\" -append request_gpus=1 -append request_cpus=32\n</code></pre> <pre><code>export FOLDER=2025-09-07-search-and-learn\nexport N_PREDICTIONS=128\nexport LEARNING_RATE=1e-4; condor_submit train_h100.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions 32 \\\n--predictions-per-epoch 32 \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs 3 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/4partitions_${N_PREDICTIONS}_lr${LEARNING_RATE}\" -append request_gpus=1 -append request_cpus=32\n</code></pre> <pre><code>export FOLDER=2025-09-07-search-and-learn\nexport N_PREDICTIONS=128\nexport LEARNING_RATE=1e-4; condor_submit train_h100.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions 16 \\\n--predictions-per-epoch 16 \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs 7 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/8partitions_${N_PREDICTIONS}_lr${LEARNING_RATE}\" -append request_gpus=1 -append request_cpus=32\n</code></pre>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#debugging","title":"Debugging","text":"<pre><code>export FOLDER=2025-09-07-debug-search\nexport BATCH_SIZE=8; export N_PREDICTIONS=128; condor_submit train_h100.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions ${N_PREDICTIONS} \\\n--max-epochs 0 \\\n--inference-batch-size ${BATCH_SIZE} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/baseline_${N_PREDICTIONS}preds_${BATCH_SIZE}batch\" -append request_gpus=1 -append request_cpus=16\n</code></pre>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#experiments-with-512-predictions","title":"Experiments with 512 predictions","text":"<pre><code>export FOLDER=2025-09-18-search-and-learn\nexport N_PREDICTIONS=512; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions ${N_PREDICTIONS} \\\n--max-epochs 0 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${N_PREDICTIONS}i_baseline\" -append request_gpus=1 -append request_cpus=12 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n</code></pre> <pre><code>export FOLDER=2025-09-18-search-and-learn\nexport INITIAL_PREDICTIONS=256\nexport EPOCHS=1\nexport PREDICTIONS_PER_EPOCH=256\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}\" -append request_gpus=1 -append request_cpus=12 -append request_memory=16G\n</code></pre>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#local-experiments","title":"Local experiments","text":""},{"location":"modeling/Iteration_23_ttt_BARC_v2/#debugging-degradation-of-scores","title":"Debugging degradation of scores","text":"<p>I have observed a degradation in the number of valid outputs when using more than one batch. Let's run some experiments to try to better understand the problem.</p> <pre><code>python scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions 32 \\\n--inference-batch-size 8 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-07-debug-unsloth-local/batch8\n\npython scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions 32 \\\n--inference-batch-size 8 \\\n--no-use-data-augmentation \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-07-debug-unsloth-local/no-data-augmentation-batch8-b\n</code></pre> <p>First experiments suggests that it is related to data augmentation. I believe I have found and fixed the bug. It seemed that I was applying data augmentation over and over on the same task, thus losing the traceability of the applied data augmentation.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#debug-runtime","title":"Debug runtime","text":"<pre><code>python scripts/search_and_learn_with_unsloth.py \\\n--initial-predictions 32 \\\n--inference-batch-size 8 \\\n--max-epochs 3 \\\n--predictions-per-epoch 32 \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/mini-arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-08-debug-runtime/batch8_32preds_3epochs\n\n</code></pre>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#how-trl-grpotrainer-works","title":"How trl GRPOTrainer works","text":"<ul> <li>https://huggingface.co/docs/trl/en/vllm_integration</li> <li>https://github.com/huggingface/trl/blob/659d2c1284e06862efbbccf64cd4310bcee4f200/trl/trainer/grpo_trainer.py#L54</li> <li>https://github.com/huggingface/trl/blob/main/trl/extras/vllm_client.py#L46</li> <li>https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_config.py#L22</li> <li>https://chatgpt.com/share/68c050f0-2ecc-8012-831d-29f7084ae526</li> <li>https://huggingface.co/learn/llm-course/en/chapter12/6</li> </ul> <p>When using vLLM, ensure the GPUs assigned for training and generation are separate to avoid NCCL communication conflicts.</p> <ul> <li>It seems that trl implements a custom VLLM that allows changing the weights.</li> <li>Examples using unsloth and GRPO do not enable VLLM, maybe GRPO patches it to use fast_generate.</li> <li>Trl code is very long, covering a lot of edge cases and difficult to understand</li> </ul>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#oom-errors-on-kaggle","title":"OOM errors on Kaggle","text":"<p>When training on the longer tasks I'm getting OOM errors on Kaggle (24GB GPU).</p> <pre><code>[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 248.00 MiB. GPU 0 has a total capacity of 22.28 GiB of which 33.38 MiB is free. Process 6314 has 22.23 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, with 199.88 MiB allocated in private pools (e.g., CUDA Graphs), and 171.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n</code></pre>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#making-code-execution-robust","title":"Making code execution robust","text":"<p>This was the problem. Sometimes execution hangs and no exception is thrown:</p> <ul> <li>https://wandb.ai/guillermobarbadillo/2025-09-07-search-and-learn/runs/0iswo84s/logs</li> <li>https://wandb.ai/guillermobarbadillo/2025-09-11-search-and-learn/runs/xldcleic/logs</li> </ul> <p>Sometimes raises exception:</p> <ul> <li>https://wandb.ai/guillermobarbadillo/2025-09-07-search-and-learn/runs/kd4qttau/logs</li> </ul> <p>Thus I have made many code changes to improve robustness, following suggestions by ChatGPT.</p> <p>On my pc it executes very fast: <code>400/400 [00:02&lt;00:00, 152.92pred/s]</code></p> <p>But in the cluster I'm seeing very slow executions:</p> <ul> <li><code>12800/12800 [50:22&lt;00:00,  4.24pred/s]</code> Experiment</li> <li><code>51200/51200 [03:28&lt;00:00, 245.29runs/s]</code> Older experiment with good speed</li> </ul> <p>However in Kaggle is also fast: <code>960/960 [00:03&lt;00:00, 265.18pred/s]</code></p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#reverting-to-old-code","title":"Reverting to old code","text":"<p>I have tried reverting back to commit 1557726a0e184d1a4e0b0490eec44bde7dde304e, from 8 september when I logged fast execution times. However the problem persisted:</p> <ul> <li>4 cpus -&gt; 41.67runs/s</li> <li>8 cpus -&gt; 61.31runs/s</li> <li>20 cpus -&gt; 56.75runs/s</li> <li>64 cpus -&gt; 9.51runs/s</li> <li>128 cpus -&gt; 9.41 runs/s</li> </ul> <p>I have also tried running on other machine (calculon19 instead of calculon21) but did not get better results:</p> <ul> <li>8 -&gt; 74.22runs/s</li> <li>16 -&gt; 86.01runs/s</li> </ul>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#simpler-script","title":"Simpler script","text":"<p>Iterations have been slow because I'm doing inference with the model first. That makes that each execution takes around 30 minutes. I need to create a script that allows me to see results much faster. That way I will run the same script with the same data in the different settings and get more information about the problem faster.</p> <p>I have prepared the script and I cannot understand the problem. Could it be a problem with the environment? TODO: repeat experiments when updating the environment</p> <pre><code>export N_CPUS=8; condor_submit train.condor command=\" python /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/debug_parallel_execution.py \\ --dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\ --prediction-path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json\" -append request_cpus=${N_CPUS} -append request_gpus=0\n</code></pre>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#the-code-isnt-robust-yet","title":"The code isn't robust yet","text":"<p>I have done a quick test trying to evaluate around ~6000 predictions per task and I have seen that the code hangs.</p> <p>If the evaluations are done sequentially, file by file, there is no problem except for this two files that produce consistent hangs:</p> <ul> <li>/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_08_31_09_47_48_predictions.json, 252</li> <li>/mnt/hdd0/Kaggle/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_01_13_46_42_predictions.json, 670</li> </ul> <p>Each file has 3200 tasks, so I could use that as reference.</p> <p>Investigating those tasks I see that both tasks have a general except clause that might be causing the problems:</p> <pre><code>while True:\n    try:\n        expand_star(center_x, center_y, star_color, center_color, distance)\n        distance += 1\n    except:\n        break\n</code></pre> <p>After fixing that I have been able to evaluate the 6064 predictions per task in one run without problems.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#results","title":"Results","text":""},{"location":"modeling/Iteration_23_ttt_BARC_v2/#unslothvllm-inference-throughput","title":"Unsloth/VLLM inference throughput","text":"<p>Making more predictions has higher throughput, also using a bigger batch size has higher throughput but at the cost of lowering prompt diversity. I would need to tune this hyperparameters.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#first-experiments","title":"First experiments","text":"<p>https://wandb.ai/guillermobarbadillo/2025-09-07-search-and-learn</p> <ul> <li>Execution time with two partitions is 6x the baseline (5300s vs 31000)</li> <li>Each search and learn iteration is taking around 72 seconds. But half of the time seems to go prepare the model for training. I need to log that.</li> <li>Training takes around 20s per task</li> <li>Inference around 10s per taks, but is twice as slower as when doing the inference of all the tasks together</li> <li>Thus I estimate that an efficient implementation could do the same in 12000  seconds (<code>30*400</code>).</li> </ul> <pre><code>#Baseline just inference\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 51200/51200 [1:17:57&lt;00:00, 10.95it/s, est. speed input: 21508.87 toks/s, output: 4414.70 toks/s]\n\n\nhttps://wandb.ai/guillermobarbadillo/2025-09-07-search-and-learn/runs/xceyyl8q/logs\n#training\n{'train_runtime': 19.3244, 'train_samples_per_second': 2.432, 'train_steps_per_second': 2.432, 'train_loss': 0.2810402642539207, 'epoch': 1.0}\n{'train_runtime': 23.5279, 'train_samples_per_second': 1.488, 'train_steps_per_second': 1.488, 'train_loss': 0.25343174253191264, 'epoch': 1.0}\n{'train_runtime': 18.2541, 'train_samples_per_second': 1.972, 'train_steps_per_second': 1.972, 'train_loss': 0.27254368571771515, 'epoch': 1.0}\n\n#inference\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64/64 [00:10&lt;00:00,  5.94it/s, est. speed input: 6103.78 toks/s, output: 2366.72 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64/64 [00:11&lt;00:00,  5.53it/s, est. speed input: 15778.11 toks/s, output: 2169.40 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 64/64 [00:12&lt;00:00,  5.10it/s, est. speed input: 9888.11 toks/s, output: 2197.39 toks/s]\n\nTasks:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 279/400 [5:23:04&lt;2:25:52, 72.33s/task]2025-09-08 06:15:54,100 - __main__ - INFO - main -\n\n#30 seconds seem to be training startup, half of the time.\n\n# effect of the number of search and learn steps: [1, 3, 7]\nTasks:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 279/400 [5:23:04&lt;2:25:52, 72.33s/task]2025-09-08 06:15:54,100 - __main__ - INFO - main -\n\nTasks:   6%|\u258b         | 26/400 [57:03&lt;13:05:33, 126.02s/task]2025-09-08 13:59:13,056 - __main__ - INFO - main - Search and learn for task 136b0064\nTasks:   3%|\u258e         | 12/400 [44:55&lt;24:35:11, 228.12s/task]2025-09-08 13:56:54,181 - __main__ - INFO - main - Search and learn for task 0a2355a6\n\n\n</code></pre> learning rate is_correct valid outputs unique outputs baseline (no finetuning) 16.90% 70.80% 49.50% 1.00E-03 14.75% 35.50% 28.11% 1.00E-04 17.50% 74.91% 51.01% 1.00E-05 18.25% 72.42% 49.18% 1.00E-06 17.25% 68.33% 47.89% 1.00E-07 16.00% 70.98% 49.70% <p>First experiments show a small but noticeable improvement when doing search and learn. Notice that only one iteration of search and learn was done. So the baseline just did 128 predictions, and the other experiments did 64 predictions, learned from those and did 64 additional predictions with the finetuned model.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#analyze-inefficiencies-in-approach","title":"Analyze inefficiencies in approach","text":"<p>The total amount of compute should be independent of the number of search and learn iterations that I do per task, but the runtime is being heavily affected:</p> <ul> <li>1 iteration: 72s/task</li> <li>3 iterations: 126s/task</li> <li>7 iterations: 228s/task</li> </ul> <p>Let's analyze why this is happening</p> <pre><code># 3 iterations 0b17323b, Total: 122s\nReset PEFT weights: 5s\nPrepare training data: 3s\nTraining startup: 14s\nTrain: 15s\nInference: 7s\nPrepare training data: 2s\nTraining startup: 15s\nInference: 7s\nPrepare training data: 2s\nTraining startup: 16s\nTrain: 15s\nInference: 9s\nTotal: 122s\n# 7 iterations 0b17323b, Total 245s\nReset PEFT weights: 6s\nPrepare training data: 1, 2, 2\nTraining startup: 10, 10, 11\nTrain: 7, 7, 7\nInference: 8, 11, 7\n# 1 iteration ff72ca3e, 86s\nReset PEFT weights: 10\nPrepare training data: 5\nTraining startup: 22\nTrain: 27\nInference: 14\n</code></pre> <p>Training startup time is not constant, that is weird. Maybe I'm not measuring it correctly and it has something to do with the data.</p> <p>This is a summary table for the 3 iterations.</p> <ul> <li>Total time (all entries): 125 s (2 min 2 s)</li> </ul> Task Count Average time Reset PEFT weights 1 5.00 s Prepare training data 3 2.33 s Training startup 3 15.00 s Train 3 15.00 s Inference 3 7.67 s <p>I need to do more experiments and better log the execution time.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#tokenize-before-training","title":"Tokenize before training","text":"<p>On a local experiment I have been able to reduce the execution time from 960s to 914s by tokenizing the dataset before training.</p> <p>I have the feeling that training startup at H100 is longer than 3090, and also since training and inference on 3090 is around 4 times slower than on the H100, the startup time has a smaller effect.</p> <p>It has taken around 228s per task on the 3090 (although I only used 4 tasks.) I was doing 128 predictions, so I could likely be doing 512 predictions on Kaggle.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#l4-is-slower-than-i-thought-half-as-fast-as-the-3090","title":"L4 is slower than I thought, half as fast as the 3090","text":"<p>When making inference for the ARC-AGI-2 evaluation set I get around 340 token/s of throughput with Kaggle's L4 GPUs. In comparison I can get around 650 token/s with my 3090 GPUs. So L4 is around half as fast as the 3090.</p> <p>Notice that the 3090 was launched in September 2020, and the L4 was launched on March 2023. Quite surprising.</p> <p>On average each prediction is taking around 1.26 seconds on Kaggle. That implies that I won't be able to do more than 512 predictions per task on a submission using the current model. And that is without considering the training time, that would be the time for just making predictions. A more conservative approach would be 256 predictions per task, or even less. Thus we need a much stronger model than the BARC one to be able to reach 85% accuracy.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#is-the-current-implementation-efficient-enough","title":"Is the current implementation efficient enough?","text":""},{"location":"modeling/Iteration_23_ttt_BARC_v2/#kaggle-l4","title":"Kaggle L4","text":"<p>TODO: I'm currently running tests on Kaggle to measure GPU usage and throughput. https://docs.google.com/spreadsheets/d/1NmmCZA7gPOyoBypwvpw_JhYdjcvqNFHibX_WahwTHIM/edit?gid=0#gid=0&amp;range=A783</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#h100","title":"H100","text":"<p>TODO: 128 preds</p> <p>TODO: 512 preds</p> <p>32 GB of RAM seem to be enough to do experiments with 512 predictions. Although the limit might be too tight.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#do-i-have-clear-evidence-that-the-approach-works","title":"Do I have clear evidence that the approach works?","text":"<p>https://wandb.ai/guillermobarbadillo/2025-09-18-search-and-learn</p> <p>Yes, the plot below shows two experiments of search and learn vs 3 baseline experiments that simply do independent predictions.</p> <p></p> <p>All the experiments do the same number of predictions: 512. The difference is that the blue and green line learn from the predictions:</p> <ul> <li>Green line does 256 predictions, learns and does other 256 predictions.</li> <li>Blue line learns every 128 predictions.</li> </ul> <p>That explains why the blue line separates early from the baseline.</p> initial predictions epochs predictions per epoch pass@n 512 0 0 23.3% 256 1 256 26.0% 128 3 128 28.3% <p>We get an improvement of 5% with the best configuration, but is very likely that we could get even better results by using more epochs (a more continuous learning approach).</p> <p>On Kaggle I have done experiments with the ARC-AGI-2 evaluation set and didn't observe improvements yet. However the start point is just 0.8% pass@2 so it is a totally different level of difficulty.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#current-implementation-is-too-slow","title":"Current implementation is too slow","text":"initial predictions epochs predictions per epoch runtime (h) 512 0 0 23 256 1 256 54 128 3 128 74 <p>However the current implementation is slow, and the worst of all is that using more epochs results on bigger time.  This happens because inference is efficient when we do a a big number of predictions on each run.</p> <p>I will deal with it on a future iteration.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#conclusion","title":"Conclusion","text":"<p>I have made a first implementation of search and learn, it is not very efficient but I have been able to verify that for a fixed prediction budget of 512 predictions, the method is able to improve 5% the pass@n rate over the baseline of 23.3% on the ARC-AGI-1 evaluation dataset.</p> <p>It is not a huge improvement, but hopefully is a good start point that I could improve on future iterations with a more efficient implementation.</p>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#next-steps","title":"Next steps","text":"<ul> <li>After seeing that throughput at inference increases with the number of predictions, I might have to   use VLLM as a server and make async calls in parallel.</li> </ul>"},{"location":"modeling/Iteration_23_ttt_BARC_v2/#todo","title":"TODO","text":"<ul> <li> Try unsloth for both training and inference</li> <li> Compare unsloth speed against trl and VLLM</li> <li> Create a smaller version of the dataset for faster experimentation</li> <li> Move code to script<ul> <li> Move current notebook to script</li> <li> Refactor</li> <li> Move code to library modules</li> <li> Save results to disk</li> <li> Log to wandb.<ul> <li> Tables, runtime...</li> <li> Task evolution</li> <li> Summary</li> <li> The goal is to be able to compare runs very easily with wandb. And also ideally to diagnose hyperparameter problems.</li> </ul> </li> <li> All parameters should be on the configuration</li> <li> Log search vs learn time</li> <li> Only save original outputs for test (that's what I need for the submission)</li> </ul> </li> <li> Try flashinfer and check if there is any speedup: https://github.com/flashinfer-ai/flashinfer<ul> <li><code>pip install flashinfer-python</code></li> <li>FileNotFoundError: [Errno 2] No such file or directory: 'nvcc'</li> <li>Tried with prebuilt wheel but freezes when starting inference. <code>pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.2.5/flashinfer_python-0.2.5+cu124torch2.6-cp38-abi3-linux_x86_64.whl#sha256=43d767b912c0c43a04be99595e0123eab9385fc72530a2874b5fb08e3145c0be Collecting flashinfer-python==0.2.5+cu124torch2.6</code></li> <li>Should revisit on a future iteration because it could give faster inference for free</li> </ul> </li> <li> Check the lora modules parameters, I'm using them without understanding</li> <li> Learning rate sweep. Using a small learning rate should be equivalent to just doing search. Using a too big lr should result in degraded metrics. There should be a sweet spot.</li> <li> Code execution is not robust.<ul> <li> Sometimes execution hangs and no exception is thrown     - [x] https://wandb.ai/guillermobarbadillo/2025-09-07-search-and-learn/runs/0iswo84s/logs     - [x] https://wandb.ai/guillermobarbadillo/2025-09-11-search-and-learn/runs/xldcleic/logs</li> <li> Sometimes raises exception<ul> <li> https://wandb.ai/guillermobarbadillo/2025-09-07-search-and-learn/runs/kd4qttau/logs</li> </ul> </li> <li> Made many changes to improve robustness: https://chatgpt.com/share/68c3ae7d-9cb4-8012-9950-9dd93606283e</li> </ul> </li> <li> Investigate the time lost on training startup</li> <li> Experiment on Kaggle<ul> <li> Upload the model. https://www.kaggle.com/models/ironbar/barc0llama-3.1-arc-potpourri-induction-8b</li> <li> Upload the code. https://www.kaggle.com/datasets/ironbar/arc25-source-code</li> <li> Create a notebook with the requirements. https://www.kaggle.com/code/ironbar/search-and-learn</li> <li> Split the data in 4, each for a GPU</li> <li> Collect the results to make a submission</li> <li> How efficient is the current implementation?</li> <li> Getting OOM cuda errors when training on the longer tasks</li> <li> Create python module to do the submission, with tests</li> <li> Need a way to evaluate the submission once it's created</li> <li> Disable internet</li> <li> Implement dry run</li> <li> Can I leave logs and keep making submissions?</li> <li> Speed seems to be lower than 3090, <code>1.26s/it, est. speed input: 2163.11 toks/s, output: 343.46 toks/s</code><ul> <li> Speed test on 3090.<ul> <li> 960 preds,  1.23it/s, est. speed input: 3389.69 toks/s, output: 547.71 toks/s</li> <li> 960 preds unquantized, 1.42it/s, est. speed input: 3943.22 toks/s, output: 652.53 toks/s</li> <li> 3840 preds, 1.47it/s, est. speed input: 4081.55 toks/s, output: 650.98 toks/s</li> <li> Yes, seems to be around twice as fast</li> </ul> </li> <li> 3x https://technical.city/en/video/GeForce-RTX-3090-vs-L4</li> <li> 2.3x https://chatgpt.com/share/68c1d348-89a4-8012-8135-a58a82bbef4d</li> <li> With current implementation I won't be able to make more than 512 predictions on a submission</li> </ul> </li> </ul> </li> <li> Check implementation of RL and how it alternates between training and inference(trl, GRPO)</li> <li> Analyze clusters results with 128 predictions. <ul> <li> Learning rate</li> <li> efficiency</li> <li> improvements</li> </ul> </li> <li> GPU efficiency and throughtput experiments on Kaggle</li> <li> Run experiments in the cluster with 512 predictions.</li> <li> How to improve efficiency? Maybe on a different iteration.<ul> <li> Do not train on all the cases</li> <li> Remove duplicates</li> <li> Filter cases with lower scores (as I did)</li> <li> How fast is inference compared to training?</li> <li> Train for multiple epochs</li> <li> LoRA parameters</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_24_RL_BARC/","title":"Iteration 24. Using RL to improve BARC induction model","text":""},{"location":"modeling/Iteration_24_RL_BARC/#iteration-24-using-rl-to-improve-barc-induction-model","title":"Iteration 24. Using RL to improve BARC induction model","text":"<p>11-09-2025</p>"},{"location":"modeling/Iteration_24_RL_BARC/#goal","title":"Goal","text":"<p>Can I improve the BARC induction model using reinforcement learning?</p>"},{"location":"modeling/Iteration_24_RL_BARC/#motivation","title":"Motivation","text":"<p>I have read the RL guide from unsloth and they say that 300 samples are enough to see an improvement in the model. Probably I will need much more compute for ARC but I would like to try.</p> <p>The BARC induction model seems to have non-zero probability of solving the ARC-AGI-1 tasks, RL is the way to increase that probability.</p> <p></p> <p>Ideas for the reward function:</p> <ul> <li>+1 if the model generates code</li> <li>+1 if the model generates running code</li> <li>Finally sum the ratio of correct grids. I believe that pixel accuracy is not a good metric, but I could try it also. I have the feeling that ARC is an all or nothing dataset, and pixel accuracy might lead to local optimums instead of leading to the global maximum.</li> </ul> <p>On a first step I could try with a single training task. Then I could move to use all the training tasks. I would measure the improvement on the training and the evaluation dataset. Finally if the technique is helpful, I would move to using the synthetic dataset in a following iteration.</p> <p>An additional motivation is that I have found that I would be able to make 512 predictions at maximum for task on the Kaggle submission. That would solve just 22% of the ARC-AGI-1 evaluation tasks. I need a model with a higher pass rate. RL is the way to get that.</p>"},{"location":"modeling/Iteration_24_RL_BARC/#development","title":"Development","text":""},{"location":"modeling/Iteration_24_RL_BARC/#thoughts","title":"Thoughts","text":"<ul> <li>As far as I know, the best setup is to use some GPUs for inference and others for training. First   experiments on my PC will use one 3090 for inference and another for training.</li> <li>The trickier part of the configuration is that we need reward signal to be able to learn. I will   start by using the training tasks from ARC-AGI-1 that have a mean pass rate of 12%. Source   That is around 1 out of 8 runs. Thus I could be doing 8 or 16 predictions per task and that should   work. On the evaluation set the pass rate falls to ~2%, requiring to do 64 or 128 predictions per   task to have some signal. So let's start with the train set and measure if that translates to   improvements on the model.</li> <li>I need to implement a reward function that executes the code generated by the model</li> </ul>"},{"location":"modeling/Iteration_24_RL_BARC/#tutorials","title":"Tutorials","text":"<ul> <li>Implementing GRPO in TRL</li> </ul> <p>The group size should be chosen based on your computational resources and the complexity of your task. For simple tasks, smaller groups (4-8) may be sufficient, while more complex reasoning tasks might benefit from larger groups (8-16).</p> <p>This is very likely related to the probability of solving the task correctly, or at least to have differences in the reward between the prompts.</p> <ul> <li>Official documentation</li> <li>TRL VLLM server</li> </ul> <p>Examples of reward functions:</p> <ul> <li>https://huggingface.co/learn/llm-course/en/chapter12/6?fw=pt#defining-reward-functions</li> </ul>"},{"location":"modeling/Iteration_24_RL_BARC/#launching-the-server","title":"Launching the server","text":"<pre><code># one gpu\nexport CUDA_VISIBLE_DEVICES=0; trl vllm-serve --max_model_len 12000 --model /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B\n# multiple gpus\nexport CUDA_VISIBLE_DEVICES=0,1; trl vllm-serve --max_model_len 12000 --model /home/gbarbadillo/models/Llama-3.1-ARC-Potpourri-Induction-8B --data-parallel-size 2\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#modify-pad-token-in-the-tokenizer-configuration","title":"Modify pad token in the tokenizer configuration","text":"<p>Simply replace the following line on <code>Llama-3.1-ARC-Potpourri-Induction-8B/tokenizer_config.json</code>:</p> <pre><code>sed -i 's/\"pad_token\": \"&lt;|eot_id|&gt;\"/\"pad_token\": \"&lt;|finetune_right_pad_id|&gt;\"/' file.json\n\n- \"pad_token\": \"&lt;|eot_id|&gt;\",\n+ \"pad_token\": \"&lt;|finetune_right_pad_id|&gt;\",\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#max-sequence-length","title":"Max sequence length","text":"<p>I have studied all the datasets and the longest prompt is always 8635 tokens, corresponding to tasks with 4 train samples with inputs and outputs of 30x30, and a test sample with the same input shape.</p> <p>Thus if I don't want to generate more than 2000 tokens, the max sequence length would be 10635 tokens.</p>"},{"location":"modeling/Iteration_24_RL_BARC/#kaggle-scoring-error","title":"Kaggle scoring error","text":"<ul> <li>Valid submission</li> <li>Invalid submission</li> </ul> <pre><code>python scripts/validate_submission.py --submission-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/sample_submission.json --dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_test_challenges.json\n\npython scripts/validate_submission.py --submission-path /mnt/hdd0/Kaggle/arc25/submissions/evaluation_2025_invalid_submission.json --dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2025/arc-agi_evaluation_challenges.json\n\npython scripts/validate_submission.py --submission-path /mnt/hdd0/Kaggle/arc25/submissions/test_2025_invalid_submission.json --dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2025/arc-agi_test_challenges.json\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#inference-to-measure-the-improvement","title":"Inference to measure the improvement","text":"<pre><code>python scripts/inference_with_BARC.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions2025-09-15-debug-grpo/lr1e-5_small-dataset_80epochs_16gens_continue/training \\\n--lora-path /mnt/hdd0/Kaggle/arc25/trainings/2025-09-15-debug-grpo/lr1e-5_small-dataset_80epochs_16gens_continue/checkpoint-5360 \\\n--n-predictions 128\n\npython scripts/inference_with_BARC.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions2025-09-15-debug-grpo/lr1e-5_small-dataset_80epochs_16gens_continue/evaluation \\\n--lora-path /mnt/hdd0/Kaggle/arc25/trainings/2025-09-15-debug-grpo/lr1e-5_small-dataset_80epochs_16gens_continue/checkpoint-5360 \\\n--n-predictions 128\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#number-of-samples-per-task","title":"Number of samples per task","text":"<p>The task with the maximum number of grids is 12. However there are only around ~20 tasks with more than 8 samples per task.</p>"},{"location":"modeling/Iteration_24_RL_BARC/#reward-design","title":"Reward design","text":"<p>The north start metric is the correct grids, pixel score is use as a tiebreaker. When code is not parsed reward is -1, and code that creates valids gets a reward of 1 vs code that does not.</p> <p>Reward scheme:</p> <pre><code>-1: code not parsed\n 0: code parsed but does not produce valid results\n 1: code produces valid results but accuracy is 0\n 1 + 8*correct_grids + pixel_score: code produces valid results with accuracy, [1, 9]\n</code></pre> <p>Reward is always in range [-1, 10]</p>"},{"location":"modeling/Iteration_24_RL_BARC/#experiments","title":"Experiments","text":""},{"location":"modeling/Iteration_24_RL_BARC/#local","title":"Local","text":""},{"location":"modeling/Iteration_24_RL_BARC/#first-steps","title":"First steps","text":"<pre><code>python scripts/rl_code_finetuning.py --learning-rate 4e-6 --epochs 80 --warmup-ratio 0.01 --gpu-memory-utilization 0.7 --num-generations 24 --lora-r 32 --output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-15-debug-grpo/lr4e-6_80epochs_24gen_32lora_new-reward\n\npython scripts/rl_code_finetuning.py --learning-rate 1e-5 --epochs 80 --warmup-ratio 0.01 --gpu-memory-utilization 0.7 --num-generations 16 --lora-r 32 --output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-15-debug-grpo/lr1e-5_80epochs_16gen_4prompts-per-step_32lora_new-reward --training-prompts-per-step 4\n\npython scripts/rl_code_finetuning.py --learning-rate 4e-5 --epochs 80 --warmup-ratio 0.01 --gpu-memory-utilization 0.68 --num-generations 16 --lora-r 16 --output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-15-debug-grpo/lr4e-5_80epochs_16gen_4prompts-per-step_16lora_new-reward --training-prompts-per-step 4\n\n# I have updated the script to use a single task per step, maybe I should do a single prompt...\npython scripts/rl_code_finetuning.py --learning-rate 4e-5 --epochs 80 --warmup-ratio 0.01 --gpu-memory-utilization 0.68 --num-generations 16 --lora-r 16 --output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-15-debug-grpo/lr4e-5_80epochs_16gen_4prompts-per-step_16lora_prompt-fix --training-prompts-per-step 4\n\npython scripts/rl_code_finetuning.py --learning-rate 2e-5 --epochs 80 --warmup-ratio 0.01 --gpu-memory-utilization 0.67 --num-generations 16 --lora-r 16 --output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-15-debug-grpo/lr2e-5_80epochs_16gen_4prompts-per-step_16lora_prompt-fix-v2 --training-prompts-per-step 4\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#speed-test","title":"Speed test","text":"<p>Let's compare the speed when using gradient accumulation steps. I believe inference shoudl be faster</p> <pre><code>export EPOCHS=1\nexport NUM_GENERATIONS=8; export ACCUM_STEPS=1;  python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.67 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-27-rl-speed-test/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_RTX3090\n\nexport NUM_GENERATIONS=16; export ACCUM_STEPS=2; python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-27-rl-speed-test/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_RTX3090\n\nexport NUM_GENERATIONS=32; export ACCUM_STEPS=4; python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-27-rl-speed-test/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_RTX3090\n\nexport NUM_GENERATIONS=64; export ACCUM_STEPS=8; python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-27-rl-speed-test/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_RTX3090\n\nexport NUM_GENERATIONS=128; export ACCUM_STEPS=16; python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-27-rl-speed-test/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_RTX3090\n\nexport NUM_GENERATIONS=256; export ACCUM_STEPS=32; python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-27-rl-speed-test/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_RTX3090\n\nexport EPOCHS=1\nexport NUM_GENERATIONS=16; export ACCUM_STEPS=1;  python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.67 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-27-rl-speed-test/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_RTX3090\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#scale-rewards","title":"Scale rewards","text":"<p>Try scale_rewards='batch', https://huggingface.co/docs/trl/main/en/grpo_trainer#trl.GRPOConfig, this migth reduce the frac_std_reward_zero</p> <pre><code>export EPOCHS=1\nexport NUM_GENERATIONS=8; export ACCUM_STEPS=2;  python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-28-rl-scale-rewards/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_baseline\n\nexport EPOCHS=1\nexport NUM_GENERATIONS=8; export ACCUM_STEPS=2;  python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--scale-rewards batch \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-28-rl-scale-rewards/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_batch\n\nexport EPOCHS=1\nexport NUM_GENERATIONS=8; export ACCUM_STEPS=4;  python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--scale-rewards group \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-28-rl-scale-rewards/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_group\n\nexport EPOCHS=1\nexport NUM_GENERATIONS=8; export ACCUM_STEPS=4;  python scripts/rl_code_finetuning.py \\\n--learning-rate 2e-5 --epochs ${EPOCHS} --warmup-ratio 0.01 --gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} --lora-r 16 --gradient-accumulation-steps ${ACCUM_STEPS} \\\n--scale-rewards batch \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-28-rl-scale-rewards/lr2e-5_${EPOCHS}epochs_${NUM_GENERATIONS}gen_${ACCUM_STEPS}accum-steps_16lora_batch\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#cluster","title":"Cluster","text":""},{"location":"modeling/Iteration_24_RL_BARC/#speed-test_1","title":"Speed test","text":"<p>Let's compare the speed when using gradient accumulation steps. I believe inference shoudl be faster</p> <pre><code>export EPOCHS=1\nexport FOLDER=2025-09-27-rl-speed-test\nexport LEARNING_RATE=1e-6\nexport N_CPUS=20\nexport LORA_R=32\nexport NUM_GENERATIONS=8; export ACUM_STEPS=1; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--lora_r ${LORA_R} \\\n--epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--learning-rate ${LEARNING_RATE} \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_H100\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=50G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n\nexport NUM_GENERATIONS=512; export ACUM_STEPS=64; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--lora_r ${LORA_R} \\\n--epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--learning-rate ${LEARNING_RATE} \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_H100\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=50G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#trainings","title":"Trainings","text":"<pre><code>export FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=1e-6\nexport NUM_GENERATIONS=16\nexport PROMPTS_PER_STEP=1\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--gpu_memory_utilization 0.3 \\\n--lora_r ${LORA_R} \\\n--warmup-ratio 0.01 \\\n--num-generations ${NUM_GENERATIONS} \\\n--epochs ${EPOCHS} \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--learning-rate ${LEARNING_RATE} \\\n--gradient-accumulation-steps ${PROMPTS_PER_STEP} \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${PROMPTS_PER_STEP}prompts-per-step_${LORA_R}lora_simplified-reward\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=200G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n237770.0 # OOM when using 128GB of RAM\n237843.0 # CUDA error: an illegal memory access was encountered\n237995.0 # at step 10117 it is collapsing\n\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--epochs ${EPOCHS} \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_simplified-reward\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=90G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n237996.0 # OOM with 54GB of RAM, relaunched with 128GB\n# Training has collapsed after 4k steps, but there were signs after step 2400\n\n# first training with scale rewards\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=64\nexport ACUM_STEPS=8\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--epochs ${EPOCHS} \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_simplified-reward\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=90G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n\n\n# Experiments to avoid training collapse\nmkdir trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-1.2\ncp -r trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward/checkpoint-2400 trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-1.2\nmkdir trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_unmasked-truncated-completions\ncp -r trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward/checkpoint-2400 trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_unmasked-truncated-completions\n\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport REPETITION_PENALTY=1.2\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 238016.0, penalty seems to be too high, collapses with weird text\n\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--epochs ${EPOCHS} \\\n--no-mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_simplified-reward_unmasked-truncated-completions\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 238017.0\n\nexport REPETITION_PENALTY=1.1\nmkdir trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\ncp -r trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward/checkpoint-2400 trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 238138., penalty seems to be too high, collapses with weird text\n\nexport REPETITION_PENALTY=1.05\nmkdir trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\ncp -r trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward/checkpoint-2400 trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 238182., collapses but on a weird way\n\n\nexport REPETITION_PENALTY=1.02\nmkdir trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\ncp -r trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward/checkpoint-2400 trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 239015.0\n\nexport REPETITION_PENALTY=1.01\nmkdir trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\ncp -r trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward/checkpoint-2400 trainings/2025-09-19-rl-first-steps/lr2e-6_epochs100_32gen_4accum-steps_32lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_simplified-reward_repetition-penalty-${REPETITION_PENALTY}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 239013.0\n\n## Start from zero\nexport REPETITION_PENALTY=1.05\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=1e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport EXPERIMENT_NAME=lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n240688.0\n\nexport REPETITION_PENALTY=1.05\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=1e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport EXPERIMENT_NAME=lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_unmasked-truncate\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--no-mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n240689.0\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#training-collapse","title":"Training collapse","text":"<p>https://wandb.ai/guillermobarbadillo/2025-09-19-rl-first-steps/runs/9lvckhn0/logs</p> Click to expand/collapse this section <pre><code>2025-09-25 12:56:23 ```python\n2025-09-25 12:56:23 from common import *\n2025-09-25 12:56:23 \n2025-09-25 12:56:23 import pattern detection as pattern\n2025-09-25 12:56:23 from the input, you will see a vertical sequence of alternating patterns of alternating patterns in a pattern of alternating patterns in a vertical sequence of alternating patterns in a 3x3 pattern. The pattern of a pattern is a pattern of alternating patterns in a 3x3 pattern.\n2025-09-25 12:56:23 \n2025-09-25 12:56:23 ```patterns in a 3x1 vertical pattern. The pattern of a pattern is a pattern of alternating patterns in a vertical sequence of alternating patterns in a 3x1 pattern in a 3x1 pattern in a 3x1 pattern. The pattern of a pattern in a 3x1 pattern in a 3x1 pattern in a 3x1 pattern in a 1 pattern in a 1 pattern in a pattern in a 1 pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a vertical pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a1 pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a vertical pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a vertical pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a vertical pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a vertical pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a vertical pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a vertical pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in a pattern in\n2025-09-25 12:56:23 2025-09-25 14:56:23,927 - arc25.logging - INFO - wrapper - Executed arc_reward in 0.0321 seconds\n</code></pre>   In this case we would be able to parse python code, so it won't get a reward of -1 but a reward of 0.   <p>I'm going to update the reward to don't make distinctions between code not parsed an unvalid output. That might prevent training collapsing. Other option would be to use some penalty over repeated text. And other option would be to use unfinished responses for training that would get reward 0.</p> <p>I have already done a few experiments with the simplified reward and collapse still happens.</p>"},{"location":"modeling/Iteration_24_RL_BARC/#evaluation","title":"Evaluation","text":"<pre><code>python scripts/inference_with_BARC.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions/2025-09-19-rl-first-steps/lr1e-6_epochs100_16gen_1prompts-per-step_32lora/evaluation \\\n--lora-path /mnt/hdd0/MEGA/TEMP/trainings/2025-09-19-rl-first-steps/lr1e-6_epochs100_16gen_1prompts-per-step_32lora/checkpoint-8400 \\\n--n-predictions 240\n</code></pre>"},{"location":"modeling/Iteration_24_RL_BARC/#results","title":"Results","text":""},{"location":"modeling/Iteration_24_RL_BARC/#reward-is-not-improving-on-first-experiments","title":"Reward is not improving on first experiments","text":"<p>https://wandb.ai/guillermobarbadillo/2025-09-14-debug-grpo</p> <p>On a first step I'm training on a single task to see if the reward improves over training.</p> <p></p> <p>Sadly reward doesn't seem to change. Maybe I'm using a too small learning rate. Let's use a constant with warmup schedule and try different learning rates.</p> <p>What could be explaining that the reward is not improving (model not learning):</p> <ul> <li>Learning rate might be too small.</li> <li>Maybe unsloth does not work well when using server model. However I have tried without a server and got OOM error.</li> <li>Data augmentation could be making the problem harder</li> <li>Maybe the reward is not good enough</li> <li>I might have to wait for longer</li> <li>Maybe the model needs more capacity, I could increase the LoRA size.</li> <li>Some parameter might be hurting, such as <code>completion_only_loss=True,</code></li> </ul> <p>TODO: I'm going to use a very high learning rate to try to destroy the policy. However it did not have any effect.</p> <p>Then I have tried using trl without unsloth. Then I have noticed problems in the server, it does not seem to support LoRA. My hypothesis is that unsloth was not trying to modify the weights.</p> <p>TODO: play with the smallest possible task so I can check if it works: 1. Unsloth without server 2. TRL with server</p> <p>Solution: It seems that unsloth does not support the trl server, and wasn't update the server. I have been able to train on one of the smallest task with unsloth and without server and it is clearly learning because if I use a small learning rate the average reward stays flat, but if I use a proper learning rate raises. I can see that with just 20 training steps, because I'm using the same task and no data augmentation for this experiment.</p> <p></p>"},{"location":"modeling/Iteration_24_RL_BARC/#local-experiments-with-70-shortest-training-tasks","title":"Local experiments with ~70 shortest training tasks","text":"<p>https://wandb.ai/guillermobarbadillo/2025-09-15-debug-grpo?nw=nwuserguillermobarbadillo</p> <ul> <li>A learning rate of 2e-5 is too high, 1e-5 seems to work but not sure if it's optimal.</li> <li>Cannot use more than 16 generations per prompt because it gives OOM error</li> <li>Cannot use Lora 32 because it also gives OOM</li> <li>I have been able to train with gradient accumulation, training is slower but seems to be more stable. I had   to repeat the prompt n times for each gradient accumulation step.</li> <li>It seems that 40 epochs might be a good training duration (check graph below). Training for 40 epochs   improves the reward over training just for 10 epochs, but training for longer did not brought better results.</li> </ul> <p></p>"},{"location":"modeling/Iteration_24_RL_BARC/#cluster-experiments-with-the-whole-arc-agi-1-training-set","title":"Cluster experiments with the whole ARC-AGI-1 training set","text":"<p>https://wandb.ai/guillermobarbadillo/2025-09-19-rl-first-steps?nw=nwuserguillermobarbadillo</p> <ul> <li>After more than 30 hours of training I don't see a clear improvement in the reward.</li> <li>Let's do experiments with a single prompt per step, increase LoRA capacity to 32, 32 generations per prompt and try also decreasing the learning rate.</li> <li>Jobs that do 32 generations per prompt have huge spikes in RAM use, more than 200GB that result on condor stopping the jobs. (235635., 235638.).   Normal jobs only seem to require 7GB. But when trying to lower the RAM requirements I got problems, so I had to use 128GB at minimum.</li> <li>I don't know why, but some trainings collapse and suddenly start doing long predictions. I'm working to understand and solve the problem.   Otherwise I cannot train for long with the whole training set.</li> </ul>"},{"location":"modeling/Iteration_24_RL_BARC/#evaluation-of-first-model-trained-with-rl-on-all-training-tasks","title":"Evaluation of first model trained with RL on all training tasks","text":"dataset experiment n_preds valid code valid outputs unique outputs train_pixel_score train_correct_grids train_pass_rate train_is_correct test_pixel_score test_correct_grids test_pass_rate test_is_correct is_correct evaluation baseline 480 100.00% 70.87% 43.82% 41.30% 2.07% 1.34% 22.50% 40.21% 1.71% 1.68% 28.50% 22.25% evaluation RL all tasks 480 96.17% 81.92% 35.98% 56.24% 6.76% 4.60% 27.75% 54.93% 5.90% 5.82% 35.25% 27.00% <p>The model trained with RL is best in all metrics except valid code and unique outputs. It improves the pass@n rate from 22.25% to 27%.</p> <p>It is possible that by improving the reward and training for longer results could improve even more. This model was trained for 8400 steps, so that is around 20 epochs for 400 training tasks.</p> dataset experiment n_preds valid code valid outputs unique outputs train_pixel_score train_correct_grids train_pass_rate train_is_correct test_pixel_score test_correct_grids test_pass_rate test_is_correct is_correct training baseline 240 100.0% 76.4% 41.1% 48.1% 11.8% 10.1% 61.8% 47.1% 11.1% 11.0% 66.8% 61.5% training RL all tasks 240 97.9% 89.9% 29.4% 67.9% 28.8% 24.9% 64.8% 66.9% 27.4% 27.1% 70.5% 64.3% <p>On the training set there are improvements, but they are small. Either I need to train for longer or some tasks are not solvable.</p>"},{"location":"modeling/Iteration_24_RL_BARC/#speed-tests","title":"Speed tests","text":"<p>https://wandb.ai/guillermobarbadillo/2025-09-27-rl-speed-test</p>"},{"location":"modeling/Iteration_24_RL_BARC/#3090-training-on-67-smallest-training-tasks","title":"3090 (training on 67 smallest training tasks)","text":"<ul> <li>Using a bigger number of generations per step results on more efficient generation until 128, then it plateaus.</li> <li>In the other side a bigger number of generations results on slower training speed</li> <li>For the 3090 32 generations per step might be the sweet spot</li> <li>I have verified that the number of gradient accumulation steps does not affect too much to the metrics. We should try to use a batch size as big as possible but the effect is not big.</li> </ul>"},{"location":"modeling/Iteration_24_RL_BARC/#h100-traininig-on-all-training-tasks","title":"H100 (traininig on all training tasks)","text":"<p>Similar conclusions for H100.</p>"},{"location":"modeling/Iteration_24_RL_BARC/#conclusion","title":"Conclusion","text":"<p>After training with RL the model solved 27% of the ARC-AGI-1 evaluation tasks compared to the 22.25% baseline (for 480 predictions). This shows that RL is able to make the model better at solving ARC tasks. I need to solve the issue of training collapse to be able to train for longer on more data.</p>"},{"location":"modeling/Iteration_24_RL_BARC/#next-steps","title":"Next steps","text":"<ul> <li>If RL is proven to work, next step is scale the training by using more GPUs and more data (BARC)</li> </ul>"},{"location":"modeling/Iteration_24_RL_BARC/#todo","title":"TODO","text":"<ul> <li> How to pass the responses to the reward function? It seems that the data should be provided as a field in the dictionary</li> <li> If I understand correctly each step a single problem is seen</li> <li> Should I modify the tokenizer directly in the model to avoid problems?</li> <li> 24GB of VRAM is not enough to do RL training with the sequence lengths of ARC -&gt; Need to go to H100<ul> <li> Update environment</li> <li> Update tokenizer conf</li> <li> Create RL training script<ul> <li> ~~Generator for the prompts~~ Not necessary, dataset is small</li> <li> Add verbose option to code evaluation</li> <li>[xz] More smooth reward, combine test and train</li> </ul> </li> </ul> </li> <li> What is the max prompt length for all the datasets available? -&gt; 8635</li> <li> GPU usage is not efficient with server mode: https://huggingface.co/blog/vllm-colocate</li> <li> Kaggle scoring error<ul> <li> Create validate submission script</li> <li> Add tests for create submission</li> <li> Check problems on existing submission</li> <li> Change priority to dataset (there might be missing tasks)</li> <li> Maybe I'm using numpy instead of float?</li> <li> https://www.kaggle.com/code/ironbar/validate-arc25-submission?scriptVersionId=262170501</li> <li> First sucessful submission. https://www.kaggle.com/code/ironbar/search-and-learn?scriptVersionId=262195260</li> <li> I suspect the problem is there were missing tasks. Can I simulate that?<ul> <li> -&gt; Lower gpu_memory and see what happens.</li> <li> Better adjustment of model hyperparameters</li> </ul> </li> </ul> </li> <li> Train with the new reward and verify that is able to learn</li> <li> Not sure if completion_only_loss is working, check what happens with collator on new trl versions<ul> <li> https://github.com/huggingface/trl/issues/3827</li> <li> As far as I can see it was removed on version 0.20 and we should use <code>completion_only_loss=True,</code> on <code>SFTConfig</code></li> <li> https://www.kaggle.com/code/ironbar/completion-only-loss-investigation</li> <li> Verify that it works by comparing these two runs:<ul> <li> https://wandb.ai/guillermobarbadillo/2025-09-18-search-and-learn/runs/27f8199j</li> <li> https://wandb.ai/guillermobarbadillo/2025-09-18-search-and-learn/runs/vacaozda?nw=nwuserguillermobarbadillo</li> <li> Verified that there is a clear difference in loss values</li> <li> Should I also use it on RL, if it works I guess so. IT DOES NOT WORK WITH RL, requires input_ids in the dataset</li> </ul> </li> </ul> </li> <li> It seems that on my current implementation using more than 1 prompt per step does not work. Maybe   I have missunderstood the implementation and I have to use the same prompt for the step.<ul> <li>It seems that if a single prompt is used on each step the reward improves: https://wandb.ai/guillermobarbadillo/2025-09-15-debug-grpo/runs/f7r56ln8  </li> </ul> </li> <li> Evaluate: /mnt/scratch/users/gbarbadillo/arc25/trainings/2025-09-19-rl-first-steps/lr1e-6_epochs100_16gen_1prompts-per-step_32lora/checkpoint-8400</li> <li> Should I use some repetition penalty when training?<ul> <li> After simplifying the reward the training still collapses: 237995.0</li> <li> Does using a bigger group size helps to prevent collapse? No</li> <li> Launched experiment with repetition penalty 1.2</li> <li> Launched experiment without masking truncated completions</li> </ul> </li> <li> More advanced reward<ul> <li>When all the rewards are equal, the loss is 0. And the model does not learn. However I would like the model to still learn when all the responses are correct. In that case I could break the ties using the length of the response. Use ockham's razor to keep responses as short as possible.</li> <li>However I'm not sure if that makes sense. Wouldn't be better to use a bigger number of predictions so there is one failing one and the model can learn the true goal?</li> </ul> </li> <li> Longer trainings with simplified reward to see if collapse happens</li> <li> Document local experiments</li> <li> There seems to be a problem with the gradient accumulation steps on this experiment: https://wandb.ai/guillermobarbadillo/2025-09-19-rl-first-steps/runs/jle1n3oa/overview</li> <li> Try scale_rewards='batch', https://huggingface.co/docs/trl/main/en/grpo_trainer#trl.GRPOConfig, this migth reduce the frac_std_reward_zero</li> <li> Update reward information with the best one</li> <li> Training experiments<ul> <li> How many epochs does the model need to learn all the tasks?</li> <li> What is the configuration that better uses the hardware. 32 generations per step</li> <li> Best learning rate</li> <li> How much the model improves after training?</li> </ul> </li> <li> MultiGPU training. Doing it on a different iteration</li> <li> Analyze disk space used by trainings</li> </ul>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/","title":"Iteration 25. Debug parallel code execution","text":""},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#iteration-25-debug-parallel-code-execution","title":"Iteration 25. Debug parallel code execution","text":"<p>16-09-2025</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#goal","title":"Goal","text":"<p>Solve the problems of slow parallel code execution in the cluster.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#motivation","title":"Motivation","text":"<p>I have a method that executes code in parallel and works very fast on my computer and on Kaggle, but weirdly it is slow on the cluster. I need to solve this issue to be able to do experiments to validate my approach.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#development","title":"Development","text":""},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#execution-is-slow-on-cluster","title":"Execution is slow on cluster","text":"<p>On my pc it executes very fast <code>400/400 [00:02&lt;00:00, 152.92pred/s]</code> and so it is on Kaggle <code>960/960 [00:03&lt;00:00, 265.18pred/s]</code>.</p> <p>But in the cluster I'm seeing very slow executions:</p> <ul> <li><code>12800/12800 [50:22&lt;00:00,  4.24pred/s]</code> Experiment</li> <li><code>51200/51200 [03:28&lt;00:00, 245.29runs/s]</code> Older experiment with good speed</li> </ul>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#reverting-to-old-code","title":"Reverting to old code","text":"<p>I have tried reverting back to commit <code>1557726a0e184d1a4e0b0490eec44bde7dde304e</code>, from 8 september when I logged fast execution times. However the problem persisted:</p> <ul> <li>4 cpus -&gt; 41.67runs/s</li> <li>8 cpus -&gt; 61.31runs/s</li> <li>20 cpus -&gt; 56.75runs/s</li> <li>64 cpus -&gt; 9.51runs/s</li> <li>128 cpus -&gt; 9.41 runs/s</li> </ul> <p>I have also tried running on other machine (calculon19 instead of calculon21) but did not get better results:</p> <ul> <li>8 -&gt; 74.22runs/s</li> <li>16 -&gt; 86.01runs/s</li> </ul>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#simpler-script","title":"Simpler script","text":"<p>Iterations have been slow because I'm doing inference with the model first. That makes that each execution takes around 30 minutes. I need to create a script that allows me to see results much faster. That way I will run the same script with the same data in the different settings and get more information about the problem faster.</p> <p>I have prepared the script and I cannot understand the problem. Could it be a problem with the environment?</p> <pre><code>python scripts/debug_parallel_execution.py\nLoaded 400 tasks with 8 predictions each.\nExecuting predictions for batch 0 with exec: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3200/3200 [00:09&lt;00:00, 347.91run/s]\nMost common errors:\nNonDeterministicCode    222\nValueError              214\nIndexError              203\nAssertionError          181\nTimeoutException         49\nAttributeError           21\nTypeError                17\nUnboundLocalError         7\nUnsafeCode                5\nKeyError                  4\nSyntaxError               4\nStopIteration             4\nNameError                 4\nZeroDivisionError         2\nRecursionError            1\nName: count, dtype: int64\n      n_preds  valid code  valid outputs  unique outputs  train_pixel_score  train_correct_grids  train_pass_rate  train_is_correct  test_pixel_score  test_correct_grids  test_pass_rate  test_is_correct  is_correct\nMEAN      8.0         1.0       0.706875        0.629062           0.415273             0.022264          0.01375            0.0425          0.404596            0.016719        0.016563             0.06        0.04\n\n\nexport N_CPUS=8; condor_submit train.condor command=\" python /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/debug_parallel_execution.py \\ --dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\ --prediction-path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json\" -append request_cpus=${N_CPUS} -append request_gpus=0\n</code></pre> <p>Notice that I get fast execution on my PC also if I disable memmapping: <code>max_nbytes=None, mmap_mode=None)</code>. The speed seems to be the same, so probably wasn't using previously because arrays are very small.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#doing-the-experiment-with-docker-on-my-machine","title":"Doing the experiment with docker on my machine","text":"<pre><code>docker run -ti -v /mnt/hdd0:/mnt/hdd0 gbarbadillo/cuda-python:python3.10-cuda14.1\ncd /mnt/hdd0/MEGA/AI/22_Kaggle/arc25\npip install tqdm numpy tqdm_joblib joblib jinja2 termcolor pandas pynvml scipy\nexport PYTHONPATH=/mnt/hdd0/MEGA/AI/22_Kaggle/arc25\npython3 scripts/debug_parallel_execution.py\n\nLoaded 400 tasks with 8 predictions each.\nExecuting predictions for batch 0 with exec: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3200/3200 [00:09&lt;00:00, 354.43run/s]\nMost common errors:\nNonDeterministicCode    222\nValueError              214\nIndexError              202\nAssertionError          181\nTimeoutException         49\nAttributeError           21\nTypeError                17\nUnboundLocalError         7\nUnsafeCode                5\nKeyError                  4\nSyntaxError               4\nStopIteration             4\nNameError                 4\nZeroDivisionError         2\nRecursionError            1\nName: count, dtype: int64\n      n_preds  valid code  valid outputs  unique outputs  train_pixel_score  train_correct_grids  train_pass_rate  train_is_correct  test_pixel_score  test_correct_grids  test_pass_rate  test_is_correct  is_correct\nMEAN      8.0         1.0       0.707187        0.629375           0.415056             0.022014          0.01375            0.0425          0.404481            0.016719        0.016563             0.06        0.04\n\n# I can restrict shm size and speed is not affected\ndocker run -ti --shm-size=64M -v /mnt/hdd0:/mnt/hdd0 gbarbadillo/cuda-python:python3.10-cuda14.1\n353.00run/s\n</code></pre> <p>Runs as fast as without docker when using my machine.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#experiments-on-laptop","title":"Experiments on laptop","text":"<pre><code>export PYTHONPATH=/mnt/data/other/code/arc25\npython scripts/debug_parallel_execution.py --dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json --prediction_path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json\n\nLoaded 400 tasks with 8 predictions each.\nExecuting predictions for batch 0 with exec: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3200/3200 [00:12&lt;00:00, 258.22run/s]\nMost common errors:\nModuleNotFoundError     1671\nNonDeterministicCode     204\nValueError               107\nIndexError                77\nAssertionError            39\nTimeoutException          34\nTypeError                 14\nAttributeError            12\nUnboundLocalError          4\nSyntaxError                4\nUnsafeCode                 3\nKeyError                   3\nNameError                  1\nZeroDivisionError          1\nName: count, dtype: int64\n      n_preds  valid code  valid outputs  unique outputs  train_pixel_score  train_correct_grids  train_pass_rate  train_is_correct  test_pixel_score  test_correct_grids  test_pass_rate  test_is_correct  is_correct\nMEAN      8.0         1.0       0.320625        0.285938           0.184007             0.016169         0.010937            0.0475          0.180193            0.013281        0.012812           0.0525      0.0475\n</code></pre> <p>Again, this is very fast.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#experiments-on-cluster-without-condor-or-docker","title":"Experiments on cluster without condor or docker","text":"<pre><code># create environment\npython3 -m venv cached-environments/debug\nsource cached-environments/debug/bin/activate\npip install tqdm numpy tqdm_joblib joblib jinja2 termcolor pandas pynvml scipy\n\n# launch script\nsource cached-environments/debug/bin/activate\nexport PYTHONPATH=/mnt/scratch/users/gbarbadillo/arc25/arc25\npython3 arc25/scripts/debug_parallel_execution.py --dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json --prediction_path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json --n_jobs 20\n\n# calculon01, 12 cores\nLoaded 400 tasks with 8 predictions each.\nExecuting predictions for batch 0 with exec: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3200/3200 [00:36&lt;00:00, 86.68run/s]\nMost common errors:\nNonDeterministicCode    204\nValueError              204\nIndexError              184\nAssertionError          168\nTimeoutException         89\nTypeError                27\nAttributeError           25\nUnboundLocalError         9\nKeyError                  5\nSyntaxError               4\nZeroDivisionError         4\nUnsafeCode                3\nNameError                 3\nStopIteration             2\nName: count, dtype: int64\n      n_preds  valid code  valid outputs  unique outputs  ...  test_correct_grids  test_pass_rate  test_is_correct  is_correct\nMEAN      8.0         1.0       0.709063            0.63  ...            0.019375         0.01875             0.07      0.0\n# not bad, considering that it had other workloads at the same time\n\n## calculon18, 64 cores\nn_jobs=-1, 114.21run/s]\nn_jobs=2, 82.98run/s\nn_jobs=5, 153.94run/s\nn_jobs=10, 171.64run/s\nn_jobs=20, 151.97run/s\nn_jobs=60, 140.57run/s\nn_jobs=-1, 140.47run/s\n# there is like a big startup time that does not happen on my machine\n\n## calculon13, 20 cores\nn_jobs=-1, 222.78run/s\nn_jobs=5, 175.63run/s\nn_jobs=10, 206.38run/s\nn_jobs=20, 238.54run/s\n# there is a weird startup time, and sometimes ending time\n\n## calculon21, 252cores\nLoaded 400 tasks with 8 predictions each.\nExecuting predictions for batch 0 with exec: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3200/3200 [00:36&lt;00:00, 87.61run/s]\n# It might be the problem of the machine. Need to try on different machines with different number of cores\n# Notice that Kaggle machines have 48 cores. https://cloud.google.com/compute/docs/gpus#l4-gpus\n\nn_jobs=-1, 138.33run/s\nn_jobs=5, 162.16run/s\nn_jobs=10, 172.60run/s\nn_jobs=20, 180.29run/s\n</code></pre>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#experiments-on-cluster-with-docker","title":"Experiments on cluster with docker","text":"<pre><code>\nsudo sudo docker run -ti -v /mnt/scratch/users/gbarbadillo/arc25:/mnt/scratch/users/gbarbadillo/arc25 gbarbadillo/cuda-python:python3.10-cuda14.1\ncd /mnt/scratch/users/gbarbadillo/arc25\nsource cached-environments/venv_0e8c9c65f4e428eaa5db41171ac52335/bin/activate\nexport PYTHONPATH=/mnt/scratch/users/gbarbadillo/arc25/arc25\npython3 arc25/scripts/debug_parallel_execution.py --dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json --prediction_path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json --n_jobs 20\n\n## calculon21, 252cores\nExecuting predictions for batch 0 with exec:  13%|\u2588\u2588\u2588\u2588\u258b                                | 401/3200 [04:26&lt;30:57,  1.51run/s]\nn_jobs=20, 1.51run/s\n\n# try increasing shm\nsudo sudo docker run -ti --shm-size=2g -v /mnt/scratch/users/gbarbadillo/arc25:/mnt/scratch/users/gbarbadillo/arc25 gbarbadillo/cuda-python:python3.10-cuda14.1\n\n## calculon21, 252cores\nn_jobs=2, 1.25run/s\nn_jobs=5, 1.48run/s\nn_jobs=20, 1.55run/s\n\n\n# try using /dev/shm\nsudo docker run -ti --ipc=host --shm-size=2g \\\n  -e TMPDIR=/dev/shm -e JOBLIB_TEMP_FOLDER=/dev/shm -e LOKY_TEMP=/dev/shm \\\n  -v /mnt/scratch/users/gbarbadillo/arc25:/mnt/scratch/users/gbarbadillo/arc25 \\\n  gbarbadillo/cuda-python:python3.10-cuda14.1\nn_jobs=20, 1.39run/s\n\n### confirm cpu limits inside the docker\nsudo sudo docker run -ti -v /mnt/scratch/users/gbarbadillo/arc25:/mnt/scratch/users/gbarbadillo/arc25 gbarbadillo/cuda-python:python3.10-cuda14.1\n# cgroup v2 (common on modern kernels)\n$ cat /sys/fs/cgroup/cpu.max 2&gt;/dev/null || true\n$ # cgroup v1 (older)\n$ cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us 2&gt;/dev/null || true\n-1\n$ cat /sys/fs/cgroup/cpu/cpu.cfs_period_us 2&gt;/dev/null || true\n100000\n$ \n$ # Are we being throttled?\n$ cat /sys/fs/cgroup/cpu.stat 2&gt;/dev/null || cat /sys/fs/cgroup/cpu/cpu.stat 2&gt;/dev/null || true\nnr_periods 0\nnr_throttled 0\nthrottled_time 0\n$ \n$ # How many CPUs are we actually allowed to run on?\n$ grep Cpus_allowed_list /proc/self/status\nCpus_allowed_list:  0-255\n$ nproc\n256\n\n## I have also tried setting environment flags without success\nexport OMP_NUM_THREADS=1 OPENBLAS_NUM_THREADS=1 MKL_NUM_THREADS=1 \\\n       NUMEXPR_NUM_THREADS=1 VECLIB_MAXIMUM_THREADS=1 BLIS_NUM_THREADS=1\nexport TMPDIR=/dev/shm JOBLIB_TEMP_FOLDER=/dev/shm LOKY_TEMP=/dev/shm\n</code></pre> <p>This shows that the problem only happens when using docker on the cluster. Docker has access to all the cpus,  we can set a big enough shm size, we can disable memmapping, but the execution is still slow.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#the-problem-seems-to-be-related-to-the-environment","title":"The problem seems to be related to the environment!","text":""},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#experiments-on-cluster-with-different-environments","title":"Experiments on cluster with different environments","text":"<pre><code>sudo sudo docker run -ti -v /mnt/scratch/users/gbarbadillo/arc25:/mnt/scratch/users/gbarbadillo/arc25 gbarbadillo/cuda-python:python3.10-cuda14.1\ncd /mnt/scratch/users/gbarbadillo/arc25\nexport PYTHONPATH=/mnt/scratch/users/gbarbadillo/arc25/arc25\npip install tqdm numpy tqdm_joblib joblib jinja2 termcolor pandas pynvml scipy\n\npython3 arc25/scripts/debug_parallel_execution.py --dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json --prediction_path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json --n_jobs 20\n# 307.04run/s\n\nExecuting predictions for batch 0 with exec: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3200/3200 [00:10&lt;00:00, 307.04run/s]\nMost common errors:\nValueError              211\nNonDeterministicCode    204\nIndexError              186\nAssertionError          172\nTimeoutException         42\nTypeError                28\nAttributeError           25\nUnboundLocalError         9\nKeyError                  5\nSyntaxError               4\nZeroDivisionError         4\nUnsafeCode                3\nNameError                 3\nStopIteration             2\nName: count, dtype: int64\n      n_preds  valid code  valid outputs  unique outputs  ...  test_correct_grids  test_pass_rate  test_is_correct  is_correct\nMEAN      8.0         1.0       0.719375         0.63875  ...            0.020313        0.019688           0.0725      0.0575\n\n# however if I activate the environment\nsource cached-environments/venv_0e8c9c65f4e428eaa5db41171ac52335/bin/activate\npython3 arc25/scripts/debug_parallel_execution.py --dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json --prediction_path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json --n_jobs 20\n# 1.11s/run\n\n# create a new environment\ndeactivate\npython3 -m venv cached-environments/debug-2\nsource cached-environments/debug-2/bin/activate\npip install tqdm numpy tqdm_joblib joblib jinja2 termcolor pandas pynvml scipy\npython3 arc25/scripts/debug_parallel_execution.py --dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json --prediction_path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json --n_jobs 20\n# 127.53run/s\n</code></pre> <p>These results show that there is something wrong with <code>venv_0e8c9c65f4e428eaa5db41171ac52335</code> that makes execution very slow. Notice that the filesystem doesn't seem to be the problem because we have created a new environment on the same folder and it is much faster.</p> <p>One weird thing is that they all have the same versions of the python libraries:</p> <pre><code>python3 - &lt;&lt;'PY'\nimport joblib, numpy, scipy, pandas\n\nprint(\"joblib:\", joblib.__version__)\nprint(\"numpy:\", numpy.__version__)\nprint(\"scipy:\", scipy.__version__)\nprint(\"pandas:\", pandas.__version__)\nfrom joblib.externals import loky\nprint(\"loky version:\", getattr(loky, \"__version__\", \"vendored\"))\nPY\n\n\njoblib: 1.5.2\nnumpy: 2.2.6\nscipy: 1.15.3\npandas: 2.3.2\nloky version: 3.5.6\n</code></pre> <p>Other tests suggested by GPT5 also show the same outputs:</p> <pre><code>python3 - &lt;&lt;'PY'\nimport os, tempfile, glob, time, joblib, numpy as np\nprint(\"which python:\", os.popen(\"which python\").read().strip())\nprint(\"TMPDIR:\", os.getenv(\"TMPDIR\"))\nprint(\"JOBLIB_TEMP_FOLDER:\", os.getenv(\"JOBLIB_TEMP_FOLDER\"))\nprint(\"LOKY_TEMP:\", os.getenv(\"LOKY_TEMP\"))\nprint(\"loky dirs now:\", glob.glob(\"/dev/shm/loky-*\")[:3])\n\nfrom joblib import Parallel, delayed\ndef f(x): return x*x\n\nstart=time.time()\nParallel(n_jobs=20, backend=\"loky\", batch_size=\"auto\")(delayed(f)(i) for i in range(20000))\nprint(\"Parallel microbench elapsed:\", round(time.time()-start,3), \"s\")\nPY\n\nwhich python: \nTMPDIR: None\nJOBLIB_TEMP_FOLDER: None\nLOKY_TEMP: None\nloky dirs now: []\nParallel microbench elapsed: 2.437 s\n\nwhich python: /mnt/scratch/users/gbarbadillo/arc25/cached-environments/debug-2/bin/python\nTMPDIR: None\nJOBLIB_TEMP_FOLDER: None\nLOKY_TEMP: None\nloky dirs now: []\nParallel microbench elapsed: 4.954 s\n\nwhich python: /mnt/scratch/users/gbarbadillo/arc25/cached-environments/venv_0e8c9c65f4e428eaa5db41171ac52335/bin/python\nTMPDIR: None\nJOBLIB_TEMP_FOLDER: None\nLOKY_TEMP: None\nloky dirs now: []\nParallel microbench elapsed: 80.421 s\n</code></pre> <pre><code>python3 - &lt;&lt;'PY'\nimport os, multiprocessing as mp, joblib\nprint(\"python:\", os.popen(\"which python\").read().strip())\nprint(\"mp start method:\", mp.get_start_method(allow_none=True))\n# joblib/loky usually uses 'loky' context internally, but this reveals if something forced 'spawn'\nimport joblib.externals.loky.backend.context as lctx\nprint(\"loky default context:\", lctx.get_context().get_start_method())\nPY\n\nThey are all equal:\n\npython: \nmp start method: None\nloky default context: loky\n\npython: /mnt/scratch/users/gbarbadillo/arc25/cached-environments/debug-2/bin/python\nmp start method: None\nloky default context: loky\n\npython: /mnt/scratch/users/gbarbadillo/arc25/cached-environments/venv_0e8c9c65f4e428eaa5db41171ac52335/bin/python\nmp start method: None\nloky default context: loky\n</code></pre> <p>However one weird thing is that launching the python terminal is much slower in the slow environment.</p> <pre><code>time python -c \"pass\"\n# The slow environment takes 4s to start, the fast environment 0.5s\n</code></pre> <p>Let's try to find the root of the problem</p> <pre><code>sudo sudo docker run -ti -v /mnt/scratch/users/gbarbadillo/arc25:/mnt/scratch/users/gbarbadillo/arc25 gbarbadillo/cuda-python:python3.10-cuda14.1\ncd /mnt/scratch/users/gbarbadillo/arc25\nexport PYTHONPATH=/mnt/scratch/users/gbarbadillo/arc25/arc25\nsource cached-environments/venv_0e8c9c65f4e428eaa5db41171ac52335/bin/activate\nsource cached-environments/debug-2/bin/activate\n\n</code></pre> <p>What if I create a big environment? Could the problem be size-related?</p> <pre><code>python3 -m venv cached-environments/debug-big\nsource cached-environments/debug-big/bin/activate\npip install -r arc25/requirements.txt\nexport PYTHONPATH=/mnt/scratch/users/gbarbadillo/arc25/arc25\npython3 arc25/scripts/debug_parallel_execution.py --dataset_path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json --prediction_path /mnt/scratch/users/gbarbadillo/arc25/predictions/2025-08-28-base-model/evaluation/8preds_2025_09_02_05_36_40_predictions.json --n_jobs 20\n# ~1 run/s\n</code></pre>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#recreate-environment-at-home-pc","title":"Recreate environment at home PC","text":"<p>Let's see if recreating the environment at home results on slow execution.</p> <pre><code>docker run -ti -v /mnt/hdd0:/mnt/hdd0 gbarbadillo/cuda-python:python3.10-cuda14.1\npython3 -m venv /mnt/hdd0/TEMP/cached-environment-from-requirements\nsource /mnt/hdd0/TEMP/cached-environment-from-requirements/bin/activate\ncd /mnt/hdd0/MEGA/AI/22_Kaggle/arc25\npip install -r requirements.txt\nexport PYTHONPATH=/mnt/hdd0/MEGA/AI/22_Kaggle/arc25\npython3 scripts/debug_parallel_execution.py\n#267-350 run/s\n\npython3 -m venv /mnt/hdd0/TEMP/cached-environment-simple\nsource /mnt/hdd0/TEMP/cached-environment-simple/bin/activate\npip install tqdm numpy tqdm_joblib joblib jinja2 termcolor pandas nvidia-ml-py scipy\nexport PYTHONPATH=/mnt/hdd0/MEGA/AI/22_Kaggle/arc25\npython3 scripts/debug_parallel_execution.py\n# 331-350 run/s\n</code></pre> <p>On my PC I can't replicate the problem, at least with the latest version of the requirements execution time is fast in both cases.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#simplying-the-code-to-diagnose-the-problem","title":"Simplying the code to diagnose the problem","text":"<p>This could be a minimal code snippet to reproduce the problem.</p> <pre><code>python3 - &lt;&lt;'PY'\nimport time\nimport sys\nfrom tqdm_joblib import tqdm_joblib\nfrom joblib import Parallel, delayed\nprint('Starting parallel microbench...')\ndef f(x): return x*x\nstart = time.time()\nn = 20000\nwith tqdm_joblib(total=n):\n    Parallel(n_jobs=20, backend=\"loky\", batch_size=\"auto\")(delayed(f)(i) for i in range(n))\nexecution_time = time.time() - start\nprint(f\"Parallel microbench elapsed {round(execution_time,3)}s for python path: {sys.executable}\")\nPY\ntime python -c \"pass\"\n</code></pre> <p>The following lines are useful to run the tests in different environments</p> <pre><code>sudo sudo docker run -ti -v /mnt/scratch/users/gbarbadillo/arc25:/mnt/scratch/users/gbarbadillo/arc25 gbarbadillo/cuda-python:python3.10-cuda14.1\nsource /mnt/scratch/users/gbarbadillo/arc25/cached-environments/venv_0e8c9c65f4e428eaa5db41171ac52335/bin/activate\nsource /mnt/scratch/users/gbarbadillo/arc25/cached-environments/debug/bin/activate\nsource /mnt/scratch/users/gbarbadillo/arc25/cached-environments/debug-2/bin/activate\nsource /mnt/scratch/users/gbarbadillo/arc25/cached-environments/debug-big/bin/activate\n</code></pre> <p>The more the workers the higher the benchmark time. Maybe one work around would be to adjust the number of workers to the number of predictions. For example use a single worker when the number of tasks is small.</p> <pre><code># calculon21\nnjobs, benchmark time\n1, 0.105\n2, 15.763\n4, 21.011\n8, 33,5\n20, 80.97\n</code></pre> <p>Let's rewrite the code.</p> <pre><code>python3 - &lt;&lt;'PY'\nimport time\nimport sys\nfrom tqdm_joblib import tqdm_joblib\nfrom joblib import Parallel, delayed\nstart = time.time()\nparallel = Parallel(n_jobs=20, backend=\"loky\", batch_size=\"auto\")\nexecution_time = time.time() - start\nprint(f\"Creating the parallel object took {round(execution_time,3)}s\")\nprint('Starting parallel microbench...')\ndef f(x): return x*x\nstart = time.time()\nn = 20000\nwith tqdm_joblib(total=n):\n    parallel(delayed(f)(i) for i in range(n))\nexecution_time = time.time() - start\nprint(f\"Parallel microbench elapsed {round(execution_time,3)}s for python path: {sys.executable}\")\nstart = time.time()\nn = 20000\nwith tqdm_joblib(total=n):\n    parallel(delayed(f)(i) for i in range(n))\nexecution_time = time.time() - start\nprint(f\"Parallel microbench second round elapsed {round(execution_time,3)}s for python path: {sys.executable}\")\nPY\n\nCreating the parallel object took 0.0s\nStarting parallel microbench...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [00:59&lt;00:00, 337.62it/s]\nParallel microbench elapsed 59.242s for python path: /mnt/scratch/users/gbarbadillo/arc25/cached-environments/venv_0e8c9c65f4e428eaa5db41171ac52335/bin/python3\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20000/20000 [00:00&lt;00:00, 37853.86it/s]\nParallel microbench second round elapsed 0.529s for python path: /mnt/scratch/users/gbarbadillo/arc25/cached-environments/ven\n</code></pre> <p>This shows that if we reuse the parallel object the second run is really fast. That is the solution to our problem. And that explains why runs where fast when I was not using batches in the cluster. </p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#trying-to-understand-the-problem","title":"Trying to understand the problem","text":"<p>https://joblib.readthedocs.io/en/latest/developing.html</p> <p>The automatic array memory mapping feature of Parallel does no longer use /dev/shm if it is too small (less than 2 GB). In particular in docker containers /dev/shm is only 64 MB by default which would cause frequent failures when running joblib in Docker containers.</p> <p>https://joblib.readthedocs.io/en/latest/parallel.html</p> <p>Here there is a description of the Parallel method from joblib and all its parameters.</p> <p>This conversation with GPT5 suggests that signal+joblib+loky seems to be the best option.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#explanation-of-the-problem","title":"Explanation of the problem","text":"<p>When we use joblib and loky to parallelize python execution, it creates n workers.  I don't know the reason, but in the cluster when we use a big python environment, the creation of a python environment is slow. It can take 3-5 seconds or even more.</p> <p>Thus if we span 20 workers, it will take 60 seconds (3*20) to create those workers. The more the workers the bigger the startup time. That does not happen on my machine or Kaggle, it is almost instantenous to span workers. It is probably related to using a slow distributed filesystem.</p> <p>On my first implementation I run all the jobs at once, and for tasks that took around 10-20 minutes to execute the startup time was not important. </p> <p>But my second implementation used smaller batches, that could take 10-15 seconds to run. In that case the startup time dominates.</p> <p>What is the solution? I have to reuse the parallel object between different runs. That way I only pay the startup time once (If there are execution errors I might have to regenerate the object, but that's a separate issue). Thus I have to encapsulate the execution function inside an object, that stores the parallel object.</p> <p>Why solving this problem was so difficult? Because there could be a lot of possible causes:</p> <ul> <li>Changes in the environment</li> <li>Changes in the code</li> <li>Changes in the cluster</li> <li>Problems with cluster disk </li> <li>At the beginning iteration was very slow because it was coupled with inference</li> <li>The problem happened only on the cluster, making testing more difficult</li> <li>Different joblib parameters</li> </ul>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#terrible-news-the-problem-is-still-not-solved","title":"Terrible news: The problem is still not solved","text":"<p>These logs from RL fine-tuning show that the problem is not solved. Executing the reward function should be almost instantaneous:</p> <pre><code>2025-09-19 09:14:29,893 - arc25.logging - INFO - wrapper - Executed arc_reward in 98.5315 seconds\n  0%|          | 1/40000 [02:20&lt;1557:46:20, 140.20s/it]2025-09-19 09:15:09,095 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 09:15:29,255 - __main__ - INFO - arc_reward - Completions length: [379, 268, 290, 261, 258, 426, 529, 230, 321, 411, 315, 416, 369, 215, 222, 293, 269, 407, 430, 274, 603, 277, 303, 269, 329, 425, 528, 460, 287, 347, 383, 430]\n2025-09-19 09:15:29,256 - __main__ - INFO - arc_reward - Rewards: [1.0171387073347857, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.194082455235095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.3031590413943355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n2025-09-19 09:15:29,257 - arc25.logging - INFO - wrapper - Executed arc_reward in 20.1615 seconds\n  0%|          | 2/40000 [03:27&lt;1079:56:44, 97.20s/it]2025-09-19 09:16:11,448 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 09:16:12,491 - __main__ - INFO - arc_reward - Completions length: [321, 317, 292, 372, 312, 309, 265, 290, 310, 363, 289, 324, 333, 248, 359, 223, 334, 393, 305, 307, 356, 451, 288, 308, 350, 284, 363, 318, 318, 269, 401, 313]\n2025-09-19 09:16:12,492 - __main__ - INFO - arc_reward - Rewards: [0.0, 1.540509259259259, 1.2694444444444444, 1.5388888888888888, 0.0, 10.0, 1.25, 1.25, 1.2694444444444444, 1.5447089947089947, 1.5849074074074072, 1.3285069444444444, 0.0, 1.413287037037037, 0.0, 4.4944444444444445, 1.6450396825396827, 1.2327548912075497, 1.4410404410404412, 0.0, 1.413287037037037, 0.0, 0.0, 1.8173611111111114, 1.381875, 0.0, 1.5, 1.4372685185185186, 1.0869675925925926, 1.1830555555555555, 1.5, 1.8173611111111114]\n2025-09-19 09:16:12,493 - arc25.logging - INFO - wrapper - Executed arc_reward in 1.0444 seconds\n  0%|          | 3/40000 [03:48&lt;694:53:10, 62.54s/it]2025-09-19 09:16:40,261 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 09:17:09,099 - __main__ - INFO - arc_reward - Completions length: [459, 472, 536, 394, 502, 450, 371, 762, 368, 380, 525, 482, 339, 439, 379, 353, 415, 528, 317, 469, 541, 425, 326, 488, 424, 447, 421, 443, 391, 419, 422, 482]\n2025-09-19 09:17:09,101 - __main__ - INFO - arc_reward - Rewards: [1.6681249999999999, 0.0, 0.0, 1.65875, 0.0, 0.0, 0.0, 0.0, 0.0, 1.9325, 0.0, 0.0, 1.92, 0.0, 0.0, 1.758125, 1.7893750000000002, 0.0, 1.9325, 0.0, 1.788125, 0.0, 1.930625, 0.0, 0.0, 1.67625, 0.0, 0.0, 0.0, 1.87625, 0.0, 0.0]\n2025-09-19 09:17:09,103 - arc25.logging - INFO - wrapper - Executed arc_reward in 28.8413 seconds\n  0%|          | 4/40000 [05:04&lt;752:27:40, 67.73s/it]2025-09-19 09:17:47,714 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 09:18:36,878 - __main__ - INFO - arc_reward - Completions length: [220, 353, 217, 287, 244, 333, 245, 221, 229, 227, 283, 226, 221, 220, 222, 260, 222, 196, 237, 311, 230, 231, 259, 325, 409, 210, 257, 268, 397, 224, 304, 278]\n2025-09-19 09:18:36,879 - __main__ - INFO - arc_reward - Rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 1.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.79, 0.0]\n2025-09-19 09:18:36,881 - arc25.logging - INFO - wrapper - Executed arc_reward in 49.1656 seconds\n</code></pre> <p>However the solution has come by copying the python environment to the local memory:</p> <pre><code>2025-09-19 16:41:05,047 - arc25.logging - INFO - wrapper - Executed arc_reward in 1.3339 seconds\n  0%|          | 1/40000 [01:30&lt;1007:33:16, 90.68s/it]2025-09-19 16:42:34,549 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 16:42:35,040 - __main__ - INFO - arc_reward - Completions length: [426, 248, 479, 467, 441, 497, 526, 464, 434, 378, 426, 478, 326, 128, 491, 341]\n2025-09-19 16:42:35,041 - __main__ - INFO - arc_reward - Rewards: [0.0, 1.2416666666666667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n2025-09-19 16:42:35,041 - arc25.logging - INFO - wrapper - Executed arc_reward in 0.4920 seconds\n  0%|          | 2/40000 [02:18&lt;726:45:39, 65.41s/it]2025-09-19 16:43:23,442 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 16:43:23,650 - __main__ - INFO - arc_reward - Completions length: [515, 438, 525, 485, 294, 231, 429, 261, 434, 501, 403, 270, 355, 452, 429, 428]\n2025-09-19 16:43:23,651 - __main__ - INFO - arc_reward - Rewards: [1.4316666666666666, 0.0, -1.0, 0.0, 0.0, 1.6901851851851852, 0.0, 1.4316666666666666, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 1.2349999999999999]\n2025-09-19 16:43:23,652 - arc25.logging - INFO - wrapper - Executed arc_reward in 0.2098 seconds\n  0%|          | 3/40000 [03:05&lt;632:14:33, 56.91s/it]2025-09-19 16:43:54,525 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 16:43:54,917 - __main__ - INFO - arc_reward - Completions length: [272, 253, 250, 220, 262, 267, 237, 294, 272, 224, 262, 248, 236, 257, 232, 224]\n2025-09-19 16:43:54,918 - __main__ - INFO - arc_reward - Rewards: [3.9299999999999997, 1.8, 3.9275, 3.9299999999999997, 1.9075, 1.77, 1.9075, 5.945, 1.8, 3.9299999999999997, 1.8425, 1.8399999999999999, 1.8900000000000001, 3.8875, 1.9075, 1.8]\n2025-09-19 16:43:54,919 - arc25.logging - INFO - wrapper - Executed arc_reward in 0.3941 seconds\n  0%|          | 4/40000 [03:38&lt;527:49:57, 47.51s/it]2025-09-19 16:44:42,267 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 16:44:42,485 - __main__ - INFO - arc_reward - Completions length: [404, 377, 280, 465, 268, 499, 458, 393, 497, 484, 325, 413, 375, 357, 433, 414]\n2025-09-19 16:44:42,486 - __main__ - INFO - arc_reward - Rewards: [1.6831275720164611, 0.0, 1.954732510288066, 0.0, 1.954732510288066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n2025-09-19 16:44:42,487 - arc25.logging - INFO - wrapper - Executed arc_reward in 0.2192 seconds\n  0%|          | 5/40000 [04:24&lt;523:05:03, 47.08s/it]2025-09-19 16:45:49,305 - arc25.logging - INFO - wrapper - Executing arc_reward...\n2025-09-19 16:45:49,373 - __main__ - INFO - arc_reward - Completions length: [884, 646, 704, 758, 655, 645, 732, 349, 599, 371, 729, 604, 454, 567, 494, 689]\n2025-09-19 16:45:49,374 - __main__ - INFO - arc_reward - Rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n2025-09-19 16:45:49,375 - arc25.logging - INFO - wrapper - Executed arc_reward in 0.0698 seconds\n</code></pre> <p>Now code execution is blazingly fast, and runtimes are shorter despite being run on A6000 compared to H100 from the previous snippet.</p>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#conclusion","title":"Conclusion","text":"<p>Execution of code is fast again in the cluster after a lot of pain. I did two things to solve the problem:</p> <ol> <li>Create a code runner object that stores the Parallel object (and avoids the penalty of creating it over and over)</li> <li>Move the python environment to local memory</li> </ol>"},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_25_debug_parallel_code_execution/#todo","title":"TODO","text":"<ul> <li> Can I simplify the problem so I can easily debug on the different environments?</li> <li> Maybe it could be as simple as changing the method that parallelizes the work, no because I use signal for timeouts.</li> <li> Experiments I would like to do:<ul> <li> Try on laptop</li> <li> Try on Docker</li> <li> Try on a cluster machine without using condor</li> <li> Try on a cluster machine with docker but without condor</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_26_more_compute/","title":"Iteration 26. Acquire more compute","text":""},{"location":"modeling/Iteration_26_more_compute/#iteration-26-acquire-more-compute","title":"Iteration 26. Acquire more compute","text":"<p>23-09-2025</p>"},{"location":"modeling/Iteration_26_more_compute/#goal","title":"Goal","text":"<p>Acquire more compute to be able to do experiments before the end of the competition</p>"},{"location":"modeling/Iteration_26_more_compute/#motivation","title":"Motivation","text":"<p>Currently we have 13xA6000 GPUs and 2xH100 GPUs in Veridas' cluster. However we are going to loose half of the A6000 due to problems with the machine so competition for the hardware will be fierce.</p> <p>It's time to use the compute offered by ARC prize partners to be able to do my experiments.</p>"},{"location":"modeling/Iteration_26_more_compute/#development","title":"Development","text":"<p>https://arcprize.org/partners</p> <ul> <li> Strong Compute. \\(5k-\\)50k. </li> <li> Lambda. $1K. Applied and received 1k in compute credits.</li> <li> Google. $?. Applied, waiting for approval</li> <li> Hyperbolic. $1K. . Applied, waiting for approval</li> <li> Modal. $500. Already applied, waiting for approval</li> <li>RunPod. $100. Decided not to apply.</li> </ul>"},{"location":"modeling/Iteration_26_more_compute/#strong-compute","title":"Strong Compute","text":"<p>I did the introductory call on 29/09/2025. My initial intention is to use machines with 3090 to run experiments with search and learn and find the best possible configuration.</p> <p>https://cp.strongcompute.ai/</p> <p>Notes from meeting with Adam:</p> <ul> <li>Documentation<ul> <li>Burst is for AI Model training. I can see many burst experiments from last year's ARC edition.</li> </ul> </li> <li>Datasets. I can use datasets to store models or data. They can be created from S3 objects or Huggingface. They say it should be faster than using Huggingface directly. At least I don't have to download the data to the temporal folder.</li> <li>Shapes. I can see the available machine types.</li> <li>I need wiregard to connect to the Sydney cluster</li> <li>They can help with multi-node trainings</li> </ul>"},{"location":"modeling/Iteration_26_more_compute/#logbook","title":"Logbook","text":"<ul> <li>I have created a new project for the experiments</li> <li>I have started a container on <code>Sydney Strong Compute Cluster</code> with 0 GPUs.</li> <li>I have added wireguard and configured it: <code>sudo wg-quick up wg0</code> to connect and <code>sudo wg-quick down wg0</code> to disconnect.</li> <li>I'm following the Hello world guide.</li> <li>Documentation is very complete. It might take a while to get used and started but once I launch the first run I believe it will be very fast to launch multiple experiments.</li> <li>References from previous arc edition:<ul> <li>https://github.com/ironbar/arc24/tree/main/scripts/strong_compute</li> <li>https://ironbar.github.io/arc24/modeling/Iteration_44_learn_to_use_strong_compute/</li> </ul> </li> <li>I was losing internet connection when using Wireguard, removing the line with <code>DNS =</code> fixed it. ChatGPT</li> <li>Create a key with <code>ssh-keygen</code> and add it to github. Remember to delete it once the challenge is over.</li> <li>Install the requirements, I believe flash-attn requires a machine with GPU to be installed</li> </ul> <pre><code>apt update &amp;&amp; apt install -y python3-dev python3-pip python3-virtualenv git nano vim\ngit config --global user.name \"Guillermo Barbadillo\"\ngit config --global user.email \"guillermobarbadillo@gmail.com\"\ncd ~\ngit clone git@github.com:ironbar/arc25.git\npython3 -m virtualenv ~/arc25_env\nsource ~/arc25_env/bin/activate\npip install -r arc25/requirements.txt\npip install unsloth_zoo==2025.9.6 # I should update the requirements\nMAX_JOBS=2 python -m pip install flash-attn==2.6.3 --no-build-isolation\n\nvim secrets.sh #export WANDB_API_KEY=\nchmod +x secrets.sh\n</code></pre> <ul> <li>Created dataset from huggingface barc0/Llama-3.1-ARC-Potpourri-Induction-8B   I don't know why but the first day it was not available, the second day it was. Seems to take time to generate the dataset.</li> <li>After installing the requirements stopping the machine took more time, probably due to saving the environment</li> <li>Starting a job takes around 10 minutes (probably spend copying the environment)</li> <li>First running job failed after 14 minutes without any error message. Might be related to low disk space, maybe I should change the cache directory for huggingface.</li> </ul>"},{"location":"modeling/Iteration_26_more_compute/#problems-with-unsloth","title":"Problems with unsloth","text":"<p>When launching the first trainings I see problems when importing unsloth</p> <pre><code>source ~/arc25_env/bin/activate\npython -c \"import unsloth\"\n2025-09-30 06:14:27,085 - datasets - INFO - &lt;module&gt; - PyTorch version 2.7.1 available.\n\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\nINFO 09-30 06:14:29 [__init__.py:241] Automatically detected platform cuda.\n\ud83e\udda5 Unsloth Zoo will now patch everything to make training faster!\nTraceback (most recent call last):\n  File \"/root/arc25/scripts/search_and_learn_with_unsloth.py\", line 16, in &lt;module&gt;\n    from unsloth import FastLanguageModel\n  File \"/root/arc25_env/lib/python3.12/site-packages/unsloth/__init__.py\", line 247, in &lt;module&gt;\n    from .models import *\n  File \"/root/arc25_env/lib/python3.12/site-packages/unsloth/models/__init__.py\", line 15, in &lt;module&gt;\n    from .llama     import FastLlamaModel\n  File \"/root/arc25_env/lib/python3.12/site-packages/unsloth/models/llama.py\", line 52, in &lt;module&gt;\n    from .vision import FastBaseModel\n  File \"/root/arc25_env/lib/python3.12/site-packages/unsloth/models/vision.py\", line 87, in &lt;module&gt;\n    from unsloth_zoo.vllm_utils import (\n  File \"/root/arc25_env/lib/python3.12/site-packages/unsloth_zoo/vllm_utils.py\", line 63, in &lt;module&gt;\n    from unsloth.models.vision import VLLM_SUPPORTED_VLM\nImportError: cannot import name 'VLLM_SUPPORTED_VLM' from partially initialized module 'unsloth.models.vision' (most likely due to a circular import) (/root/arc25_env/lib/python3.12/site-packages/unsloth/models/vision.py)\n\n# try to reinstall unsloth, but does not solve the problem\npip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth==2025.9.3 unsloth_zoo\n\n# install python 3.10\napt update\napt install -y software-properties-common\nadd-apt-repository ppa:deadsnakes/ppa -y\napt update\napt install -y python3.10 python3.10-venv python3.10-dev\n/usr/bin/python3.10 -m venv ~/arc25_env310\nsource ~/arc25_env310/bin/activate\npython -V        # should print 3.10.x\npip install -U pip\n# Does not solve the problem either\n# This is the solution\npip install unsloth_zoo==2025.9.6\n\n</code></pre>"},{"location":"modeling/Iteration_26_more_compute/#doubts-and-suggestions","title":"Doubts and suggestions","text":"<ul> <li>When I start a container and select some type of machine. Do I pay for the machine when installing python or other things? Should I select a cheap machine for development and a expensive one for training? Yes, we are charged for using the workstation. So is better to use a cheap workstation to launch jobs.</li> <li>I lost internet when using wireguard, solved.</li> <li>I have created a dataset but does not seem to be working</li> <li>Should I stop the container after creating the environment? Yes</li> <li>When trying to train on \"canada-a100-x1-p3-ws\" I get <code>Failed to build shape list from priority list, as no shapes matched the priority list</code>, same for \"canada-h100-x1-p3-ws\". Why some machines are only available for workstation and not for burst experiments?</li> <li>It would be nice to be able to sort the shapes by cost, and show also the cost per GPU (not just the total cost)</li> <li>It would be nice to be able to batch delete previous experiments, to have a cleaner interface in the web</li> </ul>"},{"location":"modeling/Iteration_26_more_compute/#training-on-the-sydney-cluster","title":"Training on the Sydney Cluster","text":"<p>I have noticed that they charge $1.25 hour per GPU on the Sydney Cluster, and slightly less for having a container active. On a machine with 4 GPUs that would be around $1.5 per 3090 GPU. Thus it does not have too much sense to use them considering that they charge $2.10 for an H100 GPU. So the rest of this section does not have too much sense, but I leave it for reference.</p> <p>I haven't received a reply of how to train on the Sydney Cluster. Thus I have had the idea to use workstations for training.</p> <p>The idea is to request workstations with a few GPUs, and launch search and learn experiments there. I won't be saving anything so in theory I should be capable of doing it.</p> <p>I can create workstations on the Sydney Cluster with 4 GPUs, and I have to attach the BARC model to them. Running htop shows that the machines have 32 cores and 378GB of RAM, more than enough.</p> <pre><code># install all the dependencies\napt update &amp;&amp; apt install -y python3-dev python3-pip python3-virtualenv git nano vim htop screen nvtop\nmkdir data training data/barc\n# copy these two files from the first workstation\nvim .ssh/id_ed25519\nchmod 600 .ssh/id_ed25519\nvim secrets.sh #export WANDB_API_KEY=\nchmod +x secrets.sh\n# clone repo and create environment\ngit config --global user.name \"Guillermo Barbadillo\"\ngit config --global user.email \"guillermobarbadillo@gmail.com\"\ngit clone git@github.com:ironbar/arc25.git\npython3 -m virtualenv ~/arc25_env\nsource ~/arc25_env/bin/activate\npip install -r arc25/requirements.txt\npip install unsloth_zoo==2025.9.6 # I should update the requirements\n# skip this step by now because it is very slow\n#MAX_JOBS=2 python -m pip install flash-attn==2.6.3 --no-build-isolation\n</code></pre>"},{"location":"modeling/Iteration_26_more_compute/#results","title":"Results","text":""},{"location":"modeling/Iteration_26_more_compute/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_26_more_compute/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_26_more_compute/#todo","title":"TODO","text":"<ul> <li> Strong compute<ul> <li> Clone arc25 repo, for that I have to add a new public key to github.</li> <li> Create python environment</li> <li> Create dataset for BARC induction model</li> <li> Add data to repo for simplicity</li> <li> Create a sample RL training script</li> <li> Launch first experiment, with wandb, saving to artifacts</li> <li> Create multiple experiments</li> <li> How to get the artifacts? It seems I can make the artifacts available to a running workstation</li> <li> Awaiting for answers to my doubts</li> <li> Use workstations for training<ul> <li>I'm really charged? 1,292.63 at the start of the experiment</li> <li>$1,292.42 while the experiment is running, some charge has already happened. I see $1.32 on pending charges.</li> <li>$1,287.19 after stopping the experiment. </li> <li>So yes, I'm being charged for using the Sydney Cluster</li> </ul> </li> </ul> </li> </ul>"},{"location":"modeling/Iteration_27_improve_search_and_learn/","title":"Iteration 27. Improve search and learn","text":""},{"location":"modeling/Iteration_27_improve_search_and_learn/#iteration-27-improve-search-and-learn","title":"Iteration 27. Improve search and learn","text":"<p>start date</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#goal","title":"Goal","text":"<p>We already know that search and learn works. Can we make it faster and better?</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#motivation","title":"Motivation","text":""},{"location":"modeling/Iteration_27_improve_search_and_learn/#development","title":"Development","text":""},{"location":"modeling/Iteration_27_improve_search_and_learn/#speed-analysis-of-all-the-parts","title":"Speed analysis of all the parts","text":"<p>The goal is to measure where the execution time is going so I can have more information to design improvements</p> <ul> <li>Probably the easiest way to do it is to use the logs from Kaggle.</li> <li>I can also use the logs from the cluster experiments</li> </ul> <p>The work has been done in the notebook <code>notebooks/013_analyze_kaggle_runtime.ipynb</code>.</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#local-experiments-with-group-sizes","title":"Local experiments with group sizes","text":"<pre><code>export GROUP_SIZE=1\npython scripts/search_and_learn_with_unsloth.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-24-debug-grouped-tasks/group-size-${GROUP_SIZE} \\\n--task-group-size ${GROUP_SIZE} \\\n--max-epochs 3 \\\n--initial-predictions 8 \\\n--predictions-per-epoch 8\n# 3978s\n\nexport GROUP_SIZE=67\npython scripts/search_and_learn_with_unsloth.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-24-debug-grouped-tasks/group-size-${GROUP_SIZE} \\\n--task-group-size ${GROUP_SIZE} \\\n--max-epochs 3 \\\n--initial-predictions 8 \\\n--predictions-per-epoch 8\n# 1493s\n\nexport GROUP_SIZE=34\npython scripts/search_and_learn_with_unsloth.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-24-debug-grouped-tasks/group-size-${GROUP_SIZE} \\\n--task-group-size ${GROUP_SIZE} \\\n--max-epochs 3 \\\n--initial-predictions 8 \\\n--predictions-per-epoch 8\n# 1549s\n\nexport GROUP_SIZE=17\npython scripts/search_and_learn_with_unsloth.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-24-debug-grouped-tasks/group-size-${GROUP_SIZE} \\\n--task-group-size ${GROUP_SIZE} \\\n--max-epochs 3 \\\n--initial-predictions 8 \\\n--predictions-per-epoch 8\n# 1585s\n\nexport GROUP_SIZE=9\npython scripts/search_and_learn_with_unsloth.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-24-debug-grouped-tasks/group-size-${GROUP_SIZE} \\\n--task-group-size ${GROUP_SIZE} \\\n--max-epochs 3 \\\n--initial-predictions 8 \\\n--predictions-per-epoch 8\n# 1615s\n\nexport GROUP_SIZE=4\npython scripts/search_and_learn_with_unsloth.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-24-debug-grouped-tasks/group-size-${GROUP_SIZE} \\\n--task-group-size ${GROUP_SIZE} \\\n--max-epochs 3 \\\n--initial-predictions 8 \\\n--predictions-per-epoch 8\n# 2029s\n\npython scripts/search_and_learn_with_unsloth.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-09-24-debug-grouped-tasks/no-learning \\\n--max-epochs 0 \\\n--initial-predictions 32\n# 566s\n</code></pre> <p>By grouping the tasks we can a speedup of around 2.5x</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#cluster-experiments-with-128-predictions","title":"Cluster experiments with 128 predictions","text":"<p>The goal of this experimentation is to find the best compromise between the number of predictions per epoch and the group size. Ideally I will do one prediction per epoch and do not group the tasks, but we have to make a compromise because of the current hardware. Once I find a good configuration we could make experiments with 512 predictions and compare against the previous iteration.</p> <pre><code>export FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=32\nexport EPOCHS=3\nexport PREDICTIONS_PER_EPOCH=32\nexport GROUP_SIZE=10\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237498.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=16\nexport EPOCHS=7\nexport PREDICTIONS_PER_EPOCH=16\nexport GROUP_SIZE=10\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237499.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=8\nexport EPOCHS=15\nexport PREDICTIONS_PER_EPOCH=8\nexport GROUP_SIZE=10\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237500.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=8\nexport EPOCHS=15\nexport PREDICTIONS_PER_EPOCH=8\nexport GROUP_SIZE=20\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237501.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=16\nexport EPOCHS=7\nexport PREDICTIONS_PER_EPOCH=16\nexport GROUP_SIZE=5\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237502.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=8\nexport EPOCHS=15\nexport PREDICTIONS_PER_EPOCH=8\nexport GROUP_SIZE=5\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237849.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=8\nexport EPOCHS=15\nexport PREDICTIONS_PER_EPOCH=8\nexport GROUP_SIZE=3\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237850.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=8\nexport EPOCHS=15\nexport PREDICTIONS_PER_EPOCH=8\nexport GROUP_SIZE=40\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237851.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=16\nexport EPOCHS=7\nexport PREDICTIONS_PER_EPOCH=16\nexport GROUP_SIZE=20\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=32\nexport EPOCHS=3\nexport PREDICTIONS_PER_EPOCH=32\nexport GROUP_SIZE=20\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237933.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=32\nexport EPOCHS=3\nexport PREDICTIONS_PER_EPOCH=32\nexport GROUP_SIZE=40\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237934.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=16\nexport EPOCHS=7\nexport PREDICTIONS_PER_EPOCH=16\nexport GROUP_SIZE=40\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237975.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=8\nexport EPOCHS=15\nexport PREDICTIONS_PER_EPOCH=8\nexport GROUP_SIZE=80\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237976.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=16\nexport EPOCHS=7\nexport PREDICTIONS_PER_EPOCH=16\nexport GROUP_SIZE=80\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237977.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=32\nexport EPOCHS=3\nexport PREDICTIONS_PER_EPOCH=32\nexport GROUP_SIZE=80\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237978.\n\nexport FOLDER=2025-09-24-search-and-learn\nexport INITIAL_PREDICTIONS=32\nexport EPOCHS=3\nexport PREDICTIONS_PER_EPOCH=32\nexport GROUP_SIZE=5\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=32G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n237998.\n</code></pre>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#cluster-experiments-with-512-predictions","title":"Cluster experiments with 512 predictions","text":"<pre><code>export FOLDER=2025-09-28-search-and-learn\nexport INITIAL_PREDICTIONS=32\nexport EPOCHS=15\nexport PREDICTIONS_PER_EPOCH=32\nexport GROUP_SIZE=5\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=48G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n238003.-238007.\n\nexport FOLDER=2025-09-28-search-and-learn\nexport INITIAL_PREDICTIONS=8\nexport EPOCHS=63\nexport PREDICTIONS_PER_EPOCH=8\nexport GROUP_SIZE=30\nexport LEARNING_RATE=2e-6; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=48G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n238008.-238011.\n\nexport FOLDER=2025-09-28-search-and-learn\nexport INITIAL_PREDICTIONS=16\nexport EPOCHS=32\nexport PREDICTIONS_PER_EPOCH=16\nexport GROUP_SIZE=20\nexport LEARNING_RATE=1e-5; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/search_and_learn_with_unsloth.py \\\n--task-group-size ${GROUP_SIZE} \\\n--initial-predictions ${INITIAL_PREDICTIONS} \\\n--predictions-per-epoch ${PREDICTIONS_PER_EPOCH} \\\n--learning-rate ${LEARNING_RATE} \\\n--max-epochs ${EPOCHS} \\\n--gpu_memory_utilization 0.5 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${INITIAL_PREDICTIONS}i_${EPOCHS}x${PREDICTIONS_PER_EPOCH}_lr${LEARNING_RATE}_${GROUP_SIZE}-group-size\" -append request_gpus=1 -append request_cpus=6 -append request_memory=48G --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")'\n</code></pre>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#results","title":"Results","text":""},{"location":"modeling/Iteration_27_improve_search_and_learn/#inference-is-not-efficient-for-a-small-number-of-predictions","title":"Inference is not efficient for a small number of predictions","text":"initial predictions epochs predictions per epoch total predictions search time (h) predictions/hour learn time (h) training steps training steps/hour 512 0 0 512 5.12 100.0 0 0 - 128 1 128 256 2.98 85.9 2.66 128 48.1 64 3 64 256 4.2 61.0 4.08 192 47.1 32 2 32 96 2.26 42.5 1.36 64 47.1 <p>The table shows how the inference efficiency decreases if we use a smaller number of predictions per epoch. In the other hand training is not affected by using a smaller number of training steps.</p> <p>If we group the predictions of different tasks, we should see improvements in speed.</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#speedup-due-to-grouping-tasks","title":"Speedup due to grouping tasks","text":"<p>By grouping the tasks we can get a speedup of around 2.5x (considering the full execution time, not just inference). These are the results of local and small experiments. It is possible that we could achieve an even higher speedup with bigger experiments.</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#searching-the-best-configuration-for-inference","title":"Searching the best configuration for inference","text":"<p>The table below shows the runtime for experiments with a budget of 128 predictions.</p> <p></p> <p>It is clear how using a smaller group size or smaller number of predictions per epoch leads to bigger runtimes.</p> <p>Now I'm going to experiment with a budget of 512 predictions. I'm going to try these configurations:</p> <ul> <li>32 predictions per epoch, group size 5</li> <li>16 predictions per epoch, group size 20</li> <li>8 predictions per epoch, group size 30</li> </ul> <p>For each configuration I should try different learning rates.</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#could-not-find-a-good-hyperparameter-setup","title":"Could not find a good hyperparameter setup","text":"<p>I have tried with different learning rates for the configuration of 32 predictions per group with a group size of 5. The problem of this experiments is that they take more than 3 days to make 512 predictions per task for the 400 validation tasks on a single A6000 GPU. Iteration is very slow.</p> <p>I have not seen a clear tendency regarding the learning rate.</p> <p>https://wandb.ai/guillermobarbadillo/2025-09-28-search-and-learn?nw=nwuserguillermobarbadillo</p> <p></p> <p>It is likely that with more time I could find some awesome configuration. All the experiments score above the 23.3% baseline from Iteration_23_ttt_BARC_v2, however I was expecting an improvement over the search and learn methods and that did not happened.</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#conclusion","title":"Conclusion","text":"<p>I have been able to speedup search and learn by grouping tasks, but I have not been able to make it more accurate.</p>"},{"location":"modeling/Iteration_27_improve_search_and_learn/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_27_improve_search_and_learn/#todo","title":"TODO","text":"<ul> <li> Maybe I should discard the idea of tuning for each task independently if the data shows that it is too inefficient<ul> <li> Edit the script to support grouping tasks</li> <li> Compare speed of grouping vs no grouping. Create a table to show the effect of group size and number of predictions per epoch</li> <li> Experiments to verify that improvement is also get when grouping</li> </ul> </li> <li> This implementation is still not efficient for H100.<ul> <li>https://wandb.ai/guillermobarbadillo/2025-09-18-search-and-learn/runs/nmzebmh1?nw=nwuserguillermobarbadillo</li> </ul> </li> <li> Once a good configuration has been found scale to 512 predictions and compare against previous results</li> <li> Optimize learning rate and group size</li> <li> Try to reduce computation cost by filtering the training data<ul> <li> Start by training on half of the samples</li> <li> Remove duplicates</li> <li> Explore a policy of keeping the best and more diverse solutions</li> </ul> </li> <li> Experiments on Kaggle to find a good configuration to submit. https://docs.google.com/spreadsheets/d/1NmmCZA7gPOyoBypwvpw_JhYdjcvqNFHibX_WahwTHIM/edit?gid=0#gid=0&amp;range=F857</li> </ul>"},{"location":"modeling/Iteration_28_refine_predictions/","title":"Iteration 28. Refine predictions","text":""},{"location":"modeling/Iteration_28_refine_predictions/#iteration-28-refine-predictions","title":"Iteration 28. Refine predictions","text":"<p>01-09-2025</p>"},{"location":"modeling/Iteration_28_refine_predictions/#goal","title":"Goal","text":"<p>Study if asking the model to refine its prediction is helpful</p>"},{"location":"modeling/Iteration_28_refine_predictions/#motivation","title":"Motivation","text":"<p>All the evolutionary search approaches use the model to refine its most promising solutions.</p> <p>I want to explore:</p> <ol> <li>Does the GPU 25GB VRAM allow to do prediction refine with BARC induction model?</li> <li>How much improvement do we get compared to doing independent predictions?</li> </ol>"},{"location":"modeling/Iteration_28_refine_predictions/#development","title":"Development","text":""},{"location":"modeling/Iteration_28_refine_predictions/#estimate-the-number-of-tokens","title":"Estimate the number of tokens","text":"<p>Without refining the longest tasks are those which have 4 training tasks of input and outputs with shape 30x30 and the test task is also 30x30. If we consider the newline token that accounts for <code>8370=30*31*(4*2+1)</code> just for the tokens. In my case adding the prompt increases the token count to 8650.</p> <p>When we refine the token we have to add:</p> <ul> <li>Code generated by the model: 1000 tokens max</li> <li>Outputs of the training samples: 3720 tokens max</li> </ul> <p>Thus without considering any message in the prompt it would be 13090 tokens. Being conservative we could request for 13500 tokens in the refining prompt, and a total sequence length of 14500 tokens considering that we allow to predict 1000 tokens.</p>"},{"location":"modeling/Iteration_28_refine_predictions/#how-much-vram-is-needed-for-14500-sequence-length","title":"How much VRAM is needed for 14500 sequence length?","text":"<p>When using unsloth I need 0.75 of the 3090 VRAM to be able to make those predictions, with VLLM is enough with 0.5.</p> <p>If I don't quantize the model to 4-bit then I need at least 0.8 memory with VLLM.</p>"},{"location":"modeling/Iteration_28_refine_predictions/#experiment-design","title":"Experiment design","text":"<p>The easiest experiment is to create a notebook where I just do solution refinement. This implies I already need to have the solutions generated and saved to disk. Probably the easiest way is to reuse predictions from search and learn experiments.</p> <p>I could select n random unsolved predictions for each task, and compare the accuracy against the baseline that does not use prediction refinement.</p>"},{"location":"modeling/Iteration_28_refine_predictions/#generate-predictions-to-refine","title":"Generate predictions to refine","text":"<pre><code>export N_PREDICTIONS=8; python scripts/search_and_learn_with_unsloth.py \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-08-generate-predictions-to-refine/${N_PREDICTIONS}i \\\n--initial-predictions ${N_PREDICTIONS}\n</code></pre>"},{"location":"modeling/Iteration_28_refine_predictions/#results","title":"Results","text":"initial predictions refinement predictions valid code valid outputs unique outputs train_pixel_score train_correct_grids train_pass_rate train_is_correct test_pixel_score test_correct_grids test_pass_rate test_is_correct is_correct 128 0 99.9% 71.7% 49.8% 42.1% 2.4% 1.6% 16.3% 40.9% 2.0% 2.0% 23.0% 16.3% 64 64 99.7% 74.0% 43.7% 45.8% 2.1% 1.1% 16.5% 44.4% 1.7% 1.6% 21.5% 16.0% <p>The baseline makes 128 predictions per task, and the contender does 64 initial predictions, selects the most promising ones (that didn't solve the train set) and refines them.</p> <p>The table shows that there is no clear difference between the approaches. Both solve almost the same number of tasks: 16%.</p>"},{"location":"modeling/Iteration_28_refine_predictions/#conclusion","title":"Conclusion","text":"<p>I have tried to refine predictions with the BARC induction model but results did not improve over just making independent predictions.</p> experiment pass@128 baseline (no refinement) 16.3% refine predictions 16.0% <p>Frontier models benefit from refining its predictions, but this 8B model does not. The model was finetuned just to make predictions, not to refine them. Very likely that ability could be developed with reinforcement learning.</p>"},{"location":"modeling/Iteration_28_refine_predictions/#next-steps","title":"Next steps","text":"<p>Focus on RL and search and learn. No more time for refinement.</p>"},{"location":"modeling/Iteration_28_refine_predictions/#todo","title":"TODO","text":"<ul> <li> How much memory is needed to do refinement? Estimate the number of necessary tokens and try with VLLM</li> <li> ~Collect predictions from previous experiments~ I have found that I wasn't saving all the required information.</li> <li> Modify search and learn to save the required information</li> <li> Create a notebook to see experiment with solution refinement</li> </ul>"},{"location":"modeling/Iteration_29_multi-gpu-rl/","title":"Iteration 29. Multi-gpu RL","text":""},{"location":"modeling/Iteration_29_multi-gpu-rl/#iteration-29-multi-gpu-rl","title":"Iteration 29. Multi-gpu RL","text":"<p>04-10-2025</p>"},{"location":"modeling/Iteration_29_multi-gpu-rl/#goal","title":"Goal","text":"<p>Can I train a model with RL and multiple GPUs?</p>"},{"location":"modeling/Iteration_29_multi-gpu-rl/#motivation","title":"Motivation","text":"<p>The current unsloth implementation is able to train at around 3k-5k steps per day depending on the number of generations. To be able to train faster I want to try to use multiple GPUs for training.</p> <p>Current BARC induction model is not strong enough to solve ARC, it needs a lot of RL training.</p>"},{"location":"modeling/Iteration_29_multi-gpu-rl/#development","title":"Development","text":""},{"location":"modeling/Iteration_29_multi-gpu-rl/#documentation","title":"Documentation","text":"<p>https://docs.unsloth.ai/basics/multi-gpu-training-with-unsloth</p> <ul> <li>DDP: Distributed Data Parallel. The most common PyTorch distributed training method. Each GPU holds a full copy of the model, processes different batches of data, and gradients are synchronized across GPUs.</li> <li>FSDP. Fully Sharded Data Parallel. A PyTorch parallelism method where model parameters, gradients, and optimizer states are sharded across GPUs to save memory. Each GPU only holds a fraction of the model at any time.</li> </ul> <p>I'm interested in DDP, because the model can fit on a single GPU.</p>"},{"location":"modeling/Iteration_29_multi-gpu-rl/#implementation","title":"Implementation","text":"<pre><code># baseline\nexport EPOCHS=1\nexport NUM_GENERATIONS=8\nexport ACCUM_STEPS=2\npython scripts/rl_code_finetuning.py \\\n--learning-rate 1e-5 \\\n--epochs ${EPOCHS} \\\n--warmup-ratio 0.01 \\\n--gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} \\\n--lora-r 16 \\\n--gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-05-debug-multigpu/baseline-1GPU\n\n# multigpu with torchrun\n# this hangs after initializing the LLM engine\nexport EPOCHS=1\nexport NUM_GENERATIONS=8\nexport ACCUM_STEPS=2\ncd scripts\ntorchrun --nproc_per_node 2 -m rl_code_finetuning_multigpu \\\n--learning-rate 1e-5 \\\n--epochs ${EPOCHS} \\\n--warmup-ratio 0.01 \\\n--gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} \\\n--lora-r 16 \\\n--gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-05-debug-multigpu/2-GPUs\n\n# accelerate\n# this also hangs after initializing the LLM engine\naccelerate launch rl_code_finetuning_multigpu.py \\\n--learning-rate 1e-5 \\\n--epochs ${EPOCHS} \\\n--warmup-ratio 0.01 \\\n--gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} \\\n--lora-r 16 \\\n--gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-05-debug-multigpu/2-GPUs\n\n\n# new script\nexport EPOCHS=1\nexport NUM_GENERATIONS=8\nexport ACCUM_STEPS=2\npython scripts/rl_code_finetuning_multigpu.py \\\n--max-seq-length 1536 \\\n--max-completion-length 512 \\\n--learning-rate 1e-5 \\\n--epochs ${EPOCHS} \\\n--warmup-ratio 0.01 \\\n--gpu-memory-utilization 0.3 \\\n--num-generations ${NUM_GENERATIONS} \\\n--lora-r 16 \\\n--gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-05-debug-multigpu/new-script-1GPU\n# 28/67 [16:41&lt;20:54, 32.18s/it\n\n# new script 2 gpus\naccelerate launch scripts/rl_code_finetuning_multigpu.py \\\n--max-seq-length 1536 \\\n--max-completion-length 512 \\\n--learning-rate 1e-5 \\\n--epochs ${EPOCHS} \\\n--warmup-ratio 0.01 \\\n--gpu-memory-utilization 0.3 \\\n--num-generations ${NUM_GENERATIONS} \\\n--lora-r 16 \\\n--gradient-accumulation-steps ${ACCUM_STEPS} \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-05-debug-multigpu/new-script-2GPUs\n# 27/33 [17:35&lt;04:01, 40.32s/it\n</code></pre> <p>Baseline experiment trains at around 28s/it and the GPU is at 100% utilization. It seems that unsloth is not currently prepared to do multigpu training, I will have to try with plain trl.</p> <p>I had to modify the <code>GRPOTrainer</code> on line 539 to add <code>quantization='bitsandbytes',</code>, otherwise I don't have enough VRAM. <code>/home/gbarbadillo/miniconda3/envs/arc25/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py</code></p> <p>When training with 2 GPUs the number of steps is halved, I would need to do experiments to verify that the model is learning correctly.</p>"},{"location":"modeling/Iteration_29_multi-gpu-rl/#validate-implementation-locally","title":"Validate implementation locally","text":"<pre><code># baseline\nexport EPOCHS=40\nexport NUM_GENERATIONS=8\nexport ACCUM_STEPS=2\npython scripts/rl_code_finetuning.py \\\n--learning-rate 1e-5 \\\n--epochs ${EPOCHS} \\\n--warmup-ratio 0.01 \\\n--max-seq-length 1536 \\\n--max-completion-length 512 \\\n--gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} \\\n--lora-r 16 \\\n--gradient-accumulation-steps ${ACCUM_STEPS} \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small-10_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-05-validate-multigpu/baseline-1GPU-${EPOCHS}epochs\n\n# new script with 1 GPU\nexport EPOCHS=40\nexport NUM_GENERATIONS=8\nexport ACCUM_STEPS=2\npython scripts/rl_code_finetuning_multigpu.py \\\n--max-seq-length 1536 \\\n--max-completion-length 512 \\\n--learning-rate 1e-5 \\\n--epochs ${EPOCHS} \\\n--warmup-ratio 0.01 \\\n--gpu-memory-utilization 0.3 \\\n--num-generations ${NUM_GENERATIONS} \\\n--lora-r 16 \\\n--gradient-accumulation-steps ${ACCUM_STEPS} \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small-10_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-05-validate-multigpu/new-script-1GPU-${EPOCHS}epochs\n</code></pre>"},{"location":"modeling/Iteration_29_multi-gpu-rl/#results","title":"Results","text":""},{"location":"modeling/Iteration_29_multi-gpu-rl/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_29_multi-gpu-rl/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_29_multi-gpu-rl/#todo","title":"TODO","text":"<ul> <li> Validate the code locally<ul> <li> Create an even smaller dataset ~ 10 tasks</li> <li> Train with the previous script on a single GPU</li> <li> When training without unsloth it does not log the same metrics</li> <li> Train with the new script on a single GPU</li> <li> Train with the new script on 2 GPUs</li> </ul> </li> </ul>"},{"location":"modeling/Iteration_30_solve_RL_collapse/","title":"Iteration 30. Solve RL Collapse","text":""},{"location":"modeling/Iteration_30_solve_RL_collapse/#iteration-30-solve-rl-collapse","title":"Iteration 30. Solve RL Collapse","text":"<p>05-10-2025</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#goal","title":"Goal","text":"<p>I need to solve RL collapse issues so I can train for longer and get a more powerful base model.</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#motivation","title":"Motivation","text":"<p>I have already seen improvements in model capabilities after training with RL on iteration 24, but when training for longer the reward collapsed.</p> <p>I need to understand the problem and fix it so I can train for longer on more data.</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#development","title":"Development","text":""},{"location":"modeling/Iteration_30_solve_RL_collapse/#thoughts-about-the-collapse-problem","title":"Thoughts about the collapse problem","text":"<p>I have seen different kinds of collapse: long predictions and gibberish prediction.</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#long-predictions","title":"Long predictions","text":"<p>I don't understand why, but the model starts doing long predictions after 5k steps of training. Predictions are being truncated because the max completion length is 1024.</p> <p></p> <p>On the first experiments I was masking the truncated completions, furthermore they should probably get a reward of 0</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#gibberish-prediction","title":"Gibberish prediction","text":"<p>When I started using repetition penalty to avoid long predictions (because I saw predictions with lots of repetitions) I also saw the model doing gibberish predictions.</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#cluster-experiments","title":"Cluster experiments","text":"<pre><code>## Start from zero\nexport REPETITION_PENALTY=1.05\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=1e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport EXPERIMENT_NAME=lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n240688.0\n\nexport REPETITION_PENALTY=1.05\nexport FOLDER=2025-09-19-rl-first-steps\nexport LEARNING_RATE=1e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport EXPERIMENT_NAME=lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_unmasked-truncate\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--no-mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n240689.0\n\n## New experiments after updating the logs and adding new reward\n# Going to increase the learning rate to try to force collapse\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v1\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 240875.0\n\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 240874.0\n\n# try without 4-bit quantization\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 241165.\n\n# Increase KL\nexport BETA=0.002\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 241166.\n\nexport BETA=0.004\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 241399.0\n\nexport BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 241400.0\n\nexport BETA=0.02\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 241916.0\n\nexport BETA=0.04\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 241917.0\n\n## Definitive training attempt, lower learning rate and increase the number of generations\nexport BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=1e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 242984.0\n\nexport BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=4e-7\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=32\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 242985.0\n\n\n## Reduce LoRA\nexport BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=8\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 243382.0\n\nexport BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 243383.0\n\n# Increase the learning rate when using lora r=1\nexport BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 243545.\n\nexport BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-05-rl-study-collapse\nexport LEARNING_RATE=1e-5\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=100\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${REWARD_NAME}_lr${LEARNING_RATE}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_${LORA_R}lora_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_training_challenges.json \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 243546.\n</code></pre>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#how-to-log-more-metrics-about-the-rewards","title":"How to log more metrics about the rewards","text":"<p>If I have access to the trainer, I could simply add metrics to the object <code>_metrics</code>.</p> <pre><code># /home/gbarbadillo/miniconda3/envs/arc25/lib/python3.10/site-packages/trl/trainer/grpo_trainer.py\nprediction_step(\n_prepare_inputs\n_generate_and_score_completions\nself._metrics[mode][\"reward\"].append(mean_grouped_rewards.mean().item())\n        self._metrics[mode][\"reward_std\"].append(std_rewards.mean().item())\n        self._metrics[mode][\"frac_reward_zero_std\"].append(is_std_zero.float().mean().item())\n\nlog(\nmetrics = {key: sum(val) / len(val) for key, val in self._metrics[mode].items()}  # average the metrics\n</code></pre> <pre><code># baseline\nexport EPOCHS=1\nexport NUM_GENERATIONS=8\nexport ACCUM_STEPS=2\npython scripts/rl_code_finetuning.py \\\n--learning-rate 1e-5 \\\n--epochs ${EPOCHS} \\\n--warmup-ratio 0.01 \\\n--max-seq-length 1536 \\\n--max-completion-length 512 \\\n--gpu-memory-utilization 0.70 \\\n--num-generations ${NUM_GENERATIONS} \\\n--lora-r 16 \\\n--gradient-accumulation-steps ${ACCUM_STEPS} \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small-10_arc-agi_training_challenges.json \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-06-debug-reward-logging/baseline-1GPU-${EPOCHS}epochs\n</code></pre>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#results","title":"Results","text":""},{"location":"modeling/Iteration_30_solve_RL_collapse/#training-collapse","title":"Training collapse","text":"<p>When training for long on all the ARC-AGI-1 training samples I have observed that the reward collapses.</p> <p>First trainings for more than 5k steps (more than 12 epochs) show the same problem. </p> <p></p> <p>The model starts to make longer predictions that fill all the output tokens, it repeats the same text over and over. After seeing this I thought the problem could be the reward function, that was making a distinction between being able to parse or not being able to parse the code. Thus it might be favoring bad code sometimes if it could be parsed.</p> <p>However simplifying the reward did not solve the problem. The metrics show the same problem:</p> <p></p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#repetition-penalty-and-unmasked-truncated-completions","title":"Repetition penalty and unmasked truncated completions","text":"<p>I have tried different configurations of repetition penalty and unmasking the truncated completions to see if I could continue a training without collapsing without much success. Sometimes I could prevent collapse but at the cost of not improving the reward.</p> <p></p> <p>Training from zero with repetition penalty or unmasked truncated completions did not avoid the problem:</p> <p></p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#simplify-the-reward","title":"Simplify the reward","text":"<p>Simplifying the reward did not solve the problem, we see the same behaviour with the two reward implementations.</p> <p>However after analyzing the truncated prompts I don't see repetitions of the same ngrams, I see giberish. It seems that the model is making a prediction, fails on some token and then derails and forgets what its doing.</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#use-unquantized-model","title":"Use unquantized model","text":"<p>Using unquantized model does not solve the problem, but training is much faster, twice as fast.</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#increase-the-kl-penalty-beta","title":"Increase the KL penalty (beta)","text":"<p>The model does not have that behaviour of repeating ngrams at the start of the training. Maybe increasing the KL penalty can avoid that behaviour to arise.</p> <p></p> <ul> <li>The experiments show a clear effect on the KL training metrics. If we increase the penalty the KL value is lower.</li> <li>Also it seems to delay the occurence of truncated completions. However they eventually start happening.</li> <li>The relation with reward is unclear.</li> </ul>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#do-we-need-a-longer-max_completion_length-than-1024","title":"Do we need a longer max_completion_length than 1024","text":"<p>The average max solving length is around 400 tokens, but we can see a small fraction of the tasks requiring close to 1024 tokens. This implies that we could get a small benefit from increasing the max_completion_length,  but the current value is a good choice.</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#lowering-the-lora-rank","title":"Lowering the LoRA rank","text":"<p>This article by thinkingmachines says that for RL LoRA rank 1 is enough.</p>"},{"location":"modeling/Iteration_30_solve_RL_collapse/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_30_solve_RL_collapse/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_30_solve_RL_collapse/#todo","title":"TODO","text":"<ul> <li> I need more information to diagnose the problem<ul> <li> I should log the prompt if it reaches the maximum completion length. That way I could see the first times that the model is doing that.</li> <li> Max reward might be a better metric that mean reward</li> <li> Log rewards of truncated completions</li> </ul> </li> <li> Maybe using the pixel score as a reward was a bad idea. It might reward the wrong thing<ul> <li> Allow to choose different rewards when training</li> <li> Try training without pixel score reward</li> </ul> </li> <li> Analyze logs of failed training, and document experiments training from zero</li> <li> Is the new reward helpful?</li> <li> Are the new metrics helpful to understand the problem? Yes</li> <li> Maybe I have to use a longer max sequence length?<ul> <li> Analyze truncate errors</li> </ul> </li> <li> Actions to solve RL collapse<ul> <li> Log ngram repetition and unique tokens</li> <li> Avoid model quantization. Not sure if will solve the problem but it's training way faster, more than x2.</li> <li> Add or increase the KL penalty</li> <li> Lower the learning rate, and/or do gradient clipping. . I'm already doing <code>max_grad_norm=0.1,</code>.</li> <li> <code>frequency_penalty</code> can be a better option than <code>repetition_penalty</code>, <code>generation_kwargs=dict(frequency_penalty=1.1)</code></li> <li> Using the information from ngram repetition metrics, add a reward penalty to ngram repetition. (VLLM does not have a ngram-repetion-penalty)</li> </ul> </li> <li> Is it enough with rank=1 for RL? https://thinkingmachines.ai/blog/lora/</li> </ul>"},{"location":"modeling/Iteration_31_how_to_improve/","title":"Iteration 31. How to improve from 20% to 100%?","text":""},{"location":"modeling/Iteration_31_how_to_improve/#iteration-31-how-to-improve-from-20-to-100","title":"Iteration 31. How to improve from 20% to 100%?","text":"<p>12-10-2025</p>"},{"location":"modeling/Iteration_31_how_to_improve/#goal","title":"Goal","text":"<p>Think how to improve from 20% to 100% on the ARC-AGI-1 evaluation set.</p>"},{"location":"modeling/Iteration_31_how_to_improve/#motivation","title":"Motivation","text":"<p>So far we have verified that we can use hindsight relabel to adapt the model to new tasks and increase the ARC score. However the BARC induction model is only solving ~20% of the ARC-AGI-1 evaluation set, and around 1% of the ARC-AGI-2 evaluation set. This model is not capable of solving ARC with the compute constraints of the submission.</p> <p>We need to think outside the box to be able to make a dramatic improvement.</p>"},{"location":"modeling/Iteration_31_how_to_improve/#development","title":"Development","text":""},{"location":"modeling/Iteration_31_how_to_improve/#how-humans-solve-abstraction-and-reasoning-arc-tasks","title":"How humans solve abstraction and reasoning (ARC) tasks?","text":"<p>Humans have the core knowledge priors, so when we look at the pixels of an ARC grid we create an abstract representation of the image. We can describe the grid with natural language, we can understand what the input grids have in common, what the outputs have in common... In summary, abstraction allows us to see not just a collection of pixels, but to build high level abstractions that represent the images and allow to compare them and see what the differences are and what do they have in common.</p> <p>Those abstractions help to reduce the search space when we try to solve a task. We describe the tasks using natural language, describing the transformation between the inputs and the outputs. When we reason we draw some hypothesis of what the task is about. Then we can use our internal world model to transform the grids using the hypothesis and verify if the outputs are correct. On every failed attempt we learn more about the task. In fact an intelligent person should make the attempts that will maximize its learning. That eventually leads to find the right solution.</p> <p>Observe, hypothesize, test, learn, repeat. Solving ARC requires to use the scientific method.</p> <p></p>"},{"location":"modeling/Iteration_31_how_to_improve/#how-my-current-system-compares-against-a-human","title":"How my current system compares against a human?","text":"<p>I'm pretty sure my model does not have abstractions as powerful and general as humans. One way to enhance the representations of the model would be the omni-arc approach from last year ARC24 challenge.</p> <p>Humans have memory. Memory allows to explore the search space without repeating previous errors. My system does many predictions independently, and this produces repeated predictions.</p> <p>Humans have the ability to correct/refine an incorrect solution. However the BARC induction model that I'm currently using does not have this ability.</p> <p></p>"},{"location":"modeling/Iteration_31_how_to_improve/#python-code-and-the-python-interpreter","title":"Python code and the python interpreter","text":"<p>I still believe the easiest way to solve ARC is to use python code and the python interpreter.</p> <p>It is true that frontier models rely on natural language to describe the tasks and use their own capabilities as a world model.</p> <p>But for any task that can be described with natural language there should also be a python program that implements the task. And I would argue that with the right DSL the python program should be short and elegant. Thus I don't see the limitations of using python.</p> <p>Finally the python interpreter always works and in contrast all models are fallible.</p>"},{"location":"modeling/Iteration_31_how_to_improve/#transduction","title":"Transduction","text":"<p>Transduction relies on the model to generate a good representation of the ARC tasks. At test time the model can use in context learning or test-time training to adapt to new tasks.</p> <p>Test-time training is crucial for transduction approaches. My solution improved from 11% to 33% just by doing test-time training.</p> <p>How could this approach be improved:</p> <ul> <li>Pre-train on more data</li> <li>Use an architecture with better inductive priors, that will better represent the programs.</li> <li>Improvements in the test-time training setup</li> </ul> <p>Transduction can solve ARC, but I don't believe is the easiest way to do it. To be able to work the model has to learn an internal representation almost equivalent to a python program. I believe that transduction is more difficult than generating python code.</p> <p>The biggest advantage of transduction is that it can use gradient descent at test-time to adapt to the new tasks. In the other hand we don't have yet a similar adaptation mechanism when using induction.</p>"},{"location":"modeling/Iteration_31_how_to_improve/#results","title":"Results","text":""},{"location":"modeling/Iteration_31_how_to_improve/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_31_how_to_improve/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_31_how_to_improve/#todo","title":"TODO","text":"<ul> <li> Analyze the predictions on the unsolved train and evaluation tasks. Are they in the right direction? Why the model is not solving them?</li> <li> Sample efficiency in the RL literature. https://chatgpt.com/share/68eb58e6-fe78-8012-b33c-cb0689f482c2</li> <li> Divide and conquer approach: What if we try to find programs that solve only a fraction of the task samples?   I believe this could give a small boost but probably won't give the improvement we need.</li> <li> A smaller model with a long context could be trained with RL to learn to search. Instead of doing   multiple independent predictions, the model would use information from previous predictions to   either refine the approach or try completely different approaches. This should be more sample   efficient than the current approach. And the training is more aligned with the goal. The first attempt   uses 9500 tokens (8500 for task encoding and 1000 for the prediction), second attempt is cheaper at 5000 (4000 for grids and 1000 for code prediction).   With a context size of 32k we could make 5 attempts in the worst case (4 train samples of 30x30).</li> <li>My current implementation only uses HER, maybe I should combine it with GRPO.</li> </ul>"},{"location":"modeling/Iteration_32_analyze_model_predictions/","title":"Iteration 32. Analyze model predictions","text":""},{"location":"modeling/Iteration_32_analyze_model_predictions/#iteration-32-analyze-model-predictions","title":"Iteration 32. Analyze model predictions","text":"<p>12-10-2025</p>"},{"location":"modeling/Iteration_32_analyze_model_predictions/#goal","title":"Goal","text":"<p>Analyze model predictions to understand the accuracy. Why is only solving ~20% of the ARC-AGI-1 evaluation tasks.</p>"},{"location":"modeling/Iteration_32_analyze_model_predictions/#motivation","title":"Motivation","text":"<p>To be able to improve I need to understand why it does not solve the tasks.</p>"},{"location":"modeling/Iteration_32_analyze_model_predictions/#development","title":"Development","text":"<p>Using predictions from previous experiments, I need to create a notebook to select the most accurate predictions and visualize them. I will do a random sampling of the unsolved tasks to diagnose the problems.</p>"},{"location":"modeling/Iteration_32_analyze_model_predictions/#results","title":"Results","text":"<p>I have analyzed a random subset of 128 predictions, 16% of the evaluation ARC-AGI-1 tasks were solved.</p> <p></p> <p>The plot shows that model has a good intuition of ARC tasks. Only 20% are complete misunderstood.</p> <p>But at the same time only 16% of the tasks are solved when doing 128 predictions per task. With 20k predictions the solve rate is 38% according to the paper. But making so many predictions does not have sense and it is not efficient. Making a few independent attempt makes sense to have diversity in the predictions, but not in the order of hundreds or thousands.</p>"},{"location":"modeling/Iteration_32_analyze_model_predictions/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_32_analyze_model_predictions/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_32_analyze_model_predictions/#todo","title":"TODO","text":"<ul> <li>[ ]</li> </ul>"},{"location":"modeling/Iteration_33_rl_barc/","title":"Iteration 33. RL with BARC data","text":""},{"location":"modeling/Iteration_33_rl_barc/#iteration-33-rl-with-barc-data","title":"Iteration 33. RL with BARC data","text":"<p>14-10-2025</p>"},{"location":"modeling/Iteration_33_rl_barc/#goal","title":"Goal","text":"<p>Train with RL from BARC synthetic datasets. Does it solve the collapsing problems? Does it produce stronger models?</p>"},{"location":"modeling/Iteration_33_rl_barc/#motivation","title":"Motivation","text":"<p>Maybe the collapsing problems comes from repeating the 400 training tasks over and over. Since we have the BARC synthetic datasets readily available, we should try to use them to see if we can get stronger models and solve the training collapse problem.</p>"},{"location":"modeling/Iteration_33_rl_barc/#development","title":"Development","text":"<p>The Heavy version of the dataset is the most interesting one because it uses more seed functions and uses the stronger gpt4 model for description generation.</p>"},{"location":"modeling/Iteration_33_rl_barc/#verify-that-i-can-train-locally","title":"Verify that I can train locally","text":"<pre><code>python scripts/rl_code_finetuning.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/dataset_100k.json.gz \\\n--epochs 1 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-14-debug-BARC/debug\n</code></pre> <p>Works perfectly.</p>"},{"location":"modeling/Iteration_33_rl_barc/#train-and-evaluate-on-cluster","title":"Train and evaluate on cluster","text":"Click to expand/collapse this section <pre><code>export BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${REWARD_NAME}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 243910.0, collapsed\n\nexport BETA=0.01\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=2e-6\nexport NUM_GENERATIONS=16\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=8\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${REWARD_NAME}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 243912.0\n\n# Increase beta and decrease max grad norm, increase the number of generations\nexport BETA=0.02\nexport MAX_GRAD_NORM=0.05\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 245086.\n\n# I want to see if I need to accumulate steps\nexport BETA=0.02\nexport MAX_GRAD_NORM=0.05\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=1\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 245087. CUDA error: an illegal memory access was encountered\n# It is not an OOM, but very suspicious to see when I remove the accum steps\n# the step the error happend the max completion length was 990, the greatest of all the steps\n# However GPU memory usage was just 60%\n\nexport BETA=0.02\nexport MAX_GRAD_NORM=0.05\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=2\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 245088. collapses\n\n# increase beta and decrease max grad norm even more\nexport BETA=0.04\nexport MAX_GRAD_NORM=0.02\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 245089.\n\n# Increase the number of generations to 64\nexport BETA=0.04\nexport MAX_GRAD_NORM=0.02\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=64\nexport ACUM_STEPS=8\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 246723.0 \n\n# increase the number of generations even more to 128\nexport BETA=0.04\nexport MAX_GRAD_NORM=0.02\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=128\nexport ACUM_STEPS=16\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 247093.0\n\n# sync checkpoints\nrsync -aPv -m  --include='*/'  --exclude *.pt --include='checkpoint-*5000/***'  --include='checkpoint-*0000/***' --exclude='*'  \\\ncalculon01:/mnt/scratch/users/gbarbadillo/arc25/trainings/2025-10-14-rl-barc   /mnt/data/MEGA/TEMP/\n</code></pre> <pre><code>export EXPERIMENT=2025-10-14-rl-barc/8lora_lr2e-6_arc-v2-no-pixel-score_epochs1_16gen_2accum-steps_repetition-penalty-1.02_masked-truncate_unquantized_beta0.01\nexport CHECKPOINT=10000; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/inference_with_BARC.py \\\n--n-predictions 128 \\\n--base-model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--lora-path /mnt/scratch/users/gbarbadillo/arc25/trainings/${EXPERIMENT}/checkpoint-${CHECKPOINT} \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--use-data-augmentation \\\n--output-folder /mnt/scratch/users/gbarbadillo/arc25/predictions/${EXPERIMENT}/checkpoint-${CHECKPOINT}/evaluation\" -append request_gpus=1 -append request_cpus=12 --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")' -append request_memory=32G\n\nexport EXPERIMENT=2025-10-14-rl-barc/1lora_lr4e-6_arc-v2-no-pixel-score_epochs1_16gen_2accum-steps_repetition-penalty-1.02_masked-truncate_unquantized_beta0.01\nexport CHECKPOINT=5000; condor_submit train.condor command=\" \npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/inference_with_BARC.py \\\n--n-predictions 128 \\\n--base-model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--lora-path /mnt/scratch/users/gbarbadillo/arc25/trainings/${EXPERIMENT}/checkpoint-${CHECKPOINT} \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--use-data-augmentation \\\n--output-folder /mnt/scratch/users/gbarbadillo/arc25/predictions/${EXPERIMENT}/checkpoint-${CHECKPOINT}/evaluation\" -append request_gpus=1 -append request_cpus=12 --append 'requirements = (TARGET.Machine == \"calculon19.das-nano.com\")' -append request_memory=32G\n</code></pre>"},{"location":"modeling/Iteration_33_rl_barc/#new-memory-limit-when-doing-code-execution","title":"New memory limit when doing code execution","text":"<pre><code>export LIMIT_MB=512; python scripts/rl_code_finetuning.py \\\n--epochs 1 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-15-debug-memory_limit/limit_${LIMIT_MB}MB \\\n--code-execution-memory-limit-mb ${LIMIT_MB}\n\nexport LIMIT_MB=256; python scripts/rl_code_finetuning.py \\\n--epochs 1 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-15-debug-memory_limit/limit_${LIMIT_MB}MB \\\n--code-execution-memory-limit-mb ${LIMIT_MB}\n</code></pre> <p>I cannot see the effect on the metrics... That's weird.</p>"},{"location":"modeling/Iteration_33_rl_barc/#update-inference-to-also-evaluate-and-save-output-compressed","title":"Update inference to also evaluate and save output compressed","text":"<p>I'm going to update the inference script to also evaluate and save the results compressed. That way I can quickly now the results of the inference.</p> <pre><code>python scripts/inference_with_BARC.py \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/small_arc-agi_training_challenges.json \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions/2025-09-15-debug-grpo/lr1e-5_small-dataset_80epochs_16gens_continue/small-training \\\n--lora-path /mnt/hdd0/Kaggle/arc25/trainings/2025-09-15-debug-grpo/lr1e-5_small-dataset_80epochs_16gens_continue/checkpoint-5360 \\\n--n-predictions 8\n</code></pre>"},{"location":"modeling/Iteration_33_rl_barc/#local-evaluation","title":"Local evaluation","text":"<pre><code>python scripts/inference_with_BARC.py \\\n--n-predictions 8 \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n--use-data-augmentation \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions/2025-10-14-rl-barc/baseline/evaluation\n\n\nEXPERIMENTS=(\n  \"2025-10-14-rl-barc/8lora_lr2e-6_arc-v2-no-pixel-score_epochs1_16gen_2accum-steps_repetition-penalty-1.02_masked-truncate_unquantized_beta0.01\"\n  \"2025-10-14-rl-barc/1lora_lr4e-6_arc-v2-no-pixel-score_epochs1_16gen_2accum-steps_repetition-penalty-1.02_masked-truncate_unquantized_beta0.01\"\n  \"2025-10-14-rl-barc/1lora_lr4e-6_0.02max-grad-norm_arc-v2-no-pixel-score_64gen_8accum-steps_repetition-penalty-1.01_masked-truncate_unquantized_beta0.04\"\n  \"2025-10-14-rl-barc/1lora_lr4e-6_0.02max-grad-norm_arc-v2-no-pixel-score_128gen_16accum-steps_repetition-penalty-1.01_masked-truncate_unquantized_beta0.04\"\n  \"2025-10-14-rl-barc/1lora_lr4e-6_0.05max-grad-norm_arc-v2-no-pixel-score_32gen_4accum-steps_repetition-penalty-1.02_masked-truncate_unquantized_beta0.02\"\n)\n\nfor EXPERIMENT in \"${EXPERIMENTS[@]}\"; do\n  echo \"Processing experiment: ${EXPERIMENT}\"\n  for CHECKPOINT in 1000 5000 10000 15000 20000; do\n    echo \"Running inference for checkpoint-${CHECKPOINT}...\"\n    python scripts/inference_with_BARC.py \\\n      --n-predictions 32 \\\n      --lora-path /mnt/hdd0/MEGA/TEMP/${EXPERIMENT}/checkpoint-${CHECKPOINT} \\\n      --dataset-path /mnt/hdd0/Kaggle/arc25/data/arc-prize-2024/arc-agi_evaluation_challenges.json \\\n      --use-data-augmentation \\\n      --output-folder /mnt/hdd0/Kaggle/arc25/predictions/${EXPERIMENT}/checkpoint-${CHECKPOINT}/evaluation\n  done\ndone\n</code></pre>"},{"location":"modeling/Iteration_33_rl_barc/#train-on-strongcompute-workstation","title":"Train on StrongCompute workstation","text":"<p>I'm going to try to train directly in the workstation as a workaround because all the normal trainings abruptly ended in 1 day. Maybe the workstation lasts more than one day. Another advantage is that I  can use machines with just one GPU.</p> Click to expand/collapse this section <pre><code># arc25\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport BETA=0.04\nexport MAX_GRAD_NORM=0.02\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n\n# arc25_1\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport BETA=0.1\nexport MAX_GRAD_NORM=0.01\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n\n# arc25_2\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport BETA=0.2\nexport MAX_GRAD_NORM=0.005\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n\n# arc25_3\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport BETA=0.4\nexport MAX_GRAD_NORM=0.002\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n\n# arc25_1\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport BETA=0.04\nexport MAX_GRAD_NORM=0.01\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n\n#arc25_2\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport BETA=0.04\nexport MAX_GRAD_NORM=0.004\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n\n#arc25_3\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport BETA=0.04\nexport MAX_GRAD_NORM=0.002\nexport REPETITION_PENALTY=1.01\nexport FOLDER=2025-10-14-rl-barc\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n</code></pre>"},{"location":"modeling/Iteration_33_rl_barc/#dataset-from-generator","title":"Dataset from generator","text":"<p>When I increased the number of generations to 64 I had to increase the RAM memory in the cluster. Maybe starting from a generator could reduce the memory requirements.</p> <pre><code>python scripts/rl_code_finetuning.py \\\n--epochs 1 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-23-debug-generator/debug\n\nexport NUM_GENERATIONS=32; python scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/dataset_100k.json.gz \\\n--epochs 1 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-23-debug-generator/debug-barc-${NUM_GENERATIONS}\n</code></pre> <p>This implementation starts with 13GB of RAM usage, grows to 17.2GB when loading the dataset, and just to 18.1GB when creating the dataset for training. So apparently is very RAM memory efficient. Previous implementation raised RAM usage to 26GB with the same configuration.</p> <pre><code>export NUM_GENERATIONS=32\nexport ACCUMULATION_STEPS=4\n\npython scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACCUMULATION_STEPS} \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/dataset_100k.json.gz \\\n--epochs 1 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-23-debug-generator/debug-barc_${NUM_GENERATIONS}generations_${ACCUMULATION_STEPS}accum-steps_from_list\n# start RAM: 13.6GB, load dataset: 17.4GB, prepare dataset for training: 70GB (needed to use swap memory)\n\npython scripts/rl_code_finetuning.py \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACCUMULATION_STEPS} \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/dataset_100k.json.gz \\\n--epochs 1 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-23-debug-generator/debug-barc_${NUM_GENERATIONS}generations_${ACCUMULATION_STEPS}accum-steps_from_generator\n# start RAM: 8GB, load dataset: 11.6GB, prepare dataset for training: 12.6GB\n</code></pre> <p>In the previous implementation I needed 70GB, with the new just 12GB. This explains the memory error that I saw in the cluster.</p>"},{"location":"modeling/Iteration_33_rl_barc/#results","title":"Results","text":""},{"location":"modeling/Iteration_33_rl_barc/#training-still-collapses-and-the-model-makes-nonsense-predictions","title":"Training still collapses and the model makes nonsense predictions","text":"<p>Despite training on a huge dataset, the training with lora rank 1 has collapsed.</p> <p></p> <p>I'm already using <code>beta=0.01, repetition_penalty=1.02, and max_grad_norm=0.1</code>, but I'm going to make those constraints harder. Also I'm going to double the number of generations from 16 to 32.</p> <p>RL is showing signs of improvements. The improvements are modest but noticeable. The problem is that training for longer will likely make the improvements bigger, but I don't have a robust training configuration yet. And the challenge ending is approaching.</p>"},{"location":"modeling/Iteration_33_rl_barc/#all-trainings-eventually-collapse","title":"All trainings eventually collapse","text":"<p>The bad thing is that these experiments take days, so iterations speed is very small. That makes difficult to find the root of the problem.</p> <p>I have tried:</p> <ul> <li>Using a bigger training dataset (from ARC-AGI-1 400 training samples to BARC 100k samples)</li> <li>Using a bigger number of generations per step (from 8 to 128)</li> <li>Increasing the KL penalty</li> <li>Decreasing the max grad norm</li> </ul> <p>But so far without any success. The training reaches a point where the model starts to predict garbage.</p>"},{"location":"modeling/Iteration_33_rl_barc/#do-the-models-improve-when-doing-rl-training-on-barc-dataset","title":"Do the models improve when doing RL training on BARC dataset?","text":"<p>The following plots show results for doing 32 predictions on the ARC-AGI-1 evaluation dataset.</p> <p></p> <p></p> <p></p> <p>Metrics such as the ratio of correct grids, or the pass rate improve over the training. However the number of solved tasks does not show the same tendency, it seems to peak after 1000 steps of training. However we must take into account that it is the metric with the highest uncertainty.</p> <p>Ideally we would like to see a smooth improvement in all the metrics throughout the training. Currently that is not the case. But maybe we have to solve the problem of collapsing first.</p> <p>In the best experiment we were able to improve the solved tasks from 8.75% to 13.5%.</p>"},{"location":"modeling/Iteration_33_rl_barc/#compare-against-previous-fine-tuned-model","title":"Compare against previous fine-tuned model","text":"dataset RL dataset RL steps n_preds valid code valid outputs unique outputs train_pixel_score train_correct_grids train_pass_rate train_is_correct test_pixel_score test_correct_grids test_pass_rate test_is_correct is_correct evaluation - - 480 100.0% 70.9% 43.8% 41.3% 2.1% 1.3% 22.5% 40.2% 1.7% 1.7% 28.5% 22.3% evaluation ARC-AGI-1 training 8400 480 96.2% 81.9% 36.0% 56.2% 6.8% 4.6% 27.8% 54.9% 5.9% 5.8% 35.3% 27.0% evaluation BARC 1000 480 100.0% 80.5% 42.4% 49.2% 3.3% 2.0% 25.8% 48.0% 2.8% 2.7% 32.3% 25.0% evaluation BARC 10000 480 100.0% 89.1% 44.8% 55.9% 4.3% 2.6% 24.3% 54.2% 3.5% 3.5% 32.3% 23.3% <p>Results do not improve over doing RL on the ARC-AGI-1 training tasks, but they improve over the baseline model.</p>"},{"location":"modeling/Iteration_33_rl_barc/#conclusion","title":"Conclusion","text":"<p>I haven't been able to find a training configuration that does not collapses. The reward improves during some steps and suddenly collapses.</p> <p>Training on a bigger dataset does not prevent the collapse. Neither does using a bigger number of generations per step. I have also tried increasing the KL penalty and decreasing the max gradient norm without success. All the trainings eventually collapse.</p> <p>Models fine-tuned with RL on BARC dataset improve on ARC-AGI-1 evaluation dataset. In the best scenario the solved tasks was improved from 8.75% to 13.5% when doing 32 predictions per task. However the tendency during training is not clear.</p>"},{"location":"modeling/Iteration_33_rl_barc/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_33_rl_barc/#todo","title":"TODO","text":"<ul> <li> Download and curate the synthetic datasets: https://huggingface.co/datasets/barc0/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems</li> <li> Could the random RAM problems be caused by evaluating the code generated by the LLM?</li> <li> Is the model improving when training on BARC data?</li> <li> Can I find a training configuration that allows me to train on the whole datasets without collapsing?   I have launched multiple experiments with different configurations of kl loss and max_grad_norm to see   if any works and which one break.</li> <li> 512 predictions with the best configuration</li> </ul>"},{"location":"modeling/Iteration_34_multi-turn_rl/","title":"Iteration 34. Multi-turn RL","text":""},{"location":"modeling/Iteration_34_multi-turn_rl/#iteration-34-multi-turn-rl","title":"Iteration 34. Multi-turn RL","text":"<p>18-10-2025</p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#goal","title":"Goal","text":"<p>Implement a script to do multi-turn RL training, and test if it has a noticeable effect on model accuracy.</p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#motivation","title":"Motivation","text":"<p>On Iteration 28 I saw that the BARC induction model is not good at refining its predictions. That forces us to just make independent predictions with the model.</p> <p>But that is not efficient, we should take into account previous predictions to avoid repeating errors and benefit from the execution feedback.</p> <p>All the evolutionary test-time compute methods are based on the capability of the model to use feedback from execution.</p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#development","title":"Development","text":""},{"location":"modeling/Iteration_34_multi-turn_rl/#unsloth-grpo-does-not-support-iterable-datasets","title":"Unsloth GRPO does not support Iterable datasets","text":"<pre><code>python scripts/multi-turn_rl_code_finetuning.py \\\n--epochs 1 \\\n--output-dir /mnt/hdd0/Kaggle/arc25/trainings/2025-10-18-debug-multi-turn-RL/baseline\n\n[rank0]: NotImplementedError: Iterable datasets are not yet supported in GRPOTrainer. Please use a standard dataset instead.\n</code></pre> <p>After changing from Dataset to IterableDataset I get this bad surprise.</p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#proof-of-concept-with-pre-generated-responses","title":"Proof of concept with pre-generated responses","text":"<p>The easiest way to test the concept is to generate a dataset were I generate predictions for the task and pick one that is not correct. This is exactly the same I did in Iteration 28 but instead of doing it at test time, I need to do it at training time using training data.</p> <p>So the best option would be to take BARC dataset and make predictions for the tasks.</p> <p>Making 8 predictions for 1000 tasks takes around one hour on a single GPU. A good proof of concept will require between 10k and 20k prompts, at least that is what I'm currently training with RL before the training collapses.</p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#inference-dataset-preparation","title":"Inference Dataset preparation","text":"<p>To prepare the dataset for inference I'm going to reuse the notebook <code>notebooks/016_prepare_BARC_data_for_training.ipynb</code>.</p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#inference","title":"Inference","text":"<pre><code>export PART=1\npython scripts/inference_with_BARC.py \\\n--n-predictions 8 \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/dataset_10k_part${PART}.json.gz \\\n--use-data-augmentation \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions/2025-10-18-barc-inference/part${PART}\n\nexport PART=2\npython scripts/inference_with_BARC.py \\\n--n-predictions 8 \\\n--dataset-path /mnt/hdd0/Kaggle/arc25/data/200k_HEAVY_gpt4o-description-gpt4omini-code_generated_problems/dataset_10k_part${PART}.json.gz \\\n--use-data-augmentation \\\n--output-folder /mnt/hdd0/Kaggle/arc25/predictions/2025-10-18-barc-inference/part${PART}\n</code></pre>"},{"location":"modeling/Iteration_34_multi-turn_rl/#dataset-for-2nd-turn-conversation-preparation","title":"Dataset for 2nd turn conversation preparation","text":"<p>I have done the work on the already existing notebook <code>notebooks/014_refine_solutions.ipynb</code>. The maximum prompt length is 8511, so I can keep the training parameters as they were.</p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#cluster-experiments","title":"Cluster experiments","text":"<pre><code>export BETA=0.02\nexport MAX_GRAD_NORM=0.05\nexport REPETITION_PENALTY=1.02\nexport FOLDER=2025-10-19-multi-turn-rl\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=32\nexport ACUM_STEPS=4\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${LORA_R}lora_lr${LEARNING_RATE}_${MAX_GRAD_NORM}max-grad-norm_${REWARD_NAME}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/multi-turn_rl_code_finetuning.py \\\n--lora_r ${LORA_R} \\\n--beta ${BETA} \\\n--max-grad-norm ${MAX_GRAD_NORM} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.01 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/refine_dataset.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=128G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 245114.0\n\nrsync -aPv -m  --include='*/'  --exclude *.pt --include='checkpoint-19699/***' --exclude='*'  calculon01:/mnt/scratch/users/gbarbadillo/arc25/trainings/2025-10-19-multi-turn-rl   /mnt/data/MEGA/TEMP/\n</code></pre>"},{"location":"modeling/Iteration_34_multi-turn_rl/#results","title":"Results","text":"<p>If we compare the base model doing 128 independent predictions against doing 64 predictions and refining the best ones that do not solve the task with the fine-tuned model, we see a small improvement in the metrics when evaluating on ARC-AGI-1 evaluation set.</p> initial predictions refinement predictions valid code valid outputs unique outputs train_pixel_score train_correct_grids train_pass_rate train_is_correct test_pixel_score test_correct_grids test_pass_rate test_is_correct is_correct 128 0 99.9% 71.7% 49.8% 42.1% 2.4% 1.6% 16.3% 40.9% 2.0% 2.0% 23.0% 16.3% 64 64 93.7% 76.5% 42.5% 48.7% 3.1% 1.6% 18.5% 47.2% 2.5% 2.4% 24.5% 17.8% <p>The improvement is more clear if we compare against the previous refinement experiment.</p> fine-tuned model valid code valid outputs unique outputs train_pixel_score train_correct_grids train_pass_rate train_is_correct test_pixel_score test_correct_grids test_pass_rate test_is_correct is_correct no 99.7% 74.0% 43.7% 45.8% 2.1% 1.1% 16.5% 44.4% 1.7% 1.6% 21.5% 16.0% yes 93.7% 76.5% 42.5% 48.7% 3.1% 1.6% 18.5% 47.2% 2.5% 2.4% 24.5% 17.8% <p>The fine-tuning metrics look healthy, although we see at the end of the training that the model starts doing long predictions.</p> <p></p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#conclusion","title":"Conclusion","text":"<p>We have observed a small improvement (16.3% -&gt; 17.8% pass-rate) when doing prediction refinement with a model fine-tuned with RL to do prediction refinement. If we had an stable RL training and enough time and compute, maybe this small improvement could be make bigger.</p>"},{"location":"modeling/Iteration_34_multi-turn_rl/#next-steps","title":"Next steps","text":"<ul> <li>Is RL the best way to teach the model to refine its predictions? Maybe we should use supervised learning   first, which has stronger learning signal.</li> </ul>"},{"location":"modeling/Iteration_34_multi-turn_rl/#todo","title":"TODO","text":"<ul> <li> On a first step I have to modify the current RL script to train on a generator</li> <li> Create two datasets of 10k tasks from BARC</li> <li> Generate predictions to create a 2nd turn dataset for RL</li> <li> Prepare the dataset for training</li> <li> Train 2nd turn RL</li> <li> Evaluate using the same setup from Iteration 28</li> </ul>"},{"location":"modeling/Iteration_35_fp16_vs_bf16/","title":"Iteration 35. FP16 vs BF16","text":""},{"location":"modeling/Iteration_35_fp16_vs_bf16/#iteration-35-fp16-vs-bf16","title":"Iteration 35. FP16 vs BF16","text":"<p>01-11-2025</p>"},{"location":"modeling/Iteration_35_fp16_vs_bf16/#goal","title":"Goal","text":"<p>Can we prevent RL training collapse if we use FP16 instead of BF16?</p>"},{"location":"modeling/Iteration_35_fp16_vs_bf16/#motivation","title":"Motivation","text":"<p>Yesterday I saw a lof of people talking in Twitter about using FP16 instead of BF16 for RL training.</p> <p></p> <p>They were refearing to the paper Defeating the Training-Inference Mismatch via FP16.</p> <p>Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to FP16 effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.</p> <p>The challenge is almost over, but I haven't been able to find the root of the RL training collapse. So it would be nice to see that doing this simple change solves the problem.</p>"},{"location":"modeling/Iteration_35_fp16_vs_bf16/#development","title":"Development","text":"<p>The idea is to find a previous training configuration that collapsed quickly (ideally in less than 24h) and run that configuration with bf16 and fp16 and hopefully see that one training collapses and the other does not.</p>"},{"location":"modeling/Iteration_35_fp16_vs_bf16/#cluster-experiments","title":"Cluster experiments","text":"<p>Remove the repetition penalty, and set beta to its default value.</p> Click to expand/collapse this section <pre><code>export DTYPE=bfloat16\nexport FOLDER=2025-11-01-rl-fp16\nexport BETA=0.001\nexport REPETITION_PENALTY=1.00\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=8\nexport ACUM_STEPS=1\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${DTYPE}_${LORA_R}lora_lr${LEARNING_RATE}_${REWARD_NAME}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--dtype ${DTYPE} \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.001 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=64G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n#251075.0\n\nexport DTYPE=float16\nexport FOLDER=2025-11-01-rl-fp16\nexport BETA=0.001\nexport REPETITION_PENALTY=1.00\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=8\nexport ACUM_STEPS=1\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${DTYPE}_${LORA_R}lora_lr${LEARNING_RATE}_${REWARD_NAME}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\ncondor_submit train.condor command=\"\npython /mnt/scratch/users/gbarbadillo/arc25/arc25/scripts/rl_code_finetuning.py \\\n--dtype ${DTYPE} \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.001 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--model-path /mnt/scratch/users/gbarbadillo/arc25/models/Llama-3.1-ARC-Potpourri-Induction-8B \\\n--dataset-path /mnt/scratch/users/gbarbadillo/arc25/data/barc/dataset_100k.json.gz \\\n--output-dir /mnt/scratch/users/gbarbadillo/arc25/trainings/${FOLDER}/${EXPERIMENT_NAME}\" -append request_gpus=1 -append request_cpus=${N_CPUS} -append request_memory=64G --append 'requirements = (TARGET.Machine == \"calculon21.das-nano.com\")'\n# 251076.0\n</code></pre>"},{"location":"modeling/Iteration_35_fp16_vs_bf16/#strong-compute-experiments","title":"Strong compute experiments","text":"<p>Calculon21 is full, so let's try to do the same experiments in StrongCompute.</p> Click to expand/collapse this section <pre><code># arc25\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport DTYPE=bfloat16\nexport FOLDER=2025-11-01-rl-fp16\nexport BETA=0.001\nexport REPETITION_PENALTY=1.00\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=8\nexport ACUM_STEPS=1\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${DTYPE}_${LORA_R}lora_lr${LEARNING_RATE}_${REWARD_NAME}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--dtype ${DTYPE} \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.001 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n\n# arc25_1\nsource /root/arc25_env/bin/activate\nsource /root/secrets.sh\nexport PYTHONPATH=$PYTHONPATH:/root/arc25\nexport DTYPE=float16\nexport FOLDER=2025-11-01-rl-fp16\nexport BETA=0.001\nexport REPETITION_PENALTY=1.00\nexport LEARNING_RATE=4e-6\nexport NUM_GENERATIONS=8\nexport ACUM_STEPS=1\nexport N_CPUS=20\nexport LORA_R=1\nexport EPOCHS=1\nexport REWARD_NAME=arc-v2-no-pixel-score\nexport EXPERIMENT_NAME=${DTYPE}_${LORA_R}lora_lr${LEARNING_RATE}_${REWARD_NAME}_epochs${EPOCHS}_${NUM_GENERATIONS}gen_${ACUM_STEPS}accum-steps_repetition-penalty-${REPETITION_PENALTY}_masked-truncate_unquantized_beta${BETA}\npython /root/arc25/scripts/rl_code_finetuning.py \\\n--dtype ${DTYPE} \\\n--beta ${BETA} \\\n--no-load-in-4bit \\\n--reward-name ${REWARD_NAME} \\\n--num-generations ${NUM_GENERATIONS} \\\n--gradient-accumulation-steps ${ACUM_STEPS} \\\n--learning-rate ${LEARNING_RATE} \\\n--lora_r ${LORA_R} \\\n--repetition-penalty ${REPETITION_PENALTY} \\\n--epochs ${EPOCHS} \\\n--mask-truncated-completions \\\n--scale-rewards batch \\\n--gpu_memory_utilization 0.3 \\\n--warmup-ratio 0.001 \\\n--max-seq-length 9700 \\\n--max-completion-length 1024 \\\n--n-jobs ${N_CPUS} \\\n--save-steps 200 \\\n--model-path /data/uds-fourth-five-hunter-250929 \\\n--dataset-path /root/data/barc/dataset_100k.json.gz \\\n--output-dir /root/trainings/${FOLDER}/${EXPERIMENT_NAME}\n</code></pre>"},{"location":"modeling/Iteration_35_fp16_vs_bf16/#results","title":"Results","text":""},{"location":"modeling/Iteration_35_fp16_vs_bf16/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_35_fp16_vs_bf16/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_35_fp16_vs_bf16/#todo","title":"TODO","text":"<ul> <li> Run a fp16 vs bf16 training</li> <li> Do I also have to look the precision used for VLLM?</li> </ul>"},{"location":"modeling/Iteration_n/","title":"Iteration n. Iteration_title","text":""},{"location":"modeling/Iteration_n/#iteration-n-iteration_title","title":"Iteration n. Iteration_title","text":"<p>start date</p>"},{"location":"modeling/Iteration_n/#goal","title":"Goal","text":""},{"location":"modeling/Iteration_n/#motivation","title":"Motivation","text":""},{"location":"modeling/Iteration_n/#development","title":"Development","text":""},{"location":"modeling/Iteration_n/#results","title":"Results","text":""},{"location":"modeling/Iteration_n/#conclusion","title":"Conclusion","text":""},{"location":"modeling/Iteration_n/#next-steps","title":"Next steps","text":""},{"location":"modeling/Iteration_n/#todo","title":"TODO","text":"<ul> <li>[ ]</li> </ul>"},{"location":"utils/00_Challenge_Workflow/","title":"Challenge workflow","text":""},{"location":"utils/00_Challenge_Workflow/#challenge-workflow","title":"Challenge workflow","text":""},{"location":"utils/00_Challenge_Workflow/#start-of-the-challenge","title":"Start of the challenge","text":"<ol> <li>Create a repository for the code using cookiecutter</li> <li>Add dates to the calendar</li> <li>Download rules of the challenge</li> <li>Bookmark challenge folder on file explorer</li> <li>Create a Google keep label for tasks and ideas of the challenge</li> <li>Download the challenge data</li> <li>Create a conda environment for the challenge and add it to jupyter</li> </ol> <pre><code>conda create -n arc25 pytest rope pylint tqdm numpy pandas scikit-learn ipython ipykernel coverage ipywidgets matplotlib python=3.10 -y\nconda activate arc25\npython -m ipykernel install --user --name $CONDA_DEFAULT_ENV --display-name \"Python ($CONDA_DEFAULT_ENV)\"\nmake env-export\n</code></pre> <ol> <li>Create a github repo to have a backup of the data. Vscode allows to do it directly without having to go to the website, choose a private repo. At the end of the challenge it will be made public.</li> <li>Use TDD methodology whenever possible, this will save time because errors won't be propagated along the challenge.</li> <li>Have an apprentice attitude, collaborate on the forum, I have a lot to learn from Kaggle.</li> <li>Add a nice picture to README</li> </ol>"},{"location":"utils/00_Challenge_Workflow/#end-of-the-challenge","title":"End of the challenge","text":"<ol> <li>Prepare a report with a summary of the approach to the challenge</li> <li>Download the Google keep tasks to the repository in pdf format</li> <li>Delete the tasks on google keep and the label</li> <li>Delete unnecessary data</li> <li>Update the environment yml</li> </ol>"},{"location":"utils/markdown_cheatsheet/","title":"Markdown cheatsheet","text":""},{"location":"utils/markdown_cheatsheet/#markdown-cheatsheet","title":"Markdown cheatsheet","text":""},{"location":"utils/markdown_cheatsheet/#examples-of-attaching-images","title":"Examples of attaching images","text":"<p>First an image with markdown syntax</p> <p></p> <p>Next an image with html syntax that allows to control the size</p> <p></p> <p></p><p></p>"},{"location":"utils/markdown_cheatsheet/#examples-of-equations","title":"Examples of equations","text":"<p>Equation on a different line:</p> \\[\\epsilon = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(log(\\frac{p_i}{a_i}))^2} \\] <p>Examples of inline equations. Let's consider \\(e^{q_i} = p_i + 1\\) and \\(e^{b_i} = a_i + 1\\).</p>"},{"location":"utils/markdown_cheatsheet/#easy-way-to-create-tables","title":"Easy way to create tables","text":"<p>http://www.tablesgenerator.com/markdown_tables#</p> representation_size fmeasure val_fmeasure 1024 0.893 0.573 512 0.819 0.476 256 0.676 0.365 128 0.45 0.33 64 0.31 0.29 32 0.26 0.26 16 0.214 0.216"},{"location":"utils/methodology/","title":"Methodology","text":""},{"location":"utils/methodology/#methodology","title":"Methodology","text":"<p>I'm following CRISP-DM 1.0 methodology for the reports.</p> <p>I have skipped Evaluation and Deployment steps because they are not usually done on Kaggle.</p> <ol> <li>Business understanding</li> <li>Data understanding</li> <li>Modeling</li> <li>Solution summary</li> </ol>"}]}